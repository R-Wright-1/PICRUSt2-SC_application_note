---
title: "Updating PICRUSt2 database"
output:
  html_document: 
    toc: yes
    toc_float: yes
---

# Get GTDB genomes

Taking them from: https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/

```{bash, eval=FALSE}
cd /bigpool/robyn/
mkdir picrust2_database
cd picrust2_database/
mkdir GTDB_r214
cd GTDB_r214
wget https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/ar53_marker_genes_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/ar53_msa_marker_genes_reps_r214.tar.gz

wget https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/ar53_msa_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/ar53_ssu_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/bac120_marker_genes_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/bac120_msa_marker_genes_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/bac120_msa_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/bac120_ssu_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/gtdb_genomes_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/bac120_taxonomy_r214.tsv https://data.gtdb.ecogenomic.org/releases/release214/214.1/bac120_r214.tree.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/bac120_metadata_r214.tsv.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/ar53_taxonomy_r214.tsv https://data.gtdb.ecogenomic.org/releases/release214/214.1/ar53_r214.tree https://data.gtdb.ecogenomic.org/releases/release214/214.1/ar53_metadata_r214.tsv.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/RELEASE_NOTES.txt https://data.gtdb.ecogenomic.org/releases/release214/214.1/VERSION.txt

tar -xvf gtdb_genomes_reps_r214.tar.gz
mkdir gtdb_genomes
find gtdb_genomes_reps_r214/ -type f -print0 | xargs -0 mv -t gtdb_genomes/
```

Install EggNOG:
```{bash, eval=FALSE}
conda create -n eggnog
conda activate eggnog
conda install -c bioconda -c conda-forge eggnog-mapper
mkdir eggnog_data
download_eggnog_data.py --data_dir eggnog_data
create_dbs.py -m diamond --dbname bacteria --taxa Bacteria --data_dir eggnog_data
```

Run EggNOG:
```{bash, eval=FALSE}
mkdir eggnog_out
#test
emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i GTDB_r214/gtdb_genomes/GCF_945605565.1_genomic.fna.gz -o test --output_dir eggnog_out
#this took ~3000 seconds (maybe more, I don't remember)

emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i GTDB_r214/gtdb_genomes/GCF_945605565.1_genomic.fna.gz -o test2 --output_dir eggnog_out --cpu 12 
#this took 477 secs

emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i GTDB_r214/gtdb_genomes/GCF_945605565.1_genomic.fna.gz -o test3 --output_dir eggnog_out --cpu 12 --dbmem
#this took longer... 515 secs

#copy data/database to scratch
emapper.py -m diamond --itype genome --genepred prodigal --data_dir /scratch/ramdisk/eggnog_data -i GTDB_r214/gtdb_genomes/GCF_945605565.1_genomic.fna.gz -o test4 --output_dir eggnog_out --cpu 12
#342 seconds

#temporary output folders in scratch
emapper.py -m diamond --itype genome --genepred prodigal --data_dir /scratch/ramdisk/eggnog_data -i GTDB_r214/gtdb_genomes/GCF_945605565.1_genomic.fna.gz -o test5 --output_dir eggnog_out --cpu 12 --scratch_dir /scratch/robyn/
#341 seconds

ls GTDB_r214/gtdb_genomes/*.fna.gz > gtdb_genomes_list.txt
python
import os
genomes = os.listdir('GTDB_r214/gtdb_genomes/')
genomes = [g for g in genomes if '.fna.gz' in g]
with open('gtdb_genomes_list.txt', 'w') as f:
  for g in genomes:
    f.write(g+'\n')
quit()

parallel -j 4 --progress --eta -a gtdb_genomes_list.txt 'emapper.py -m diamond --itype genome --genepred prodigal --data_dir /scratch/ramdisk/eggnog_data -i GTDB_r214/gtdb_genomes/{} -o {.} --output_dir eggnog_out/ --cpu 12 --scratch_dir /scratch/robyn/'
#didn't use this as it was going to take ~100 days. Ran it for a while though.

(/usr/bin/time -V emapper.py -m diamond --itype genome --genepred prodigal --data_dir /scratch/ramdisk/eggnog_data -i GTDB_r214/gtdb_genomes/GCF_945605565.1_genomic.fna -o test5 --output_dir eggnog_out_time --cpu 12 --scratch_dir /scratch/robyn/) 2> time_check.txt
#seeing how much memory we use
#time I don't seem to be able to get to work. Tried unzipping the fasta file but it seems to make it slower?
emapper.py -m diamond --itype genome --genepred prodigal --data_dir /scratch/ramdisk/eggnog_data -i GTDB_r214/gtdb_genomes/GCF_945605565.1_genomic.fna -o test5 --output_dir eggnog_out_time --cpu 12 --scratch_dir /scratch/robyn/
#450 seconds
```

Test prokka:
```{bash, eval=FALSE}
conda create -n prokka
conda activate prokka
conda install bioconda::prokka
gunzip GTDB_r214/gtdb_genomes/GCF_945605565.1_genomic.fna.gz
prokka --cpus 12 --outdir prokka_out --prefix GCF_945605565.1 --force GTDB_r214/gtdb_genomes/GCF_945605565.1_genomic.fna
```
Walltime used: 3.95 minutes

Test MicrobeAnnotator:
```{bash, eval=FALSE}
conda create -n MicrobeAnnotator
conda activate MicrobeAnnotator
conda remove --name MicrobeAnnotator --all
conda create -n microbeannotator python=3.7 pip microbeannotator=2.0.5
conda activate microbeannotator
pip install hmmer==0.1.0
conda install conda-forge::python-wget
conda install conda-forge::biopython
microbeannotator_db_builder -d MicrobeAnnotator_DB -m blast,diamond -t 24 --no_aspera
#microbeannotator_db_builder -d MicrobeAnnotator_DB_light -m blast,diamond -t 24 --light
```
Struggling with this install - it runs a long time but always seems to come to errors.

Probably just try with EggNOG on Compute Canada.

Get list of genomes we already have all output files for:
```{python, eval=FALSE}
import os

eout = os.listdir('eggnog_out/')
eout = [e.split('.')[0] for e in eout]
eout_set = set(eout)
got_annotation = []
for e in eout_set:
  if eout.count(e) == 5:
    got_annotation.append(e)
    
with open('vulcan_annotations.txt', 'w') as f:
  for genome in got_annotation:
    w = f.write(genome+'\n')
```

# Repeat with Compute Canada

```{bash, eval=FALSE}
ssh rwright@graham.alliancecan.ca
#password with !
cd scratch
mkdir GTDB_r214
cd GTDB_r214
wget https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/ar53_marker_genes_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/ar53_msa_marker_genes_reps_r214.tar.gz

wget https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/ar53_msa_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/ar53_ssu_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/bac120_marker_genes_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/bac120_msa_marker_genes_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/bac120_msa_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/bac120_ssu_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/gtdb_genomes_reps_r214.tar.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/bac120_taxonomy_r214.tsv https://data.gtdb.ecogenomic.org/releases/release214/214.1/bac120_r214.tree.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/bac120_metadata_r214.tsv.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/ar53_taxonomy_r214.tsv https://data.gtdb.ecogenomic.org/releases/release214/214.1/ar53_r214.tree https://data.gtdb.ecogenomic.org/releases/release214/214.1/ar53_metadata_r214.tsv.gz https://data.gtdb.ecogenomic.org/releases/release214/214.1/RELEASE_NOTES.txt https://data.gtdb.ecogenomic.org/releases/release214/214.1/VERSION.txt

#tar -xvf gtdb_genomes_reps_r214.tar.gz
#mkdir gtdb_genomes
#find gtdb_genomes_reps_r214/ -type f -print0 | xargs -0 mv -t gtdb_genomes/
#it seems like the gtdb_genomes_reps_r214.tar.gz didn't download completely, so copying across all of the fasta files from vulcan (easier to do this as it doesn't matter if the download gets interrupted).

```

Install EggNOG:
```{bash, eval=FALSE}
#conda create -n eggnog
#conda activate eggnog
#conda install -c bioconda -c conda-forge eggnog-mapper
#conda doesn't work well with compute canada
#having issues unzipping things on compute canada. Downloading to vulcan and then copying folder across
#wget https://github.com/eggnogdb/eggnog-mapper/archive/refs/tags/2.1.12.zip
#unzip 2.1.12.zip
#scp -r eggnog-mapper-2.1.12/ rwright@graham.alliancecan.ca:/home/rwright/scratch/
cp -r scratch/eggnog-mapper-2.1.12/ tools/ #been having issues with not much storage space available - asked Ben to move his things
python setup.py install
cd ..
cd ..
cd scratch
mkdir eggnog_data
download_eggnog_data.py --data_dir eggnog_data
create_dbs.py -m diamond --dbname bacteria --taxa Bacteria --data_dir eggnog_data
create_dbs.py -m diamond --dbname archaea --taxa Archaea --data_dir eggnog_data


rm gtdb_genomes/gtdb_genomes/*.decomp
```

Test single genome:
```{bash, eval=FALSE}
mkdir eggnog_out
emapper.py -m diamond --itype genome --genepred prodigal --data_dir /home/rwright/scratch/eggnog_data -i /home/rwright/scratch/gtdb_genomes/gtdb_genomes/GCA_000007325.1_genomic.fna.gz -o test2 --output_dir /home/rwright/scratch/eggnog_out --cpu 12
#310 seconds

salloc --time=1:0:0 --ntasks=12 --mem-per-cpu 120G
```

Test submit job command line:
test_eggnog.sh
```{bash, eval=FALSE}
#!/bin/bash
source /home/rwright/.bashrc
emapper.py -m diamond --itype genome --genepred prodigal --data_dir /home/rwright/scratch/eggnog_data -i /home/rwright/scratch/gtdb_genomes/gtdb_genomes/GCA_000007325.1_genomic.fna.gz -o test2 --output_dir /home/rwright/scratch/eggnog_out --cpu 12
```

```{bash, eval=FALSE}
sbatch --job-name=test_eggnog.job --output=/home/rwright/scratch/out/test_eggnog.out --error=/home/rwright/scratch/out/test_eggnog.err --mem=120G --time=0-10:00 test_eggnog.sh
scancel 18148754 #cancelled because I asked for 10 hours not 10 minutes!
sbatch --job-name=test_eggnog.job --output=/home/rwright/scratch/out/test_eggnog.out --error=/home/rwright/scratch/out/test_eggnog.err --mem=120G --time=0-00:10 test_eggnog.sh
```

Modify test_eggnog.sh:
```{bash, eval=FALSE}
#!/bin/bash
source /home/rwright/.bashrc
emapper.py -m diamond --itype genome --genepred prodigal --data_dir /home/rwright/scratch/eggnog_data -i /home/rwright/scratch/gtdb_genomes/gtdb_genomes/GCA_000007325.1_genomic.fna.gz -o test3 --output_dir /home/rwright/scratch/eggnog_out --cpu 12 --scratch_dir /home/rwright/scratch/
```

Also forgot to add any more cpus.
Resubmit:
```{bash, eval=FALSE}
sbatch --job-name=test_eggnog_2.job --output=/home/rwright/scratch/out/test_eggnog_2.out --error=/home/rwright/scratch/out/test_eggnog_2.err --mem=120G --time=0-01:00 test_eggnog.sh --cpu=12
```

More CPU's don't seem to work. Trying with an hour and no extra CPU's

Run with job file test_job.job:
```{bash, eval=FALSE}
#!/bin/bash
#SBATCH --job-name=test_eggnog_job.job
#SBATCH --output=/home/rwright/scratch/out/test_eggnog_job.out
#SBATCH --error=/home/rwright/scratch/out/test_eggnog__job.err
#SBATCH --mem=120G
#SBATCH --time=0-01:00
#SBATCH --cpus-per-task=12
#SBATCH --mail-user=robyn.wright@dal.ca
#SBATCH --mail-type=ALL
source /home/rwright/.bashrc
emapper.py -m diamond --itype genome --genepred prodigal --data_dir /home/rwright/scratch/eggnog_data/ -i /home/rwright/scratch/gtdb_genomes/gtdb_genomes/GCA_000007325.1_genomic.fna.gz -o test4 --output_dir /home/rwright/scratch/eggnog_out/ --cpu 12 --scratch_dir /home/rwright/scratch/
```
Within this, I can see that it's actually running with 12 CPUs. No idea why it doesn't when I test run from command line, though!
Output:
```{bash, eval=FALSE}
seff 18150149
Job ID: 18150149
Cluster: graham
User/Group: rwright/rwright
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 12
CPU Utilized: 00:33:57
CPU Efficiency: 50.37% of 01:07:24 core-walltime
Job Wall-clock time: 00:05:37
Memory Utilized: 3.31 GB
Memory Efficiency: 2.76% of 120.00 GB
```

Script to create all job files:
```{python, eval=FALSE}
#cd scratch
#mkdir job_files
#mkdir eggnog_scratch
import os

genomes = os.listdir('gtdb_genomes/gtdb_genomes/')
output = os.listdir('eggnog_out/')
output = list(set([o.split('.')[0] for o in output]))
genomes_strings = []
for g in range(len(genomes)):
  gen_name = genomes[g].split('_')[0]+'_'+genomes[g].split('_')[1]
  gns = genomes[g].split('.')[0]
  if gns in output: continue
  string = 'emapper.py -m diamond --itype genome --genepred prodigal --data_dir /home/rwright/scratch/eggnog_data/ -i /home/rwright/scratch/gtdb_genomes/gtdb_genomes/'
  string += genomes[g]
  string += ' -o '+gen_name
  string += ' --output_dir /home/rwright/scratch/eggnog_out/ --cpu 12 --scratch_dir /home/rwright/scratch/eggnog_scratch/\n'
  genomes_strings.append(string)
  
#batches of 500
fn = 'file_name'
string = '#!/bin/bash\n'
string += '#SBATCH --job-name=eggnog_'+fn+'.job\n'
string += '#SBATCH --output=/home/rwright/scratch/out/eggnog_'+fn+'.out\n'
string += '#SBATCH --error=/home/rwright/scratch/out/eggnog_'+fn+'.err\n'
string += '#SBATCH --mem=30G\n'
string += '#SBATCH --time=7-00:00\n'
string += '#SBATCH --cpus-per-task=12\n'
string += 'source /home/rwright/.bashrc\n'
using_string = str(string.replace(fn, 'job1'))
count = 1
for g in range(len(genomes_strings)):
  using_string += genomes_strings[g]
  if g % 500 == 0 or g == len(genomes_strings)-1:
    with open('job_files/job'+str(count)+'.job', 'w') as f:
      w = f.write(using_string)
    print(count)
    count += 1
    using_string = str(string.replace(fn, 'job'+str(count)))

#submit jobs
jobs = os.listdir('job_files/')
jobs = [j for j in jobs if 'GC' not in j]
jobs = sorted(jobs)
for j in jobs:
  os.system('sbatch job_files/'+j)


#this was what I initially ran that made too many jobs
# for g in range(len(genomes)):
#   gen_name = genomes[g].split('_')[0]+'_'+genomes[g].split('_')[1]
#   #if g > 10: break
#   string = '#!/bin/bash\n'
#   string += '#SBATCH --job-name=eggnog_'+gen_name+'.job\n'
#   string += '#SBATCH --output=/home/rwright/scratch/out/eggnog_'+gen_name+'.out\n'
#   string += '#SBATCH --error=/home/rwright/scratch/out/eggnog_'+gen_name+'.err\n'
#   string += '#SBATCH --mem=30G\n'
#   string += '#SBATCH --time=0-00:20\n'
#   string += '#SBATCH --cpus-per-task=12\n'
#   string += 'source /home/rwright/.bashrc\n'
#   string += 'emapper.py -m diamond --itype genome --genepred prodigal --data_dir /home/rwright/scratch/eggnog_data/ -i /home/rwright/scratch/gtdb_genomes/gtdb_genomes/'
#   string += genomes[g]
#   string += ' -o '+gen_name
#   string += ' --output_dir /home/rwright/scratch/eggnog_out/ --cpu 12 --scratch_dir /home/rwright/scratch/eggnog_scratch/'
#   with open('job_files/job'+gen_name+'.job', 'w') as f:
#     w = f.write(string)
  
```

Can't submit this many jobs individually. Doing in sets of 500 genomes in single job script.
```{bash, eval=FALSE}
squeue -u rwright | awk '{print $1}' | xargs -n 1 scancel
```
The above worked after modification!

Remembered that I already have annotations for a lot of the genomes. Made a list on vulcan, copied this across. Now make a script to move those genomes so they can't be found when eggnog tries with that one.
```{bash, eval=FALSE}
#scp vulcan_annotations.txt rwright@graham.alliancecan.ca:/home/rwright/scratch/
mkdir gtdb_genomes/got_annotations/
```

```{python, eval=FALSE}
import os
genomes = os.listdir('gtdb_genomes/gtdb_genomes/')
got_genomes = []
for row in open('vulcan_annotations.txt', 'r'):
  got_genomes.append(row.replace('\n', ''))

got_genomes = set(got_genomes)

for genome in genomes:
  gen_name = genome.split('.')[0]
  if gen_name in got_genomes:
    m = os.system('mv gtdb_genomes/gtdb_genomes/'+genome+' gtdb_genomes/got_annotations/')
    
print(len(genomes))
#85234
genomes = os.listdir('gtdb_genomes/gtdb_genomes/')
print(len(genomes))
#80285
```
Did this while the jobs were already running, so hopefully it won't cause anything to stop!

Ran out of space - transferred results across to Vulcan, now adding these to annotations:
```{bash, eval=FALSE}
scp -r rwright@graham.alliancecan.ca:/home/rwright/scratch/eggnog_out/ .
```

```{python, eval=FALSE}
import os

annotations = os.listdir('eggnog_out_graham/eggnog_out/')
annotations = [a.split('.')[0] for a in annotations if '.annotations' in a]

with open('graham_annotations.txt', 'w') as f:
  for a in annotations:
    w = f.write(a+'\n')
```

```{python, eval=FALSE}
import os
genomes = os.listdir('gtdb_genomes/gtdb_genomes/')
got_genomes = []
for row in open('graham_annotations.txt', 'r'):
  got_genomes.append(row.replace('\n', ''))

got_genomes = set(got_genomes)

for genome in genomes:
  gen_name = genome.split('.')[0]
  if gen_name in got_genomes:
    m = os.system('mv gtdb_genomes/gtdb_genomes/'+genome+' gtdb_genomes/got_annotations/')
    
print(len(genomes))
#155614
genomes = os.listdir('gtdb_genomes/gtdb_genomes/')
print(len(genomes))
#5170
```

Moving across from Graham again July 2nd:
```{bash}
scp -r rwright@graham.alliancecan.ca:/home/rwright/eggnog_out/ eggnog_out_graham_2/
```

On Graham:
```{bash}
rm -r eggnog_out/
mkdir eggnog_out
mv scratch/eggnog_out/*.annotations eggnog_out/
rm scratch/eggnog_out/*
rm scratch/eggnog_scratch/*
scp -r rwright@graham.alliancecan.ca:/home/rwright/eggnog_out/ eggnog_out_graham_3/
```

Add these to annotations again:
```{python, eval=FALSE}
import os

annotations = os.listdir('eggnog_out_graham_2/')
annotations = [a.split('.')[0] for a in annotations if '.annotations' in a]

with open('graham_annotations_2.txt', 'w') as f:
  for a in annotations:
    w = f.write(a+'\n')
    
annotations = os.listdir('eggnog_out_graham_3/')
annotations = [a.split('.')[0] for a in annotations if '.annotations' in a]

with open('graham_annotations_3.txt', 'w') as f:
  for a in annotations:
    w = f.write(a+'\n')
```

```{python, eval=FALSE}
import os

genomes = []
for row in open('gtdb_genomes_list.txt', 'r'):
  genomes.append(row.split('.')[0].replace('\n', ''))
  
got_genomes = []
for row in open('graham_annotations.txt', 'r'):
  got_genomes.append(row.split('.')[0].replace('\n', ''))

for row in open('graham_annotations_2.txt', 'r'):
  got_genomes.append(row.split('.')[0].replace('\n', ''))
  
for row in open('graham_annotations_3.txt', 'r'):
  got_genomes.append(row.split('.')[0].replace('\n', ''))

for row in open('vulcan_annotations.txt', 'r'):
  got_genomes.append(row.replace('\n', ''))

got_genomes = set(got_genomes)
print(len(genomes))
#85206
print(len(got_genomes))
#85208

need_genomes = []
for genome in genomes:
  if genome not in got_genomes:
    need_genomes.append(genome)
    
print(need_genomes)
#['GCF_004153545', 'GCF_900406815']

with open('need_annotations_jul_24.txt', 'w') as f:
  for row in need_genomes:
    w = f.write(row+'\n')
```

Run these last two!!
```{bash}
conda activate eggnog

gunzip GTDB_r214/gtdb_genomes/GCF_004153545.1_genomic.fna.gz
gunzip GTDB_r214/gtdb_genomes/GCF_900406815.1_genomic.fna.gz

emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i GTDB_r214/gtdb_genomes/GCF_004153545.1_genomic.fna -o GCF_004153545.1_genomic.fna --output_dir eggnog_out_last --cpu 12 --scratch_dir /scratch/robyn/

emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i GTDB_r214/gtdb_genomes/GCF_900406815.1_genomic.fna -o GCF_900406815.1_genomic.fna --output_dir eggnog_out_last --cpu 12 --scratch_dir /scratch/robyn/

#test with one that we already have output for:
gunzip GTDB_r214/gtdb_genomes/GCA_023428205.1_genomic.fna.gz
emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i GTDB_r214/gtdb_genomes/GCA_023428205.1_genomic.fna -o GCA_023428205.1_genomic.fna --output_dir eggnog_out_last --cpu 12 --scratch_dir /scratch/robyn/
rm eggnog_out_last/GCA_023428205.1_genomic*

#this works, so it must be something with the other files.
#due to some sequences with all repeats? Just manually delete these parts?
cp GTDB_r214/gtdb_genomes/GCF_004153545.1_genomic.fna GTDB_r214/gtdb_genomes/GCF_004153545.1_genomic.edit.fna
cp GTDB_r214/gtdb_genomes/GCF_900406815.1_genomic.fna GTDB_r214/gtdb_genomes/GCF_900406815.1_genomic.edit.fna
vi GTDB_r214/gtdb_genomes/GCF_004153545.1_genomic.edit.fna
#remove >NZ_SDVA01000103.1 and >NZ_SDVA01000104.1
vi GTDB_r214/gtdb_genomes/GCF_900406815.1_genomic.edit.fna
#remove >NZ_OVCN01000074.1 and >NZ_OVCN01000073.1 and >NZ_OVCN01000071.1

emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i GTDB_r214/gtdb_genomes/GCF_004153545.1_genomic.edit.fna -o GCF_004153545.1_genomic.edit.fna --output_dir eggnog_out_last --cpu 12 --scratch_dir /scratch/robyn/

emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i GTDB_r214/gtdb_genomes/GCF_900406815.1_genomic.edit.fna -o GCF_900406815.1_genomic.edit.fna --output_dir eggnog_out_last --cpu 12 --scratch_dir /scratch/robyn/
```

## Collate output from EggNOG

Initial tests on a single output:
```{python}
import os
import pandas as pd

genome = 'GCA_023428205.1_genomic'
file = 'eggnog_out/'+genome+'.fna.emapper.annotations'
df = pd.read_csv(file, index_col=0, header=4, sep='\t')
hits = df.index.values
hits_unique = list(set(hits))
print(len(hits), len(hits_unique))
#1532 1532
#these are the same length

# print(df.columns)
# #Index(['seed_ortholog', 'evalue', 'score', 'eggNOG_OGs', 'max_annot_lvl', 'COG_category', 'Description', 'Preferred_name', 'GOs', 'EC', 'KEGG_ko', 'KEGG_Pathway', 'KEGG_Module', 'KEGG_Reaction', 'KEGG_rclass', 'BRITE', 'KEGG_TC', 'CAZy', 'BiGG_Reaction', 'PFAMs'], dtype='object')
# evals = set(df['evalue'].values)
# print(max(evals))
# 
# print(df.loc[:, ['Preferred_name']].head(10))
#picrust now: COG, EC, KO, PFAM, Pheno, Tigrfam

columns = ['eggNOG_OGs', 'COG_category', 'Description', 'Preferred_name', 'GOs', 'EC', 'KEGG_ko', 'KEGG_Pathway', 'KEGG_Module', 'KEGG_Reaction', 'KEGG_rclass', 'BRITE', 'KEGG_TC', 'CAZy', 'BiGG_Reaction', 'PFAMs']
dicts = {}
for col in columns:
  dicts[col] = {}

for col in columns:
  df[col] = df[col]
  vals = list(df[col].values)
  vals = [v for v in vals if str(v) != 'nan' and str(v) != '-']
  vals = ','.join(vals).split(',')
  vals_unique = set(vals)
  for v in vals_unique:
    if v in dicts[col]:
      dicts[col][v].append([genome, vals.count(v)])
    else:
      dicts[col][v] = [[genome, vals.count(v)]]
 
  

```

Now combining for all:
```{python}
import os
import pandas as pd
from multiprocessing import Pool
from multiprocessing import freeze_support
from multiprocessing import Process, Manager
# import functools
# from itertools import repeat

folders = ['eggnog_out/', 'eggnog_out_graham/eggnog_out/', 'eggnog_out_graham_2/', 'eggnog_out_graham_3/', 'eggnog_out_last/']
files = []
for folder in folders:
  folder_files = os.listdir(folder)
  folder_files = [folder+f for f in folder_files]
  files += folder_files

files = [f for f in files if '.emapper.annotations' in f]
columns = ['Preferred_name', 'GOs', 'EC', 'KEGG_ko', 'CAZy', 'BiGG_Reaction', 'PFAMs']

for col in columns:
  os.system('mkdir collate_annotations/'+col)
  
def process_file(f):
  try:
    if '_genomic' in f:
      genome = f.split('/')[-1].split('_genomic')[0]
    elif '.fna' in f:
      genome = f.split('/')[-1].split('.fna')[0]
    else:
      genome = f.split('/')[-1].split('.emapper')[0]
    print(genome)
    df = pd.read_csv(f, index_col=0, header=4, sep='\t')
    for col in columns:
      vals = list(df[col].values)
      vals = [v for v in vals if str(v) != 'nan' and str(v) != '-']
      vals = ','.join(vals).split(',')
      vals_unique = set(vals)
      this_df = []
      for v in vals_unique:
        this_df.append([v, vals.count(v)])
      this_df = pd.DataFrame(this_df, columns=['Names', genome])
      this_df.set_index('Names', inplace=True)
      # col_df[col].append(this_df)
      this_df.to_csv('collate_annotations/'+col+'/'+genome)
  except:
    with open('couldnt_collate_annotations.txt', 'a') as fi:
      w = fi.write(f+'\t'+genome+'\n')
  return

def run_multiprocessing(func, i, n_processors):
    with Pool(processes=n_processors) as pool:
        return pool.map(func, i)

def main():
    print('Starting processing')
    run_multiprocessing(process_file, files, 24)

if __name__ == "__main__":
    freeze_support()   # required to use multiprocessing
    main()

```

Some of the annotation files seem to be empty... re-run these.

Try with just one:
```{bash, eval=FALSE}
#GCA_003565625.1
conda activate eggnog
gunzip GTDB_r214/gtdb_genomes/GCA_003565625.1_genomic.fna.gz
emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i GTDB_r214/gtdb_genomes/GCA_003565625.1_genomic.fna -o GCA_003565625.1 --output_dir eggnog_out_last --cpu 24 --scratch_dir /scratch/robyn/
```

Now get all genomes that we don't have annotations for:
```{bash, eval=FALSE}
mkdir genomes_no_annotations
```

```{python, eval=FALSE}
import os

f = 'couldnt_collate_annotations.txt'
genomes = set(os.listdir('GTDB_r214/gtdb_genomes/'))
genomes_to_move = []

for row in open(f, 'r'):
  genomes_to_move.append(row.split('\t')[1].replace('\n', ''))
  
for genome in genomes_to_move:
  for fn in genomes:
    if genome in fn:
      os.system('mv GTDB_r214/gtdb_genomes/'+fn+' genomes_no_annotations/')
```

```{bash, eval=FALSE}
parallel -j 24 --eta --progress 'gunzip {}' ::: genomes_no_annotations/*
parallel -j 1 --eta --progress 'emapper.py -m diamond --itype genome --genepred prodigal --data_dir eggnog_data -i {} -o {/.} --output_dir eggnog_out_last --cpu 24 --scratch_dir /scratch/robyn/' ::: genomes_no_annotations/*
```

Now have 431,128 files across these folders: eggnog_out, eggnog_out_graham/eggnog_out, eggnog_out_graham_2, eggnog_out_graham_3, eggnog_out_last

Consolidate annotations first:
```{python}
#mkdir eggnog_out_consolidated
import os
from multiprocessing import Pool
from multiprocessing import freeze_support
from multiprocessing import Process, Manager

def get_genome_name(fn):
    if '_genomic' in fn:
      genome = fn.split('/')[-1].split('_genomic')[0]
    elif '.fna' in fn:
      genome = fn.split('/')[-1].split('.fna')[0]
    else:
      genome = fn.split('/')[-1].split('.emapper')[0]
    return(genome)
  
def copy_file(fn):
  os.system('cp '+fn+' eggnog_out_consolidated/'+get_genome_name(f)+'.emapper.annotations')
  return

folders = ['eggnog_out_last/', 'eggnog_out/', 'eggnog_out_graham/eggnog_out/', 'eggnog_out_graham_2/', 'eggnog_out_graham_3/']
count = 0
for fol in folders:
  #if fol != 'eggnog_out_last/': continue
  files = [f for f in os.listdir(fol) if '.emapper.annotations' in f]
  files_genomes, files_to_move, existing_genomes, empty_files = {}, [], [], []
  for gen in os.listdir('eggnog_out_consolidated'): existing_genomes.append(get_genome_name(gen))
  for f in files: files_genomes[f] = get_genome_name(f)
  for f in files:
    if get_genome_name(f) not in existing_genomes:
      if os.path.getsize(fol+f) > 0:
        files_to_move.append(fol+f)
      else:
        empty_files.append(f)
        print('Empty file: ', fol+f)
  for f in files_to_move:
    copy_file(f)
    #if count > 10: break
    if count % 100 == 0: print(count)
    count += 1
  

```

Tidy:
```{bash, eval=FALSE}
mkdir all_intermediate_annotations
mv collate_annotations all_intermediate_annotations/
mv couldnt_collate_annotations* all_intermediate_annotations/
mv eggnog_out all_intermediate_annotations/
mv eggnog_out_graham* all_intermediate_annotations/
mv eggnog_out_last all_intermediate_annotations/
mv graham_annotations* all_intermediate_annotations/
mv eggnog_data* all_intermediate_annotations/
rm -r emappertmp_prod_4lytkhqx/
mv eggnog_out_time/ all_intermediate_annotations/
parallel -j 24 'gzip {}' ::: genomes_no_annotations/*
mv need_annotations_jul_24.txt all_intermediate_annotations/
mv time_check.txt all_intermediate_annotations/
mv vulcan_annotations.txt all_intermediate_annotations/

mkdir collate_annotations
```

Taking script from above again:
```{python}
import os
import pandas as pd
from multiprocessing import Pool
from multiprocessing import freeze_support
from multiprocessing import Process, Manager

folders = ['eggnog_out_consolidated/']
files = []
for folder in folders:
  folder_files = os.listdir(folder)
  folder_files = [folder+f for f in folder_files]
  files += folder_files

files = [f for f in files if '.emapper.annotations' in f]
columns = ['Preferred_name', 'GOs', 'EC', 'KEGG_ko', 'CAZy', 'BiGG_Reaction', 'PFAMs']

for col in columns:
  os.system('mkdir collate_annotations/'+col)
  
def process_file(f):
  try:
    if '_genomic' in f:
      genome = f.split('/')[-1].split('_genomic')[0]
    elif '.fna' in f:
      genome = f.split('/')[-1].split('.fna')[0]
    else:
      genome = f.split('/')[-1].split('.emapper')[0]
    print(genome)
    df = pd.read_csv(f, index_col=0, header=4, sep='\t')
    for col in columns:
      vals = list(df[col].values)
      vals = [v for v in vals if str(v) != 'nan' and str(v) != '-']
      vals = ','.join(vals).split(',')
      vals_unique = set(vals)
      this_df = []
      for v in vals_unique:
        this_df.append([v, vals.count(v)])
      this_df = pd.DataFrame(this_df, columns=['Names', genome])
      this_df.set_index('Names', inplace=True)
      # col_df[col].append(this_df)
      this_df.to_csv('collate_annotations/'+col+'/'+genome)
  except:
    with open('couldnt_collate_annotations_2.txt', 'a') as fi:
      w = fi.write(f+'\t'+genome+'\n')
  return

def run_multiprocessing(func, i, n_processors):
    with Pool(processes=n_processors) as pool:
        return pool.map(func, i)

def main():
    print('Starting processing')
    run_multiprocessing(process_file, files, 24)

if __name__ == "__main__":
    freeze_support()   # required to use multiprocessing
    main()

```

One single file that this didn't work for:
```{bash, eval=FALSE}
mv eggnog_out_consolidated/GCA_005217565.1.emapper.annotations eggnog_out_consolidated/GCA_005217565.1_needs_fixing.emapper.annotations
```

```{python, eval=FALSE}
import os
import pandas as pd
from multiprocessing import Pool
from multiprocessing import freeze_support
from multiprocessing import Process, Manager

columns = ['Preferred_name', 'GOs', 'EC', 'KEGG_ko', 'CAZy', 'BiGG_Reaction', 'PFAMs']

f = 'eggnog_out_consolidated/GCA_005217565.1_needs_fixing.emapper.annotations' #this is the one that doesn't work!
#f = 'eggnog_out_consolidated/GCA_005217455.1.emapper.annotations' #this is one that I'm testing!
#df = pd.read_csv(f, index_col=0, header=4, sep='\t')

new_rows = []
for row in open(f, 'r'):
  row = row.split('\t')
  if len(row) > 21:
    row = ['WRPE01000008.1_30', '635013.TherJR_2833', '4.09e-46', '169.0', 'COG0845@1|root,COG0845@2|Bacteria,1V2JZ@1239|Firmicutes,24A5A@186801|Clostridia,261XD@186807|Peptococcaceae', '186801|Clostridia', 'M', 'DNA polymerase', 'dnaE', '-', '2.7.7.7', 'ko:K02337,ko:K14162', 'ko00230,ko00240,ko01100,ko03030,ko03430,ko03440,map00230,map00240,map01100,map03030,map03430,map03440', 'M00260', 'R00375,R00376,R00377,R00378', 'RC02795', 'ko00000,ko00001,ko00002,ko01000,ko03032,ko03400', '-', '-', '-', 'DNA_pol3_alpha,HHH_6,PHP,tRNA_anti-codon\n']
  row = '\t'.join(row)
  new_rows.append(row)

with open('eggnog_out_consolidated/GCA_005217565.1.emapper.annotations', 'w') as f:
  for row in new_rows:
    f.write(row)
    
df = pd.read_csv('eggnog_out_consolidated/GCA_005217565.1.emapper.annotations', index_col=0, header=4, sep='\t')
f = 'eggnog_out_consolidated/GCA_005217565.1.emapper.annotations'
genome = f.split('/')[-1].split('.emapper')[0]
for col in columns:
      vals = list(df[col].values)
      vals = [v for v in vals if str(v) != 'nan' and str(v) != '-']
      vals = ','.join(vals).split(',')
      vals_unique = set(vals)
      this_df = []
      for v in vals_unique:
        this_df.append([v, vals.count(v)])
      this_df = pd.DataFrame(this_df, columns=['Names', genome])
      this_df.set_index('Names', inplace=True)
      # col_df[col].append(this_df)
      this_df.to_csv('collate_annotations/'+col+'/'+genome)
    
```

Now lets add all of the annotations together:
```{bash, eval=FALSE}
mkdir picrust_annotations
```

```{python, eval=FALSE}
import os
import pandas as pd
# from multiprocessing import Pool
# from multiprocessing import freeze_support
# from multiprocessing import Process, Manager

folders = os.listdir('collate_annotations')
# for folder in folders:
#   annotations = os.listdir('collate_annotations/'+folder)
#   os.system('mkdir picrust_annotations/'+folder+'_intermediate')
#   for a in range(len(annotations)):
#     df = pd.read_csv('collate_annotations/'+folder+'/'+annotations[a], index_col=0, header=0)
#     if a % 100 == 0: 
#       print(folder, a)
#       if a != 0:
#         join_df.to_csv('picrust_annotations/'+folder+'_intermediate/'+str(a)+'.csv')
#       join_df = df.copy(deep=True)
#     else:
#       join_df = join_df.join(df).fillna(value=0)
# 
#Realised that there were some missing annotations because they were only saved every 100
# 
# for folder in folders:
#   annotations = os.listdir('collate_annotations/'+folder)
#   #os.system('mkdir picrust_annotations/'+folder+'_intermediate')
#   got_annotations = os.listdir('picrust_annotations/'+folder+'_intermediate')
#   got_annotations = [int(f.replace('.csv', '')) for f in got_annotations]
#   for a in range(len(annotations)):
#     if a < max(got_annotations): continue
#     else:
#       df = pd.read_csv('collate_annotations/'+folder+'/'+annotations[a], index_col=0, header=0)
#       try:
#         join_df
#       except:
#         join_df = df.copy(deep=True)
#     if a % 100 == 0:
#       print(folder, a)
#       if a != 0:
#         join_df.to_csv('picrust_annotations/'+folder+'_intermediate/'+str(a)+'.csv')
#       join_df = df.copy(deep=True)
#     else:
#       join_df = join_df.join(df).fillna(value=0)
#   join_df.to_csv('picrust_annotations/'+folder+'_intermediate/'+str(a)+'.csv')

# if running again, don't worry about the above and just run this
for folder in folders:
  annotations = os.listdir('collate_annotations/'+folder)
  os.system('mkdir picrust_annotations/'+folder+'_intermediate')
  for a in range(len(annotations)):
    df = pd.read_csv('collate_annotations/'+folder+'/'+annotations[a], index_col=0, header=0)
    if a % 100 == 0:
      print(folder, a)
      if a != 0:
        join_df.to_csv('picrust_annotations/'+folder+'_intermediate/'+str(a)+'.csv')
      join_df = df.copy(deep=True)
    else:
      join_df = join_df.join(df).fillna(value=0)
  join_df.to_csv('picrust_annotations/'+folder+'_intermediate/'+str(a)+'.csv')
  
```

Next step:

```{python, eval=FALSE}
#this is super slow...
# for folder in folders:
#   intermediates = os.listdir('picrust_annotations/'+folder+'_intermediate')
#   joined_dfs = []
#   print(folder, 'Processing the files of 100')
#   for i in range(len(intermediates)):
#     df = pd.read_csv('picrust_annotations/'+folder+'_intermediate/'+intermediates[i], index_col=0, header=0)
#     if i % 20 == 0:
#       print(folder, i)
#     if i == 0:
#       join_df = df.copy(deep=True)
#     else:
#       join_df = join_df.join(df).fillna(value=0)
#   joined_dfs.append(join_df)
#   print(folder, 'Now joining all of them')
#   combined_df = pd.concat(joined_dfs)
#   combined_df = combined_df.fillna(value=0)
#   combined_df = combined_df.groupby(by=combined_df.index, axis=0).sum()
#   combined_df.to_csv('picrust_annotations/'+folder+'.csv')
#   # for j in range(len(joined_dfs)):
#   #   if j == 0:
#   #     all_joined = joined_dfs[j].copy(deep=True)
#   #join_df.to_csv('picrust_annotations/'+folder+'.csv')
```

Try again:
```{python, eval=FALSE}
import os
import pandas as pd

#folders = ['Preferred_name']
folders = os.listdir('collate_annotations')

for folder in folders:
  if os.path.exists('picrust_annotations/'+folder+'.txt'): continue
  intermediates = os.listdir('picrust_annotations/'+folder+'_intermediate')
  dfs = []
  print('Looping folders', folder)
  rows = []
  for i in range(len(intermediates)):
    print('Looping intermediates', folder, i)
    df = pd.read_csv('picrust_annotations/'+folder+'_intermediate/'+intermediates[i], index_col=0, header=0)
    df.index = df.index.map(str)
    df = df.astype(int).astype(str)
    dfs.append(df)
    rows.extend(df.index.values)
  rows = sorted(list(set(rows)))
  rows = set(rows)
  
  overall_dict, genomes = {k:[] for k in rows}, []
  for d in range(len(dfs)):
    print('Looping dataframes', folder, d)
    zero = ['0' for z in range(dfs[d].shape[1])]
    for row in rows:
      if row in dfs[d].index.values:
        overall_dict[row].extend(dfs[d].loc[row, :])
      else:
        overall_dict[row].extend(zero)
    genomes.extend(list(dfs[d].columns))
    
  for key in overall_dict:
    print(len(overall_dict[key]))
    
  df_list, keys = [], []
  for key in overall_dict:
    df_list.append(overall_dict[key])
    keys.append(key)
  
  with open('picrust_annotations/'+folder+'.txt', 'a') as f:
    w = f.write('index\t'+'\t'.join(genomes)+'\n')
    for r in range(len(df_list)):
      print('Looping list to save', folder, r, keys[r])
      w = f.write(keys[r]+'\t'+'\t'.join(df_list[r])+'\n')
    
```

Not everything seems to be in the annotated files. Going to go through and add these to the files:
There is 100 missing from each one so I must have missed adding these somehow!
```{python}
import os

folders = os.listdir('collate_annotations')
for f in folders:
  print(f)
  annotations = []
  count = 0
  for row in open('picrust_annotations/'+f+'.txt', 'r'):
    if count > 0: break
    row = row.replace('\n', '').split('\t')
    annotations = row[1:]
    count += 1
  annotations_order = list(annotations)
  annotations = set(annotations)
  all_annotations = os.listdir('collate_annotations/'+f)
  need_annotations = [g for g in all_annotations if g not in annotations]
  current_annotation_dict = {}
  print('Reading in current annotations')
  for row in open('picrust_annotations/'+f+'.txt', 'r'):
    if 'index' in row: continue
    row = row.replace('\n', '').split('\t')
    current_annotation_dict[row[0]] = row[1:]
  print('Got all', len(current_annotation_dict), 'annotations')
  print('Getting missing annotations')
  for genome in need_annotations:
    this_annotation_dict = {}
    for row in open('collate_annotations/'+f+'/'+genome, 'r'):
      if genome in row: continue
      row = row.replace('\n', '').split(',')
      this_annotation_dict[row[0]] = row[1]
    length = 0
    for func in current_annotation_dict:
      if length == 0: length = len(current_annotation_dict[func])
      if func in this_annotation_dict:
        current_annotation_dict[func].append(this_annotation_dict[func])
      else:
        current_annotation_dict[func].append('0')
    for func in this_annotation_dict:
      if func not in current_annotation_dict:
        current_annotation_dict[func] = ['0' for l in range(length)]
        current_annotation_dict[func].append(this_annotation_dict[func])
    annotations_order.append(genome)
  print('Now got', len(current_annotation_dict), 'annotations')
  df_list, keys = [], []
  for key in current_annotation_dict:
    df_list.append(current_annotation_dict[key])
    keys.append(key)
  print('Now got annotations for this many genomes:', len(annotations_order))
  with open('picrust_annotations/'+f+'_all.txt', 'w') as fi:
    w = fi.write('index\t'+'\t'.join(annotations_order)+'\n')
    print('Looping list to save', f)
    for r in range(len(df_list)):
      w = fi.write(keys[r]+'\t'+'\t'.join(df_list[r])+'\n')

```

# Investigate tree etc

```{bash, eval=FALSE}
cd GTDB_r214
tar -xvf bac120_ssu_reps_r214.tar.gz
wget https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_all/ssu_all_r214.tar.gz
tar -xvf ssu_all_r214.tar.gz
tar -xvf bac120_msa_marker_genes_reps_r214.tar.gz
tar -xvf ar53_ssu_reps_r214.tar.gz
wget https://data.gtdb.ecogenomic.org/releases/release214/214.1/auxillary_files/sp_clusters_r214.tsv
wget https://data.gtdb.ecogenomic.org/releases/release214/214.1/bac120_r214.tree
wget https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_all/bac120_marker_genes_all_r214.tar.gz
```

So I think that I will need to create an alignment of the ssu reps, maybe separately for bacteria and archaea? Or can this tree be combined? Then this could be used for insertion of sequences into the existing tree?

```{bash, eval=FALSE}
mkdir gtdb_picrust_files
cp GTDB_r214/bac120_ssu_reps_r214.fna gtdb_picrust_files/
cp GTDB_r214/ar53_ssu_reps_r214.fna gtdb_picrust_files/
cp GTDB_r214/ssu_all_r214.fna gtdb_picrust_files/
cp GTDB_r214/ar53_r214.tree gtdb_picrust_files/
cp GTDB_r214/bac120_r214.tree gtdb_picrust_files/

grep -c ">" gtdb_picrust_files/bac120_ssu_reps_r214.fna #46891
grep -c ">" gtdb_picrust_files/ar53_ssu_reps_r214.fna #2812
#these are obviously different numbers from the number of genomes that I have annotated..... maybe some of these are from species clusters? - have all genomes here, but one from each species cluster
less sp_clusters_r214.tsv | wc -l #85206
#maybe best to filter the overall 16S file to the genomes that I have?
```

Filter ssu file - this took me a while to figure out, but the reason that I can't find all of the genomes in the file is because not all of the genomes have a 16S gene :( some have multiple... so just going to continue with all of them for now for making a MSA, and then will worry about that after. 

```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC
import os

ssu_all = 'gtdb_picrust_files/ssu_all_r214.fna'
genomes_annotated = os.listdir('collate_annotations/EC/')
genomes_annotated = set(genomes_annotated)

ids = []
for record in SeqIO.parse(ssu_all, "fasta"):
  ids.append(record.id)
  
print(genomes_annotated[:10])
print(ids[:10])

ids_annotated_overlap = []
for i in ids:
  g = i.split('~')[0].split('_', 1)[1]
  if g in genomes_annotated:
    ids_annotated_overlap.append(i)
    
ids_annotated_overlap = set(ids_annotated_overlap)

filtered_ssu = []
for record in SeqIO.parse(ssu_all, "fasta"):
  if record.id in ids_annotated_overlap:
    filtered_ssu.append(record)

SeqIO.write(filtered_ssu, 'gtdb_picrust_files/ssu_filtered_r214.fna', "fasta")
```

So after all this, we can actually just use the bac120_ssu_reps_r214.fna file that has been made. 

Now look at trees:
```{python, eval=FALSE}
from ete3 import Tree
import os

tree_name_bac = 'gtdb_picrust_files/bac120_r214.tree'
tree_name_arc = 'gtdb_picrust_files/ar53_r214.tree'

genomes_annotated = os.listdir('collate_annotations/EC/')
genomes_annotated = set(genomes_annotated)
  
tree_bac = Tree(tree_name_bac, format=1, quoted_node_names=True)
bac_genomes_in_tree = []
for node in tree_bac.traverse("postorder"):
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    bac_genomes_in_tree.append(node.name)
  
tree_arc = Tree(tree_name_arc, format=1, quoted_node_names=True)
arc_genomes_in_tree = []
for node in tree_arc.traverse("postorder"):
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    arc_genomes_in_tree.append(node.name)
    
print(len(bac_genomes_in_tree)+len(arc_genomes_in_tree))
#85205
```

So the trees at least only contain the representative species. 

## Create MSA/HMM of 16S genes

```{bash, eval=FALSE}
cat gtdb_picrust_files/bac120_ssu_reps_r214.fna gtdb_picrust_files/ar53_ssu_reps_r214.fna > gtdb_picrust_files/combined_bac120_ar53_ssu_reps_r214.fna
```

```{bash, eval=FALSE}
conda activate clustalo
clustalo --in=gtdb_picrust_files/combined_bac120_ar53_ssu_reps_r214.fna --out=gtdb_picrust_files/combined_bac120_ar53_ssu_reps_r214.sto --force --outfmt=st --wrap=80 --threads 24

esl-reformat -o gtdb_picrust_files/combined_bac120_ar53_ssu_reps_r214.fna afa gtdb_picrust_files/combined_bac120_ar53_ssu_reps_r214.sto

hmmbuild --cpu 24 gtdb_picrust_files/combined_bac120_ar53_ssu_reps_r214.hmm gtdb_picrust_files/combined_bac120_ar53_ssu_reps_r214.sto
```

Now do this separately for each:
```{bash, eval=FALSE}
conda activate clustalo
parallel -j 1 'clustalo --in={} --out={.}.sto --force --outfmt=st --wrap=80 --threads 24' ::: gtdb_picrust_files/bac120_ssu_reps_r214.fna gtdb_picrust_files/ar53_ssu_reps_r214.fna

parallel -j 1 'esl-reformat -o {.}_aligned.fna afa {}' ::: gtdb_picrust_files/bac120_ssu_reps_r214.sto gtdb_picrust_files/ar53_ssu_reps_r214.sto

parallel -j 1 'hmmbuild --cpu 24 {.}.hmm {}' ::: gtdb_picrust_files/bac120_ssu_reps_r214.sto gtdb_picrust_files/ar53_ssu_reps_r214.sto
```

Get tree models for bacteria and archaea separately:
```{bash, eval=FALSE}
cd gtdb_picrust_files
raxml-ng --evaluate --msa ar53_ssu_reps_r214_aligned.fna --tree ar53_r214.tree --prefix ar53_r214 --model GTR+G —threads 24
```

This doesn't work, maybe some not allowed characters?
```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC
import os
from ete3 import Tree
import os

aligned_fasta = 'ar53_ssu_reps_r214_aligned.fna'
new_fasta = []
for record in SeqIO.parse(aligned_fasta, "fasta"):
  new_record = SeqRecord(record.seq, id=record.id, description='')
  new_fasta.append(new_record)

SeqIO.write(new_fasta, aligned_fasta.replace('_aligned', '_aligned_fixed'), "fasta")

tree_file = 'ar53_r214.tree'
tree = Tree(tree_file, format=1, quoted_node_names=True)
other_nodes = []
for node in tree.traverse("postorder"):
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    do_something = True
  elif '_' in node.name:
    for s in node.name:
      if not s.isalnum():
        if s not in ['_', ':', ' ', '.', '-', ';']:
          other_nodes.append(node.name)
          break
    #other_nodes.append(node.name)

print(other_nodes[:100])
#Doesn't really seem like I have characters that shouldn't be there. Not sure why it isn't working.
```

Try again:
```{bash, eval=FALSE}
raxml-ng --evaluate --msa ar53_ssu_reps_r214_aligned_fixed.fna --tree ar53_r214.tree --prefix ar53_r214 --model GTR+G —threads 24
#ERROR: ERROR reading tree file (LIBPLL-111): syntax error, unexpected end of file, expecting CPAR or COMMA. (line 1 column 152-178)
#looking at the file, starting at column 152 is: '100.0:f__Undinarchaeaceae; g__Undinarchaeum' 178 would be around the space. So maybe the space is making the tree parser believe that there's an unfinished string? I'll try removing spaces from node names.
```

```{python, eval=FALSE}
from ete3 import Tree
import os

tree_file = 'ar53_r214.tree'
tree = Tree(tree_file, format=1, quoted_node_names=True)
for node in tree.traverse("postorder"):
  if ' ' in node.name:
    node.name = node.name.replace(' ', '')

tree.write(outfile=tree_file.replace('.tree', '_fixed.tree'), format=1)
```

Try again:
```{bash, eval=FALSE}
rm *raxml*
raxml-ng --evaluate --msa ar53_ssu_reps_r214_aligned_fixed.fna --tree ar53_r214_fixed.tree --prefix ar53_r214_raxml --model GTR+G —threads 24
#ERROR: Some taxa are missing from the alignment file
```

Try removing nodes that aren't genomes?
```{python, eval=FALSE}
from ete3 import Tree
import os

tree_file = 'ar53_r214_fixed.tree'
tree = Tree(tree_file, format=1, quoted_node_names=True)
genome_nodes = []
for node in tree.traverse("postorder"):
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    genome_nodes.append(node.name)
    
tree_red = tree
tree_red.prune(genome_nodes)
tree_red == tree
#True
tree_red.write(outfile=tree_file.replace('.tree', '_reduced.tree'), format=1)
```


They're identical anyway. So let's start again but we'll run the raxml-check first:
```{bash, eval=FALSE}
mkdir raxml_issues
mv *raxml* raxml_issues/
raxml-ng --check --msa ar53_ssu_reps_r214_aligned.fna --model GTR+G --prefix ar53_r214_raxml-check
```

Filter the tree file to include only these:
```{python, eval=FALSE}
from ete3 import Tree
import os
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC

fixed_alignment = 'ar53_r214_raxml-check.raxml.reduced.phy'
genomes = []
for row in open(fixed_alignment, 'r'):
  if 'GB_' in row or 'GCA_' in row or 'GCF_' in row:
    genomes.append(row.split(' ')[0])

genomes = set(genomes)

tree_file = 'ar53_r214.tree'
tree = Tree(tree_file, format=1, quoted_node_names=True)
genome_nodes = []
all_genomes = []
for node in tree.traverse("postorder"):
  if ' ' in node.name:
    node.name = node.name.replace(' ', '')
  if node.name in genomes:
    genome_nodes.append(node.name)
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    all_genomes.append(node.name)

print(len(genome_nodes), len(all_genomes))
#(2795, 4416)
tree_red = tree
tree_red.prune(genome_nodes)
tree == tree_red
#True
tree_red.write(outfile=tree_file.replace('.tree', '_fixed.tree'), format=1)
```

Try again:
```{bash, eval=FALSE}
raxml-ng --evaluate --msa ar53_r214_raxml-check.raxml.reduced.phy --tree ar53_r214_fixed.tree --prefix ar53_r214_raxml --model GTR+G —threads 24
```

This works! Repeat for bacteria:
```{bash, eval=FALSE}
raxml-ng --check --msa bac120_ssu_reps_r214_aligned.fna --model GTR+G --prefix bac120_r214_raxml-check
```

Filter the tree file to include only these:
```{python, eval=FALSE}
from ete3 import Tree
import os
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC

fixed_alignment = 'bac120_r214_raxml-check.raxml.reduced.phy'
genomes = []
for row in open(fixed_alignment, 'r'):
  if 'GB_' in row or 'GCA_' in row or 'GCF_' in row:
    genomes.append(row.split(' ')[0])

genomes = set(genomes)

tree_file = 'bac120_r214.tree'
tree = Tree(tree_file, format=1, quoted_node_names=True)
genome_nodes = []
all_genomes = []
for node in tree.traverse("postorder"):
  if ' ' in node.name:
    node.name = node.name.replace(' ', '')
  if node.name in genomes:
    genome_nodes.append(node.name)
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    all_genomes.append(node.name)

print(len(genome_nodes), len(all_genomes))
#(45792, 80789)
tree_red = tree
tree_red.prune(genome_nodes)
tree == tree_red # somehow this line doesn't seem to work as it always says true and I don't believe it - or maybe I needed to redefine the tree_red somehow? It is just a direct copy rather than "deep" copy?
#True
tree_red.write(outfile=tree_file.replace('.tree', '_fixed.tree'), format=1)
```

```{bash, eval=FALSE}
raxml-ng --evaluate --msa bac120_r214_raxml-check.raxml.reduced.phy --tree bac120_r214_fixed.tree --prefix bac120_r214_raxml --model GTR+G —threads 24
```

Now make the hmm files again:
```{bash, eval=FALSE}
esl-reformat -o ar53_r214_raxml-check.raxml.reduced.sto stockholm ar53_r214_raxml-check.raxml.reduced.phy
#Alignment input parse error:
#   alignment length disagrees with header: header said 5162, parsed 5163
#   while reading PHYLIP (interleaved) file ar53_r214_raxml-check.raxml.reduced.phy
#   at or near line 2796
cp ar53_r214_raxml-check.raxml.reduced.phy ar53_r214_raxml-check.raxml.reduced_edit.phy
vi ar53_r214_raxml-check.raxml.reduced_edit.phy
#switch first line from 2795 5162 to 2795 5163
esl-reformat -o ar53_r214_raxml-check.raxml.reduced_edit.sto stockholm ar53_r214_raxml-check.raxml.reduced_edit.phy

esl-reformat -o bac120_r214_raxml-check.raxml.reduced.sto stockholm bac120_r214_raxml-check.raxml.reduced.phy
#Alignment input parse error:
#   alignment length disagrees with header: header said 9038, parsed 9039
#   while reading PHYLIP (interleaved) file bac120_r214_raxml-check.raxml.reduced.phy
#   at or near line 45793
cp bac120_r214_raxml-check.raxml.reduced.phy bac120_r214_raxml-check.raxml.reduced_edit.phy
vi bac120_r214_raxml-check.raxml.reduced_edit.phy
#switch first line from 45792 9038 to 45792 9039
esl-reformat -o bac120_r214_raxml-check.raxml.reduced_edit.sto stockholm bac120_r214_raxml-check.raxml.reduced_edit.phy

hmmbuild --cpu 24 bac120_r214_raxml-check.raxml.reduced_edit.hmm bac120_r214_raxml-check.raxml.reduced_edit.sto
hmmbuild --cpu 24 ar53_r214_raxml-check.raxml.reduced_edit.hmm ar53_r214_raxml-check.raxml.reduced_edit.sto
```

Now copy all output:
```{bash, eval=FALSE}
mkdir bac_ref
mkdir arc_ref

esl-reformat -o bac_ref/bac_ref.fna afa bac120_r214_raxml-check.raxml.reduced_edit.phy
esl-reformat -o arc_ref/arc_ref.fna afa ar53_r214_raxml-check.raxml.reduced_edit.phy

cp bac120_r214_raxml-check.raxml.reduced_edit.hmm bac_ref/bac_ref.hmm
cp ar53_r214_raxml-check.raxml.reduced_edit.hmm arc_ref/arc_ref.hmm

cp bac120_r214_fixed.tree bac_ref/bac_ref.tre
cp ar53_r214_fixed.tree arc_ref/arc_ref.tre

cp bac120_r214_raxml.raxml.bestModel bac_ref/bac_ref.model
cp ar53_r214_raxml.raxml.bestModel arc_ref/arc_ref.model
```

# Test these files!!

Get tutorial files:
```{bash, eval=FALSE}
conda activate picrust-v2.5.3 

mkdir picrust_test
cd picrust_test
wget http://kronos.pharmacology.dal.ca/public_files/picrust/picrust2_tutorial_files/chemerin_16S.zip
unzip chemerin_16S.zip
cd ..
```

Try placing the sequences!
```{bash, eval=FALSE}
place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/chemerin_16S/placed_seqs.tre -p 12 --intermediate picrust_test/chemerin_16S/placement_working --ref_dir bac_ref --min_align 0.2
```
They don't align :( - my guess is because the 16S alignment seems way too long?

Let's go back and remake these...

# Make the files again

```{bash, eval=FALSE}
mkdir alignment_with_gtdb_seqs_only
mv *raxml* alignment_with_gtdb_seqs_only/
mv *fixed* alignment_with_gtdb_seqs_only/
mv *.sto alignment_with_gtdb_seqs_only/
mv *_ref alignment_with_gtdb_seqs_only/
mv *_aligned* alignment_with_gtdb_seqs_only/
mv combined_bac120_ar53_ssu_reps_r214.* alignment_with_gtdb_seqs_only/
mv ssu_* alignment_with_gtdb_seqs_only/
```

Get previous 16S seqs for alignment:
```{bash, eval=FALSE}
conda activate clustalo
cp /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/pro_ref/pro_ref.fna current_pro_ref.fna

clustalo --in=ar53_ssu_reps_r214.fna --out=ar53_ssu_reps_r214_pro_ref.sto --profile1 current_pro_ref.fna --force --outfmt=st --wrap=80 --threads 24

clustalo --in=bac120_ssu_reps_r214.fna --out=bac120_ssu_reps_r214_pro_ref.sto --profile1 current_pro_ref.fna --force --outfmt=st --wrap=80 --threads 24

#--maxseqlen=

esl-reformat -o ar53_ssu_reps_r214_pro_ref.fna afa ar53_ssu_reps_r214_pro_ref.sto
```

These still look long... double checked and it's not actually that the sequences didn't align, but probably a similar naming issue to what I found before.

Will just try with the regular alignment:
```{bash, eval=FALSE}
conda activate picrust-v2.5.3 

place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/chemerin_16S/placed_seqs.tre -p 12 --intermediate picrust_test/chemerin_16S/placement_working_default_files
```

Looking at the files in more detail, the bac_ref.fna file has all sequence names as ">GB_GCA_000" (nothing more), so they actually are all identical!
I'll have a look at all of the output again to see where this issue came from. Maybe the reformat step?

```{bash, eval=FALSE}
conda activate clustalo
cd alignment_with_gtdb_seqs_only/
less bac120_r214_raxml-check.raxml.reduced_edit.phy # this has all of the sequence names in correctly, so it must be the esl-reformat step
mv bac_ref/bac_ref.fna bac_ref/bac_ref_bad_names.fna
mv arc_ref/arc_ref.fna arc_ref/arc_ref_bad_names.fna
```

Try reformatting with Bio SeqIO instead? Looks like Bio SeqIO defaults to only reading the first 10 characters of a sequence name for phylip files, so maybe this is always the way? Will read it in manually and save to fasta:
```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment

files = ['bac120_r214_raxml-check.raxml.reduced_edit.phy', 'ar53_r214_raxml-check.raxml.reduced_edit.phy']
new_names = ['bac_ref/bac_ref.fna', 'arc_ref/arc_ref.fna']

for f in range(len(files)):
  sequences = {}
  seq_records = []
  count = 0
  for row in open(files[f], 'r'):
    #if count > 10: break
    if '_' in row:
      name, seq = row.replace('\n', '').split(' ')
      sequences[name] = seq
      seq_records.append(SeqRecord(Seq(seq),id=name, description=''))
    count += 1
  msa = MultipleSeqAlignment(seq_records)
  AlignIO.write(msa, new_names[f], "fasta")
```

Now try PICRUSt2 again:
```{bash, eval=FALSE}
conda activate picrust-v2.5.3 
cp -r alignment_with_gtdb_seqs_only/bac_ref/ .
cp -r alignment_with_gtdb_seqs_only/arc_ref/ .

place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/chemerin_16S/placed_seqs.tre -p 12 --intermediate picrust_test/chemerin_16S/placement_working_bac_ref --ref_dir bac_ref
#Standard error of the above failed command:
#--mapali MSA bac_ref/bac_ref.fna isn't same as the one HMM came from (checksum mismatch)
```

Aaaaaand edit again:
```{bash, eval=FALSE}
conda activate clustalo 
mv bac_ref/bac_ref.hmm bac_ref/bac_ref_bad_names.hmm
mv arc_ref/arc_ref.hmm arc_ref/arc_ref_bad_names.hmm

hmmbuild --cpu 24 bac_ref/bac_ref.hmm bac_ref/bac_ref.fna
hmmbuild --cpu 24 arc_ref/arc_ref.hmm arc_ref/arc_ref.fna
```

And PICRUSt2 again:
```{bash, eval=FALSE}
conda activate picrust-v2.5.3 

place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/chemerin_16S/placed_seqs.tre -p 12 --intermediate picrust_test/chemerin_16S/placement_working_bac_ref --ref_dir bac_ref
#Warning - 27 input sequences aligned poorly to reference sequences (--min_align option specified a minimum proportion of 0.8 aligning to reference sequences). These input sequences will not be placed and will be excluded from downstream steps.

#This is the set of poorly aligned input sequences to be excluded: 48292bb527301239168556057a7805bb, 6d5af1db8934361889a6fb8d80c70836, cebb9cfc1d8430f13650adacf98b0a61, de1be320837d0ac367b4ccef7f7ef44c, d307aef8086bbdb1c3e4f3d5511412dc, c6f8879689d204423d60258f2759ebf1, 5c5f6d5a2d45a21bc576e475985efa17, ff33233ffebbe6e3720af8bba7e89f08, d6cecaaf52a711cc96417a5334067830, 40c2346925c479d2f90d76aea73b0975, 691eeed271e420c8e7a91d5ea0cf5431, abd40160e03332710641ee74b1b42816, 763af2e6cfd1d573893fa6d28aafc4b5, f5b23f626e3f2d2d1213f83c6de3e385, 7e5d7cc681f5a0c25e0816a77d59ffb2, 86681c8e9c64b6683071cf185f9e3419, 8db33d54e5184b0f448ae140f793368a, b639d3575f66b7b6b5dd908999072997, 343635a5abc8d3b1dbd2b305eb0efe32, cfe4029cc35d2695cf0ce8c5ff332956, bac85f511e578b92b4a16264719bdb2d, 94be7094bb21b6425f166ebb9f64f64f, a895f151de7f3a3e9b84e15800425631, 663baa27cddeac43b08f428e96650bf5, 288c8176059111c4c7fdfb0cd5afce64, f3e89fa547a772288e6624c88135a47f, daac2b933e609bc0b483a4c25c92a055

place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/chemerin_16S/placed_seqs.tre -p 12 --intermediate picrust_test/chemerin_16S/placement_working_bac_ref_min_align --ref_dir bac_ref --min_align 0.5
# this worked!!!
```

Maybe I need to look at trimming the alignments?
Let's investigate how much of the first section of the alignments that for lots of sequences looks to be all '-' is actually informative for any:
```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
#from Bio.Alphabet import IUPAC
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment
import numpy as np

alignment = AlignIO.read(open("bac_ref/bac_ref.fna"), "fasta")
print("Alignment length %i" % alignment.get_alignment_length())
#Alignment length 9038
seq_lengths = []
for record in alignment:
  for s in range(len(str(record.seq))):
    if str(record.seq)[s] != '-':
      seq_lengths.append(s)
      break
    
print(min(seq_lengths))
#0
print(max(seq_lengths))
#8053
print(np.mean(seq_lengths))
#1619.0734189378056
print(np.median(seq_lengths))
#821.0


#and again for the alignment that used the previous pro_ref.fna
alignment = AlignIO.read(open("bac120_ssu_reps_r214_pro_ref.sto"), "stockholm")
print("Alignment length %i" % alignment.get_alignment_length())
#Alignment length 10112
seq_lengths = []
for record in alignment:
  for s in range(len(str(record.seq))):
    if str(record.seq)[s] != '-':
      seq_lengths.append(s)
      break
    
print(min(seq_lengths))
#0
print(max(seq_lengths))
#8053
print(np.mean(seq_lengths))
#1619.0734189378056
print(np.median(seq_lengths))
#821.0

```

So these are too long. Looking at PICRUSt2 methods, ssu-align was used. Maybe this will be more appropriate? Let's try again.

# Make files again with ssu-align

First step to remove identical sequences while keeping information about these genomes:
```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment
import numpy as np

files = ['ar53_ssu_reps_r214.fna', 'bac120_ssu_reps_r214.fna']
new_files = ['ar53_ssu_reps_r214_dedup.fna', 'bac120_ssu_reps_r214_dedup.fna']
new_dup = ['ar53_ssu_reps_r214_duplicates.txt', 'bac120_ssu_reps_r214_duplicates.txt']

#first, check for duplicated genome IDs
for f in range(len(files)):
  ids = {}
  count = 0
  for record in SeqIO.parse(files[f], "fasta"):
    ids[record.id] = record.seq
    count += 1
  print(len(ids), count)
  #2812 2812
  #46891 46891
#all good - nothing duplicated

for f in range(len(files)):
  #if f != 0: continue
  sequences = {}
  count = 0
  for record in SeqIO.parse(files[f], "fasta"):
    seq = str(record.seq)
    if seq in sequences:
      sequences[seq].append(record.id)
    else:
      sequences[seq] = [record.id]
    count += 1
  print(len(sequences), count)
  #2795 2812
  #45791 46891
  duplicated_seqs = []
  dedup_records = []
  for seq in sequences:
    dedup_records.append(SeqRecord(Seq(seq),id=sequences[seq][0], description=''))
    if len(sequences[seq]) > 1:
      duplicated_seqs.append(','.join(sequences[seq]))
  print(duplicated_seqs)
  with open(new_dup[f], 'w') as fi:
    for row in duplicated_seqs:
      w = fi.write(row+'\n')
  SeqIO.write(dedup_records, new_files[f], "fasta")
  
```

Maybe just cluster them with vsearch like PICRUSt2 did? The above code doesn't account for things that could be identical but one is longer...
```{bash, eval=FALSE}
vsearch --cluster_fast ar53_ssu_reps_r214.fna --id 1 --uc ar53_ssu_reps_r214_clusters.uc --threads 12
#Reading file ar53_ssu_reps_r214.fna 100%  
#3255227 nt in 2812 seqs, min 200, max 2326, avg 1158
#Masking 100%  
#Sorting by length 100%
#Counting unique k-mers 100%  
#Clustering 100%  
#Sorting clusters 100%
#Writing clusters 100%  
#Clusters: 2764 Size min 1, max 4, avg 1.0
#Singletons: 2721, 96.8% of seqs, 98.4% of clusters

vsearch --cluster_fast bac120_ssu_reps_r214.fna --id 1 --uc bac120_ssu_reps_r214_clusters.uc --threads 12
#Reading file bac120_ssu_reps_r214.fna 100%  
#59154163 nt in 46891 seqs, min 200, max 3300, avg 1262
#Masking 100%  
#Sorting by length 100%
#Counting unique k-mers 100%  
#Clustering 100%  
#Sorting clusters 100%
#Writing clusters 100%  
#Clusters: 45032 Size min 1, max 61, avg 1.0
#Singletons: 43777, 93.4% of seqs, 97.2% of clusters
```

Sort through cluster output:
```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment
import numpy as np

clusters = ['ar53_ssu_reps_r214_clusters.uc', 'bac120_ssu_reps_r214_clusters.uc']
fasta = ['ar53_ssu_reps_r214.fna', 'bac120_ssu_reps_r214.fna']
new_fasta = ['ar53_ssu_reps_r214_vsearch_dedup.fna', 'bac120_ssu_reps_r214_vsearch_dedup.fna']
dup_files = ['ar53_ssu_reps_r214_vsearch_dups.txt', 'bac120_ssu_reps_r214_vsearch_dups.txt']

for c in range(len(clusters)):
  if c == 0: continue
  cluster_rep = []
  dups = {}
  all_genomes = []
  rows = []
  for row in open(clusters[c], 'r'):
    row = row.replace('\n', '').split('\t')[8:]
    try:
      row.remove('*')
    except:
      row = row
    rows.append(','.join(row))
  rows = set(rows)
  for row in rows:
    if ',' not in row:
      cluster_rep.append(row)
      all_genomes.append(row)
    else:
      row = row.split(',')
      all_genomes.append(row[0])
      all_genomes.append(row[1])
      if row[0] in dups:
        dups[row[0]].append(row[1])
      elif row[1] in dups:
        dups[row[1]].append(row[0])
      else:
        dups[row[0]] = [row[1]]
  print(len(set(all_genomes)))
  print(len(cluster_rep), len(dups))
  cluster_rep = set(cluster_rep)
  dups_rev = {}
  for dup in dups:
    if dups[dup][0] in dups_rev:
      dups_rev[dups[dup][0]].append(dup)
    else:
      dups_rev[dups[dup][0]] = [dup]
  dup_file = []
  for dup in dups_rev:
    dup_file.append(dup+','+','.join(dups_rev[dup]))
  with open(dup_files[c], 'w') as f:
    for row in dup_file:
      w = f.write(row+'\n')
  new_seqs = []
  for record in SeqIO.parse(fasta[c], "fasta"):
    if record.id in cluster_rep:
      new_seqs.append(SeqRecord(record.seq,id=record.id, description=''))
  SeqIO.write(new_seqs, new_fasta[c], "fasta")
    
```

Install ssu-align:
```{bash, eval=FALSE}
cd tools
conda activate clustalo
wget eddylab.org/software/ssu-align/ssu-align-0.1.1.tar.gz
tar xfz ssu-align-0.1.1.tar.gz
cd ssu-align-0.1.1
./configure
make
sudo make install

export PATH="$PATH:/usr/local/bin"
export MANPATH="$MANPATH:/usr/local/share/man"
export SSUALIGNDIR="/usr/local/share/ssu-align-0.1.1"
#source ∼/.bashrc
which ssu-align
```

And run ssu-align:
```{bash, eval=FALSE}
#ar53_ssu_reps_r214_vsearch_dedup.fna
#bac120_ssu_reps_r214_vsearch_dedup.fna
cd /bigpool/robyn/picrust2_database/gtdb_picrust_files/
ssu-align --rfonly ar53_ssu_reps_r214_vsearch_dedup.fna ssu_align_ar53_ssu_reps_r214_vsearch_dedup
ssu-align --rfonly bac120_ssu_reps_r214_vsearch_dedup.fna ssu_align_bac120_ssu_reps_r214_vsearch_dedup
```

There were sequences that didn't align with the bacterial model - this is probably what threw it off so much the first time. Proceed with the ones that did... probably will remain unclear whether these are bad sequences or bad genomes, but better to remove anyway.

```{bash, eval=FALSE}
esl-reformat -o ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea.fna afa ssu_align_ar53_ssu_reps_r214_vsearch_dedup/ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea.stk

esl-reformat -o ssu_align_bac120_ssu_reps_r214_vsearch_dedup.bacteria.fna afa ssu_align_bac120_ssu_reps_r214_vsearch_dedup/ssu_align_bac120_ssu_reps_r214_vsearch_dedup.bacteria.stk
```

Run the raxml check:
```{bash, eval=FALSE}
raxml-ng --check --msa ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea.fna --model GTR+G --prefix ar53_r214_raxml-check
#this didn't need fixing!

raxml-ng --check --msa ssu_align_bac120_ssu_reps_r214_vsearch_dedup.bacteria.fna --model GTR+G --prefix bac120_r214_raxml-check
#this had some duplicates removed still apparently
```

Filter the tree file to include only these:
```{python, eval=FALSE}
from ete3 import Tree
import os
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
#from Bio.Alphabet import IUPAC

fixed_alignments = ['ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea.fna', 'bac120_r214_raxml-check.raxml.reduced.phy']
tree_files = ['ar53_r214.tree', 'bac120_r214.tree']
for f in range(len(fixed_alignments)):
  fixed_alignment = fixed_alignments[f]
  tree_file = tree_files[f]
  genomes = []
  for row in open(fixed_alignment, 'r'):
    if 'GB_' in row or 'GCA_' in row or 'GCF_' in row:
      genomes.append(row.split(' ')[0].replace('>', '').replace('\n', ''))
  genomes = set(genomes)
  print(len(genomes))
  #2741
  #44752
  tree = Tree(tree_file, format=1, quoted_node_names=True)
  genome_nodes = []
  all_genomes = []
  for node in tree.traverse("postorder"):
    if ' ' in node.name:
      node.name = node.name.replace(' ', '')
    if node.name in genomes:
      genome_nodes.append(node.name)
    if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
      all_genomes.append(node.name)
  print(len(genome_nodes), len(all_genomes))
  #(2741, 4416)
  #(44752, 80789)
  tree_red = tree
  tree_red.prune(genome_nodes)
  tree_red.write(outfile=tree_file.replace('.tree', '_fixed.tree'), format=1)
```

Run raxml:
```{bash, eval=FALSE}
raxml-ng --evaluate --msa ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea.fna --tree ar53_r214_fixed.tree --prefix ar53_r214_raxml --model GTR+G —threads 24

raxml-ng --evaluate --msa bac120_r214_raxml-check.raxml.reduced.phy --tree bac120_r214_fixed.tree --prefix bac120_r214_raxml --model GTR+G —threads 24
```

Now reformat for the hmm files:
```{bash, eval=FALSE}
esl-reformat -d -o ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea_dna.fna afa ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea.fna

esl-reformat -o ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea_dna.sto stockholm ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea_dna.fna
```

Resave the bacteria phylip file:
```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment

files = ['bac120_r214_raxml-check.raxml.reduced.phy']
new_names = ['bac120_r214_raxml-check.raxml.reduced.fna']

for f in range(len(files)):
  sequences = {}
  seq_records = []
  count = 0
  for row in open(files[f], 'r'):
    #if count > 10: break
    if '_' in row:
      name, seq = row.replace('\n', '').split(' ')
      sequences[name] = seq
      seq_records.append(SeqRecord(Seq(seq),id=name, description=''))
    count += 1
  msa = MultipleSeqAlignment(seq_records)
  AlignIO.write(msa, new_names[f], "fasta")
```

Convert the fasta file to DNA and reformat to stockholm:
```{bash, eval=FALSE}
esl-reformat -d -o bac120_r214_raxml-check.raxml.reduced_dna.fna afa bac120_r214_raxml-check.raxml.reduced.fna

esl-reformat -o bac120_r214_raxml-check.raxml.reduced_dna.sto stockholm bac120_r214_raxml-check.raxml.reduced_dna.fna
```

And run the HMMs:
```{bash, eval=FALSE}
hmmbuild --cpu 24 ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea_dna.hmm ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea_dna.sto
hmmbuild --cpu 24 bac120_r214_raxml-check.raxml.reduced_dna.hmm bac120_r214_raxml-check.raxml.reduced_dna.sto
```

Now copy all output:
```{bash, eval=FALSE}
mv arc_ref/ arc_ref_initial
mv bac_ref/ bac_ref_initial

mkdir bac_ref
mkdir arc_ref

cp bac120_r214_raxml-check.raxml.reduced_dna.fna bac_ref/bac_ref.fna
cp ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea_dna.fna arc_ref/arc_ref.fna

cp bac120_r214_raxml-check.raxml.reduced_dna.hmm bac_ref/bac_ref.hmm
cp ssu_align_ar53_ssu_reps_r214_vsearch_dedup.archaea_dna.hmm arc_ref/arc_ref.hmm

cp bac120_r214_fixed.tree bac_ref/bac_ref.tre
cp ar53_r214_fixed.tree arc_ref/arc_ref.tre

cp bac120_r214_raxml.raxml.bestModel bac_ref/bac_ref.model
cp ar53_r214_raxml.raxml.bestModel arc_ref/arc_ref.model
```

## Test again!

```{bash, eval=FALSE}
conda activate picrust-v2.5.3 

place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/chemerin_16S/placed_seqs.tre -p 12 --intermediate picrust_test/chemerin_16S/placement_working_ssu_align_arc --ref_dir arc_ref

place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/chemerin_16S/placed_seqs.tre -p 12 --intermediate picrust_test/chemerin_16S/placement_working_ssu_align_bar --ref_dir bac_ref
```
Everything worked!!! I guess maybe we'll have to run the predictions with both and then see which gives the smallest NSTI...? This is in the next step but we'll need the trait tables for that :)

# Start again and get 16S genes from genomes

Note that doing it this way means that I can count how many copies per genome

```{bash, eval=FALSE}
conda install -c bioconda -c conda-forge barrnap
mkdir genomes_to_search_barrnap
mkdir genomes_to_search_barrnap/bacteria
mkdir genomes_to_search_barrnap/archaea
```

### First get bacterial and archaeal genomes separately using tree files

```{python}
from ete3 import Tree
import os

tree_name_bac = 'gtdb_picrust_files/bac120_r214.tree'
tree_name_arc = 'gtdb_picrust_files/ar53_r214.tree'
  
tree_bac = Tree(tree_name_bac, format=1, quoted_node_names=True)
bac_genomes_in_tree = []
for node in tree_bac.traverse("postorder"):
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    bac_genomes_in_tree.append(node.name)
  
tree_arc = Tree(tree_name_arc, format=1, quoted_node_names=True)
arc_genomes_in_tree = []
for node in tree_arc.traverse("postorder"):
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    arc_genomes_in_tree.append(node.name)

genomes = os.listdir('GTDB_r214/gtdb_genomes/')
genomes = [g for g in genomes if '.decomp' not in g]
bac_genomes_in_tree = [g.split('_', 1)[1] for g in bac_genomes_in_tree]
arc_genomes_in_tree = [g.split('_', 1)[1] for g in arc_genomes_in_tree]
bac_genomes_in_tree = set(bac_genomes_in_tree)
arc_genomes_in_tree = set(arc_genomes_in_tree)
bac_genomes = []
arc_genomes = []

bac_count = 0
arc_count = 0
for genome in genomes:
  if genome.replace('_genomic.fna.gz', '') in bac_genomes_in_tree:
    m = os.system('mv GTDB_r214/gtdb_genomes/'+genome+' genomes_to_search_barrnap/bacteria')
    bac_count += 1
  elif genome.replace('_genomic.fna.gz', '') in arc_genomes_in_tree:
    m = os.system('mv GTDB_r214/gtdb_genomes/'+genome+' genomes_to_search_barrnap/archaea')
    arc_count += 1
  if bac_count % 1000 == 0:
    print('Bacterial genomes moved: '+str(bac_count))
  if arc_count % 1000 == 0:
    print('Archaeal genomes moved: '+str(arc_count))
    
print(bac_count, arc_count)
#77930 4184
```

List of bacteria:
```{python, eval=FALSE}
import os

genomes = os.listdir('bacteria')
with open('bacteria.txt', 'w') as f:
  for genome in genomes:
    f.write(genome.replace('.gz', '')+'\n')
```

### Run barrnap

```{bash, eval=FALSE}
cd genomes_to_search_barrnap
mkdir barrnap_bacteria
mkdir barrnap_bacteria_08
mkdir barrnap_archaea_08
mkdir barrnap_archaea

parallel -j 24 --progress --eta 'gunzip {}' ::: archaea/*.gz
parallel -j 8 --progress --eta 'barrnap -q -k arc {} --outseq barrnap_archaea/{/} --reject 0.5 --threads 6' ::: archaea/*.fna

parallel -j 24 --progress --eta -a bacteria.txt 'gunzip bacteria/{}.gz'
parallel -j 8  --progress --eta -a bacteria.txt 'barrnap -q -k bac bacteria/{} --outseq barrnap_bacteria/{} --reject 0.5 --threads 6'

parallel -j 8  --progress --eta -a bacteria.txt 'barrnap -q -k bac bacteria/{} --outseq barrnap_bacteria_08/{} --reject 0.8 --threads 6'

parallel -j 8 --progress --eta 'barrnap -q -k arc {} --outseq barrnap_archaea_08/{/} --reject 0.8 --threads 6' ::: archaea/*.fna

mkdir barrnap_bacteria_16S_single_0.8
mkdir barrnap_archaea_16S_single_0.8
mkdir barrnap_bacteria_16S_multiple_0.8
mkdir barrnap_archaea_16S_multiple_0.8

```

### Count copies per genome and get files with multiple 16S copies only

```{python, eval=FALSE}
from Bio import SeqIO
import os
import pandas as pd
import numpy as np

kingdom = 'bacteria'
genomes = os.listdir('barrnap_'+kingdom+'_08')

copies = []
for genome in genomes:
  count = 0
  sequences = []
  for record in SeqIO.parse('barrnap_'+kingdom+'_08/'+genome, "fasta"):
    if '16S' in record.id:
      sequences.append(record)
      count += 1
  copies.append([genome, count])
  if sequences != []:
    if len(sequences) == 1:
      w = SeqIO.write(sequences, 'barrnap_'+kingdom+'_16S_single_0.8/'+genome, "fasta")
    else:
      w = SeqIO.write(sequences, 'barrnap_'+kingdom+'_16S_multiple_0.8/'+genome, "fasta")

with open(kingdom+'_16S_copies_0.8.txt', 'w') as f:
  for copy in copies:
    w = f.write(copy[0].replace('_genomic.fna', '')+'\t'+str(copy[1])+'\n')
    
k05 = pd.read_csv(kingdom+'_16S_copies_0.5.txt', sep='\t', header=None, index_col=0)
k05 = k05[k05.max(axis=1) > 0]
k08 = pd.read_csv(kingdom+'_16S_copies_0.8.txt', sep='\t', header=None, index_col=0)
k08 = k08[k08.max(axis=1) > 0]
print(kingdom)
print('50%: ', k05.shape[0], np.mean(k05.iloc[:, 0].values), np.median(k05.iloc[:, 0].values), np.max(k05.iloc[:, 0].values))
print('80%: ', k08.shape[0], np.mean(k08.iloc[:, 0].values), np.median(k08.iloc[:, 0].values), np.max(k08.iloc[:, 0].values))
# archaea
# 50%:  1898 1.1991570073761855 1.0
# 80%:  1553 1.2137797810688988 1.0
# bacteria
# 50%:  36644 1.777071280427901 1.0 37
# 80%:  32595 1.8404663291915937 1.0 37
```

### Now cluster the ones with multiple 16S copies

```{bash, eval=FALSE}
mkdir barrnap_bacteria_16S_clustered_0.8
parallel -j 8 'vsearch --cluster_fast {} --id 0.9 --centroids barrnap_bacteria_16S_clustered_0.8/{/} --threads 6' ::: barrnap_bacteria_16S_multiple_0.8/*

mkdir barrnap_archaea_16S_clustered_0.8
parallel -j 8 --progress --eta 'vsearch --cluster_fast {} --id 0.9 --centroids barrnap_archaea_16S_clustered_0.8/{/} --threads 6' ::: barrnap_archaea_16S_multiple_0.8/*
```

### Make files with single 16S for each genome

```{python, eval=FALSE}
import os
from Bio import SeqIO
import random
from Bio.SeqRecord import SeqRecord

kingdoms = ['archaea', 'bacteria']
for kingdom in kingdoms:
  clusters = os.listdir('barrnap_'+kingdom+'_16S_clustered_0.8')
  singles = []
  ids = []
  for genome in clusters:
    records = []
    for record in SeqIO.parse('barrnap_'+kingdom+'_16S_clustered_0.8/'+genome, "fasta"):
      this_record = SeqRecord(record.seq, id=genome.replace('_genomic.fna', ''), description='')
      records.append(this_record)
    if len(records) > 1: 
      num = int(random.choice(range(len(records))))
      singles.append(records[num])
      ids.append(records[num].id)
    else:
      singles.append(records[0])
      ids.append(records[0].id)
  single_copies = os.listdir('barrnap_'+kingdom+'_16S_single_0.8/')
  for genome in single_copies:
    for record in SeqIO.parse('barrnap_'+kingdom+'_16S_single_0.8/'+genome, "fasta"):
      this_record = SeqRecord(record.seq, id=genome.replace('_genomic.fna', ''), description='')
      singles.append(this_record)
      ids.append(this_record.id)
  SeqIO.write(singles, kingdom+"_16S_genes_0.8.fasta", "fasta")
```

Look at these sequence lengths:
```{python, eval=FALSE}
from Bio import SeqIO
import numpy as np

kingdom = 'archaea'
lengths = []
for record in SeqIO.parse(kingdom+"_16S_genes_0.8.fasta", "fasta"):
  lengths.append(len(str(record.seq)))

print(min(lengths), max(lengths), np.mean(lengths), np.median(lengths))
#bacteria
#1268 1862 1509.0679552078539 1520.0
#archaea
#1270 2547 1474.9130714745654 1472.0
```

### Cluster these single 16S genes for all genomes

```{bash, eval=FALSE}
vsearch --cluster_fast archaea_16S_genes_0.8.fasta --id 1 --centroids archaea_16S_centroids_0.8.fasta --uc archaea_16S_clusters_0.8.uc --threads 24
# Reading file archaea_16S_genes_0.8.fasta 100%  
# 2290540 nt in 1553 seqs, min 1270, max 2547, avg 1475
# Masking 100%  
# Sorting by length 100%
# Counting unique k-mers 100%  
# Clustering 100%  
# Sorting clusters 100%
# Writing clusters 100%  
# Clusters: 1539 Size min 1, max 3, avg 1.0
# Singletons: 1526, 98.3% of seqs, 99.2% of clusters

vsearch --cluster_fast bacteria_16S_genes_0.8.fasta --id 1 --centroids bacteria_16S_centroids_0.8.fasta --uc bacteria_16S_clusters_0.8.uc --threads 24
# Reading file bacteria_16S_genes_0.8.fasta 100%  
# 49188070 nt in 32595 seqs, min 1268, max 1862, avg 1509
# Masking 100%  
# Sorting by length 100%
# Counting unique k-mers 100%  
# Clustering 100%  
# Sorting clusters 100%
# Writing clusters 100%  
# Clusters: 31499 Size min 1, max 35, avg 1.0
# Singletons: 30794, 94.5% of seqs, 97.8% of clusters
```

### Align sequences

ssu-align:
```{bash, eval=FALSE}
conda activate clustalo
export SSUALIGNDIR="/usr/local/share/ssu-align-0.1.1"
ssu-align --rfonly archaea_16S_centroids_0.8.fasta archaea_16S_centroids_ssu_align_0.8

ssu-align --rfonly bacteria_16S_centroids_0.8.fasta bacteria_16S_centroids_ssu_align_0.8

########################

esl-reformat -o archaea_16S_centroids_ssu_align_0.8.fna afa archaea_16S_centroids_ssu_align_0.8/archaea_16S_centroids_ssu_align_0.8.archaea.stk

esl-reformat -o bacteria_16S_centroids_ssu_align_0.8.fna afa bacteria_16S_centroids_ssu_align_0.8/bacteria_16S_centroids_ssu_align_0.8.bacteria.stk

cd ..
```

Look at these sequence lengths (unaligned):
```{python, eval=FALSE}
from Bio import SeqIO
import numpy as np

kingdom = 'bacteria'
lengths = []
for record in SeqIO.parse(kingdom+'_16S_centroids_ssu_align_0.8.fna', "fasta"):
  lengths.append(len(str(record.seq)))

print(min(lengths), max(lengths), np.mean(lengths), np.median(lengths))
#bacteria
#1508 1508 1508.0 1508.0
#archaea
#1582 1582 1582.0 1582.0
```

### Now look at choosing best genome for each cluster

Archaea:
```{python, eval=FALSE}
import os
import pandas as pd
from Bio import SeqIO
import numpy as np
import random

clusters = 'genomes_to_search_barrnap/archaea_16S_clusters_0.8.uc'
aligned_fasta = 'genomes_to_search_barrnap/archaea_16S_centroids_ssu_align_0.8.fna'
md = pd.read_csv('GTDB_r214/ar53_metadata_r214.tsv', index_col=0, header=0, sep='\t')
md = md[md['gtdb_representative'] == 't']

genes_16S = []
for record in SeqIO.parse(aligned_fasta, "fasta"):
  genes_16S.append(record.id)
  
rename_md = {}
for row in md.index.values:
  rename_md[row] = row.split('_', 1)[1]

md = md.rename(index=rename_md)

clusters_all_genomes, clusters_centroid = {}, []
for row in open(clusters, 'r'):
  row = row.replace('\n', '').split('\t')
  if row[-1] != '*':
    genomes = row[-2:]
    if genomes[0] in genes_16S:
      #print('First one', genomes)
      clusters_centroid.append(genomes[0])
      if genomes[0] in clusters_all_genomes:
        clusters_all_genomes[genomes[0]].append(genomes[1])
      else:
        clusters_all_genomes[genomes[0]] = [genomes[1]]
    elif genomes[1] in genes_16S:
      clusters_centroid.append(genomes[1])
      if genomes[1] in clusters_all_genomes:
        clusters_all_genomes[genomes[1]].append(genomes[0])
      else:
        clusters_all_genomes[genomes[1]] = [genomes[0]]
    else:
      print('Neither', genomes)

#now choose best genome from all clusters
cluster_bests = {}
for cluster in clusters_all_genomes:
  md_cluster = md.loc[[cluster]+clusters_all_genomes[cluster], :]
  completeness = md_cluster['checkm_completeness'].values
  contamination = md_cluster['checkm_contamination'].values
  best = ''
  if len(set(completeness)) == 1:
    if len(set(contamination)) == 1:
      num = int(random.choice(range(md_cluster.shape[0])))
      best = md_cluster.index.values[num]
    else:
      lowest, best = 100, ''
      for row in md_cluster.index.values:
        if md_cluster.loc[row, 'checkm_contamination'] < lowest:
          lowest, best = md_cluster.loc[row, 'checkm_contamination'], row
  else:
    highest, best = 0, ''
    for row in md_cluster.index.values:
      if md_cluster.loc[row, 'checkm_completeness'] > highest:
        highest, best = md_cluster.loc[row, 'checkm_completeness'], row
  cluster_bests[cluster] = best

#rename fasta file with the best of the cluster
new_records = []
all_ids = []
for record in SeqIO.parse(aligned_fasta, "fasta"):
  if record.id in cluster_bests:
    print('Changing', record.id, 'to', cluster_bests[record.id])
    record.id = cluster_bests[record.id]
  all_ids.append(record.id)
  new_records.append(record)

SeqIO.write(new_records, aligned_fasta.replace('.fna', '_best.fna'), "fasta")

#make file with name of best genome in cluster and other genomes within the cluster
with open('genomes_to_search_barrnap/archaea_16S_clusters_processed_0.8.txt', 'w') as f:
  w = f.write('Centroid\tBest\tAll genomes\n')
  for cluster in clusters_all_genomes:
    w = f.write(cluster+'\t'+cluster_bests[cluster]+'\t'+cluster+','+','.join(clusters_all_genomes[cluster])+'\n')
  
#get reduced metadata file including only those genomes that are the best
md = md.loc[all_ids, :]
md.to_csv('archaea_metadata_clusters_ssu_align_centroids_0.8.csv')
```

Bacteria:
```{python, eval=FALSE}
import os
import pandas as pd
from Bio import SeqIO
import numpy as np
import random

clusters = 'genomes_to_search_barrnap/bacteria_16S_clusters_0.8.uc'
aligned_fasta = 'genomes_to_search_barrnap/bacteria_16S_centroids_ssu_align_0.8.fna'
md = pd.read_csv('GTDB_r214/bac120_metadata_r214.tsv', index_col=0, header=0, sep='\t')
md = md[md['gtdb_representative'] == 't']

genes_16S = []
for record in SeqIO.parse(aligned_fasta, "fasta"):
  genes_16S.append(record.id)
  
rename_md = {}
for row in md.index.values:
  rename_md[row] = row.split('_', 1)[1]

md = md.rename(index=rename_md)

clusters_all_genomes, clusters_centroid = {}, []
for row in open(clusters, 'r'):
  row = row.replace('\n', '').split('\t')
  if row[-1] != '*':
    #print(row)
    genomes = row[-2:]
    if genomes[0] in genes_16S:
      #print('First one', genomes)
      clusters_centroid.append(genomes[0])
      if genomes[0] in clusters_all_genomes:
        clusters_all_genomes[genomes[0]].append(genomes[1])
      else:
        clusters_all_genomes[genomes[0]] = [genomes[1]]
    elif genomes[1] in genes_16S:
      clusters_centroid.append(genomes[1])
      if genomes[1] in clusters_all_genomes:
        clusters_all_genomes[genomes[1]].append(genomes[0])
      else:
        clusters_all_genomes[genomes[1]] = [genomes[0]]
    else:
      neither = True #note that this means that the sequences were removed from this step because they fitted the model of a different domain the best

#now choose best genome from all clusters
cluster_bests = {}
for cluster in clusters_all_genomes:
  md_cluster = md.loc[[cluster]+clusters_all_genomes[cluster], :]
  completeness = md_cluster['checkm_completeness'].values
  contamination = md_cluster['checkm_contamination'].values
  best = ''
  if len(set(completeness)) == 1:
    if len(set(contamination)) == 1:
      num = int(random.choice(range(md_cluster.shape[0])))
      best = md_cluster.index.values[num]
    else:
      lowest, best = 100, ''
      for row in md_cluster.index.values:
        if md_cluster.loc[row, 'checkm_contamination'] < lowest:
          lowest, best = md_cluster.loc[row, 'checkm_contamination'], row
  else:
    highest, best = 0, ''
    for row in md_cluster.index.values:
      if md_cluster.loc[row, 'checkm_completeness'] > highest:
        highest, best = md_cluster.loc[row, 'checkm_completeness'], row
  cluster_bests[cluster] = best

#rename fasta file with the best of the cluster
new_records = []
all_ids = []
for record in SeqIO.parse(aligned_fasta, "fasta"):
  if record.id in cluster_bests:
    print('Changing', record.id, 'to', cluster_bests[record.id])
    record.id = cluster_bests[record.id]
  all_ids.append(record.id)
  new_records.append(record)

SeqIO.write(new_records, aligned_fasta.replace('.fna', '_best.fna'), "fasta")

#make file with name of best genome in cluster and other genomes within the cluster
with open('genomes_to_search_barrnap/bacteria_16S_clusters_processed_0.8.txt', 'w') as f:
  w = f.write('Centroid\tBest\tAll genomes\n')
  for cluster in clusters_all_genomes:
    w = f.write(cluster+'\t'+cluster_bests[cluster]+'\t'+cluster+','+','.join(clusters_all_genomes[cluster])+'\n')
  
#get reduced metadata file including only those genomes that are the best
md = md.loc[all_ids, :]
md.to_csv('bacteria_metadata_clusters_ssu_align_centroids_0.8.csv')
```

```{bash, eval=FALSE}
mv gtdb_picrust_files gtdb_picrust_files_first_try
mkdir gtdb_picrust_files
```

### And filter the files to only include genomes <= 10% contamination and >= 90% completion

```{python}
import pandas as pd
from Bio import SeqIO
from ete3 import Tree

domains = ['bacteria', 'archaea']
trees = ['GTDB_r214/bac120_r214.tree', 'GTDB_r214/ar53_r214.tree']

for d in range(len(domains)):
  domain = domains[d]
  tree = Tree(trees[d], format=1, quoted_node_names=True)
  aligned_fasta = 'genomes_to_search_barrnap/'+domain+'_16S_centroids_ssu_align_0.8_best.fna'
  md = pd.read_csv(domain+'_metadata_clusters_ssu_align_centroids_0.8.csv', index_col=0, header=0)
  md = md[md['checkm_contamination'] <= 10]
  md = md[md['checkm_completeness'] >= 90]
  new_records = []
  for record in SeqIO.parse(aligned_fasta, "fasta"):
    if record.id in md.index.values:
      new_records.append(record)
  SeqIO.write(new_records, 'gtdb_picrust_files/'+domain+'_16S_centroids_ssu_align_best_reduced_0.8.fna', "fasta")
  pruning = []
  names = []
  for node in tree.traverse("postorder"):
    if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
      new_name = str(node.name).split('_', 1)[1]
      node.name = new_name
      names.append(new_name)
      if new_name in md.index.values:
        pruning.append(new_name)
  tree.prune(pruning)
  tree.write(outfile='gtdb_picrust_files/'+domain+'_16S_centroids_ssu_align_best_reduced_0.8.tre')
  
```

### Run the raxml check

```{bash, eval=FALSE}
cd gtdb_picrust_files
raxml-ng --check --msa archaea_16S_centroids_ssu_align_best_reduced_0.8.fna --model GTR+G --prefix archaea_0.8_raxml-check
#this didn't need fixing!
#esl-reformat -o archaea_raxml-check.raxml.reduced.phy phylip archaea_16S_centroids_ssu_align_best_reduced.fna
#but for some reason, the place_seqs step didn't work when I'd used this so I've just made the steps the same for both of them

raxml-ng --check --msa bacteria_16S_centroids_ssu_align_best_reduced_0.8.fna --model GTR+G --prefix bacteria_0.8_raxml-check
#this had some duplicates removed still apparently (copied this from previously, but outcome is the same!!)
```

Convert archaea alignment to phylip so we have the same for both:
```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment

seq_records = []
seqs, lengths = 0, []
for record in SeqIO.parse('archaea_16S_centroids_ssu_align_best_reduced_0.8.fna', 'fasta'):
  seq_records.append(record)
  seqs += 1
  lengths.append(len(str(record.seq)))
  
# count = 0
# for row in open('bacteria_0.8_raxml-check.raxml.reduced.phy', 'r'):
#   if count < 2: print([row])
#   count += 1
  
with open('archaea_0.8_raxml-check.raxml.reduced.phy', 'w') as f:
  f.write(str(seqs)+' '+str(int(lengths[0]))+'\n')
  for record in seq_records:
    f.write(record.id+' '+str(record.seq)+'\n')
  
```

### Filter the tree files to include only these

```{python, eval=FALSE}
from ete3 import Tree
import os
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
#from Bio.Alphabet import IUPAC

fixed_alignments = ['bacteria_0.8_raxml-check.raxml.reduced.phy', 'archaea_0.8_raxml-check.raxml.reduced.phy']
tree_files = ['bacteria_16S_centroids_ssu_align_best_reduced_0.8.tre', 'archaea_16S_centroids_ssu_align_best_reduced_0.8.tre']

for f in range(len(fixed_alignments)):
  fixed_alignment = fixed_alignments[f]
  tree_file = tree_files[f]
  genomes = []
  for row in open(fixed_alignment, 'r'):
    if 'GB_' in row or 'GCA_' in row or 'GCF_' in row:
      genomes.append(row.split(' ')[0].replace('>', '').replace('\n', ''))
  
  genomes = set(genomes)
  print(len(genomes))
  #29311, 
  tree = Tree(tree_file, format=1, quoted_node_names=True)
  genome_nodes = []
  for node in tree.traverse("postorder"):
    if node.name in genomes:
      genome_nodes.append(node.name)
  
  tree.prune(genome_nodes)
  tree.write(outfile=tree_file.replace('.tre', '_fixed.tre'), format=1)
```

### Run raxml

```{bash, eval=FALSE}
mkdir archaea_first_raxml
mv archaea_raxml.raxml* archaea_first_raxml/

raxml-ng --evaluate --msa archaea_0.8_raxml-check.raxml.reduced.phy --tree archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre --prefix archaea_0.8_raxml --model GTR+G —threads 24

raxml-ng --evaluate --msa bacteria_0.8_raxml-check.raxml.reduced.phy --tree bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre --prefix bacteria_0.8_raxml --model GTR+G —threads 24
```

### Resave the phylip files and reformat for the hmm files

```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment

files = ['bacteria_0.8_raxml-check.raxml.reduced.phy', 'archaea_0.8_raxml-check.raxml.reduced.phy']
new_names = ['bacteria_0.8_raxml-check.raxml.reduced.fna', 'archaea_0.8_raxml-check.raxml.reduced.fna']

for f in range(len(files)):
  sequences = {}
  seq_records = []
  count = 0
  for row in open(files[f], 'r'):
    #if count > 10: break
    if '_' in row:
      name, seq = row.replace('\n', '').split(' ')
      sequences[name] = seq
      seq_records.append(SeqRecord(Seq(seq),id=name, description=''))
    count += 1
  msa = MultipleSeqAlignment(seq_records)
  AlignIO.write(msa, new_names[f], "fasta")
```

Convert the fasta file to DNA and reformat to stockholm:
```{bash, eval=FALSE}
esl-reformat -d -o bacteria_0.8_raxml-check.raxml.reduced_dna.fna afa bacteria_0.8_raxml-check.raxml.reduced.fna

esl-reformat -o bacteria_0.8_raxml-check.raxml.reduced_dna.sto stockholm bacteria_0.8_raxml-check.raxml.reduced_dna.fna

esl-reformat -d -o archaea_0.8_raxml-check.raxml.reduced_dna.fna afa archaea_0.8_raxml-check.raxml.reduced.fna

esl-reformat -o archaea_0.8_raxml-check.raxml.reduced_dna.sto stockholm archaea_0.8_raxml-check.raxml.reduced_dna.fna
```

### Resave the archaea file and reformat for the hmm files

```{python}
# from Bio import SeqIO
# from Bio.SeqRecord import SeqRecord
# from Bio.Seq import Seq
# from Bio.Alphabet import IUPAC
# from Bio import AlignIO
# from Bio.Align import MultipleSeqAlignment
# 
# files = ['bacteria_raxml-check.raxml.reduced.phy']
# new_names = ['bacteria_raxml-check.raxml.reduced.fna']
# 
# for f in range(len(files)):
#   sequences = {}
#   seq_records = []
#   count = 0
#   for row in open(files[f], 'r'):
#     #if count > 10: break
#     if '_' in row:
#       name, seq = row.replace('\n', '').split(' ')
#       sequences[name] = seq
#       seq_records.append(SeqRecord(Seq(seq),id=name, description=''))
#     count += 1
#   msa = MultipleSeqAlignment(seq_records)
#   AlignIO.write(msa, new_names[f], "fasta")
```

### Reformat for the hmm files

```{bash, eval=FALSE}
#esl-reformat -d -o archaea_16S_centroids_ssu_align_best_reduced_dna.fna afa archaea_16S_centroids_ssu_align_best_reduced.fna

#esl-reformat -o archaea_16S_centroids_ssu_align_best_reduced_dna.sto stockholm archaea_16S_centroids_ssu_align_best_reduced_dna.fna
```


### Run the HMMs

```{bash, eval=FALSE}
#hmmbuild --cpu 24 archaea_16S_centroids_ssu_align_best_reduced_dna.hmm archaea_16S_centroids_ssu_align_best_reduced_dna.sto

#esl-reformat -o archaea_16S_centroids_ssu_align_best_reduced_dna_reformat.fna fasta archaea_16S_centroids_ssu_align_best_reduced_dna.sto

hmmbuild --cpu 24 bacteria_0.8_raxml-check.raxml.reduced_dna.hmm bacteria_0.8_raxml-check.raxml.reduced_dna.sto
hmmbuild --cpu 24 archaea_0.8_raxml-check.raxml.reduced_dna.hmm archaea_0.8_raxml-check.raxml.reduced_dna.sto
```

Get RAXML_info files:
```{bash, eval=FALSE}
conda install bioconda::raxml
raxmlHPC -g bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre -s bacteria_0.8_raxml-check.raxml.reduced_dna.fna -m GTRGAMMA -n bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed -p 13

raxmlHPC -g archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre -s archaea_0.8_raxml-check.raxml.reduced_dna.fna -m GTRGAMMA -n archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed -p 13

#having issues with running with the bacteria
#but also this is an older version
#maybe the raxmlHPC-SSE3 is a newer version? Seems to be 8.2.12

raxmlHPC-SSE3 -g bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre -s bacteria_0.8_raxml-check.raxml.reduced_dna.fna -m GTRGAMMA -n bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed -p 13

raxmlHPC-SSE3 -g archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre -s archaea_0.8_raxml-check.raxml.reduced_dna.fna -m GTRGAMMA -n archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed_SSE3 -p 13
#manually removed the "Partition: 0 with name: No Name Provided" line
#and this worked!!!

raxmlHPC-PTHREADS-SSE3 -g bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre -s bacteria_0.8_raxml-check.raxml.reduced_dna.fna -m GTRGAMMA -n bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed_SSE3 -p 13 -T 24
```



### Copy and rename the resulting files

```{bash, eval=FALSE}
cd ..
mv gtdb_picrust_ref/ gtdb_0.5_picrust_ref
mkdir gtdb_picrust_ref
#mkdir gtdb_picrust_ref/arc_ref
mkdir gtdb_picrust_ref/bac_ref
#cp gtdb_picrust_files/archaea_16S_centroids_ssu_align_best_reduced_dna_reformat.fna gtdb_picrust_ref/arc_ref/arc_ref.fna
#cp gtdb_picrust_files/archaea_16S_centroids_ssu_align_best_reduced_dna.hmm gtdb_picrust_ref/arc_ref/arc_ref.hmm
#cp gtdb_picrust_files/archaea_16S_centroids_ssu_align_best_reduced.tre gtdb_picrust_ref/arc_ref/arc_ref.tre
#cp gtdb_picrust_files/archaea_raxml.raxml.bestModel gtdb_picrust_ref/arc_ref/arc_ref.model
#mv gtdb_picrust_ref/arc_ref gtdb_picrust_ref/arc_ref_initial

cp gtdb_picrust_files/bacteria_0.8_raxml-check.raxml.reduced_dna.fna gtdb_picrust_ref/bac_ref/bac_ref.fna
cp gtdb_picrust_files/bacteria_0.8_raxml-check.raxml.reduced_dna.hmm gtdb_picrust_ref/bac_ref/bac_ref.hmm
cp gtdb_picrust_files/bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre gtdb_picrust_ref/bac_ref/bac_ref.tre
#cp gtdb_picrust_files/bacteria_0.8_raxml.raxml.log gtdb_picrust_ref/bac_ref/bac_ref.raxml_info

mkdir gtdb_picrust_ref/arc_ref
cp gtdb_picrust_files/archaea_0.8_raxml-check.raxml.reduced_dna.fna gtdb_picrust_ref/arc_ref/arc_ref.fna
cp gtdb_picrust_files/archaea_0.8_raxml-check.raxml.reduced_dna.hmm gtdb_picrust_ref/arc_ref/arc_ref.hmm
cp gtdb_picrust_files/archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre gtdb_picrust_ref/arc_ref/arc_ref.tre
cp gtdb_picrust_files/archaea_0.8_raxml.raxml.bestModel gtdb_picrust_ref/arc_ref/arc_ref.model
cp gtdb_picrust_files/RAxML_info.archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed_SSE3 gtdb_picrust_ref/arc_ref/arc_ref.raxml_info
```

## Using 0.5 sequences

### Try insertion into tree

Get the test data again:
```{bash, eval=FALSE}
cp -r gtdb_picrust_files_first_try/picrust_test/ .
#then removed all files that weren't the test data
```

Try running PICRUSt2:
```{bash, eval=FALSE}
conda activate picrust-v2.5.3
place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/placed_seqs_bac.tre -p 12 --intermediate picrust_test/placement_working_bac --ref_dir gtdb_picrust_ref/bac_ref --min_align 0.8

place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/placed_seqs_arc.tre -p 12 --intermediate picrust_test/placement_working_arc --ref_dir gtdb_picrust_ref/arc_ref --min_align 0.8
```
This works!!!!

## Filter trait tables for each of bacteria and archaea

```{bash, eval=FALSE}
mkdir gtdb_picrust_ref/bac_annotations
mkdir gtdb_picrust_ref/arc_annotations
conda deactivate
```

First filter the metadata again and make these files:
```{python, eval=FALSE}
import pandas as pd
from Bio import SeqIO

metadata = ['archaea_metadata_clusters_ssu_align_centroids.csv', 'bacteria_metadata_clusters_ssu_align_centroids.csv']
cluster_information = ['genomes_to_search_barrnap/archaea_16S_clusters_processed.txt', 'genomes_to_search_barrnap/bacteria_16S_clusters_processed.txt']
refs = ['gtdb_picrust_ref/arc_ref/arc_ref.fna', 'gtdb_picrust_ref/bac_ref/bac_ref.fna']
annotations = ['gtdb_picrust_ref/arc_annotations/', 'gtdb_picrust_ref/bac_annotations/']
copies_16S = ['genomes_to_search_barrnap/archaea_16S_copies_0.5.txt', 'genomes_to_search_barrnap/bacteria_16S_copies_0.5.txt']

for d in range(len(metadata)):
  md = pd.read_csv(metadata[d], index_col=0, header=0)
  genomes = []
  for record in SeqIO.parse(refs[d], "fasta"):
    genomes.append(record.id)
  md = md.loc[genomes, :]
  md = md.loc[:, ['checkm_completeness', 'checkm_contamination', 'checkm_marker_count', 'checkm_marker_lineage', 'gc_percentage', 'genome_size', 'gtdb_taxonomy', 'ncbi_organism_name', 'ncbi_taxid', 'ncbi_taxonomy', 'ncbi_taxonomy_unfiltered', 'ncbi_total_length', 'ssu_gg_taxonomy', 'ssu_silva_taxonomy']]
  md['16S copies'] = ''
  md['Other genomes within this 16S cluster'] = ''
  md['Cluster centroid 16S sequences'] = ''
  copy_number = pd.read_csv(copies_16S[d], sep='\t', index_col=0, header=None)
  copy_number.columns = ['16S copies']
  for row in md.index.values:
    if row in copy_number.index.values:
      md.loc[row, '16S copies'] = copy_number.loc[row, '16S copies']
  cluster = pd.read_csv(cluster_information[d], sep='\t', header=0, index_col=0)
  for row in cluster.index.values:
    if cluster.loc[row, 'Best'] in md.index.values:
      md.loc[cluster.loc[row, 'Best'], 'Cluster centroid 16S sequences'] = row
      md.loc[cluster.loc[row, 'Best'], 'Other genomes within this 16S cluster'] = cluster.loc[row, 'All genomes']
  md.to_csv(annotations[d]+'metadata.csv')

for d in range(len(metadata)):
  md = pd.read_csv(annotations[d]+'metadata.csv', index_col=0, header=0)
  md = md.loc[:, ['16S copies']]
  md = md.reset_index()
  md.columns = ['assembly', '16S_rRNA_Count']
  md = md.set_index('assembly')
  md.to_csv(annotations[d]+'16S.txt', sep='\t')
```

Now get the trait data:
```{python, eval=FALSE}
import os
import pandas as pd
from Bio import SeqIO

out_folders = ['gtdb_picrust_ref/arc_annotations/', 'gtdb_picrust_ref/bac_annotations/']
annotations = ['picrust_annotations/'+f for f in os.listdir('picrust_annotations/') if '_all.txt' in f]
annotations = ['picrust_annotations/KEGG_ko_all.txt', 'picrust_annotations/Preferred_name_all.txt', 'picrust_annotations/PFAMs_all.txt', 'picrust_annotations/GOs_all.txt', 'picrust_annotations/CAZy_all.txt', 'picrust_annotations/BiGG_Reaction_all.txt']
for a in range(len(annotations)):
  print(annotations[a])
  annot_df = pd.read_csv(annotations[a], index_col=0, header=0, sep='\t').transpose()
  for d in range(len(out_folders)):
    print(out_folders[d])
    md = pd.read_csv(out_folders[d]+'metadata.csv', index_col=0, header=0)
    annot_red = annot_df.copy(deep=True)
    annot_red = annot_red.loc[md.index.values, :]
    annot_red = annot_red[annot_red.max(axis=1) > 0]
    annot_red.index.name = 'assembly'
    annot_red.to_csv(out_folders[d]+annotations[a].split('/')[1].replace('_all', ''), sep='\t')
```

## Now try hidden state prediction

```{bash, eval=FALSE}
conda activate picrust-v2.5.3

hsp.py -t picrust_test/placed_seqs_bac.tre --observed_trait_table gtdb_picrust_ref/bac_annotations/EC.txt -o picrust_test/EC_predicted_bac.tsv.gz -p 12 -n

#Maybe I need to stop running this separately and just run the full pipeline?

picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/EC.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/16S.txt --no_pathways --no_regroup --verbose

#Error - at least one trait in the prediction table was entirely missing values.
#Execution halted

gunzip gtdb_picrust_ref/bac_annotations/*.txt

picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_gz -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/EC.txt.gz --marker_gene_table gtdb_picrust_ref/bac_annotations/16S.txt.gz --no_pathways --no_regroup --verbose

#same error
gunzip gtdb_picrust_ref/bac_annotations/*.txt.gz
```

```{python}
import pandas as pd

ec = pd.read_csv('gtdb_picrust_ref/bac_annotations/EC.txt', index_col=0, header=0, sep='\t')
print(ec)
ec = ec.transpose()
ec = ec[ec.max(axis=1) > 0]
print(ec)
ec = ec.transpose()
ec.to_csv('gtdb_picrust_ref/bac_annotations/EC_fixed.txt')
```

```{bash, eval=FALSE}
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_fix -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/EC_fixed.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/16S.txt --no_pathways --no_regroup --verbose

#Error running this command:                                                                                          #                      
#hsp.py --tree picrust_test/picrust2_out_pipeline_bac_fix/out.tre --output picrust_test/picrust2_out_pipeline_bac_fix/marker_predicted_and_nsti.tsv.gz --observed_trait_table gtdb_picrust_ref/bac_annotations/16S.txt --hsp_method mp --edge_exponent 0.5 --seed 100 --calculate_NSTI --processes 1 --verbose   

#Standard error of the above failed command:
#Rscript /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_nsti.R picrust_test/picrust2_out_pipeline_bac_fix/out.tre /tmp/tmpegvm4nsd/known_tips.txt /tmp/tmpegvm4nsd/nsti_out.txt
#Warning messages:
#1: package ‘castor’ was built under R version 4.4.1 
#2: package ‘Rcpp’ was built under R version 4.4.1 

#Rscript /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R picrust_test/picrust2_out_pipeline_bac_fix/out.tre /tmp/tmp12d3t2rh/subset_tab_0 mp 0.5 FALSE FALSE /tmp/tmpd3f6l4zq/predicted_counts.txt /tmp/tmpd3f6l4zq/predicted_ci.txt 100

#Error running this command:
#Rscript /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R picrust_test/picrust2_out_pipeline_bac_fix/out.tre /tmp/tmp12d3t2rh/subset_tab_0 mp 0.5 FALSE FALSE /tmp/tmpd3f6l4zq/predicted_counts.txt /tmp/tmpd3f6l4zq/predicted_ci.txt 100

#Standard error of the above failed command:
#Warning messages:
#1: package ‘castor’ was built under R version 4.4.1 
#2: package ‘Rcpp’ was built under R version 4.4.1 
#Error: 
#Error - at least one trait in the prediction table was entirely missing values.
#Execution halted
```

OK, so I need to go through the whole PICRUSt2 script process again... taking parts of the wrap_hsp.py script (specifically castor_hsp_workflow) to test:
```{python, eval=FALSE}
from os import path
import sys
import pandas as pd
from math import ceil
from joblib import Parallel, delayed
from picrust2.util import system_call_check, TemporaryDirectory
import os

# #hsp.py --tree picrust_test/picrust2_out_pipeline_bac_fix/out.tre --output picrust_test/picrust2_out_pipeline_bac_fix/marker_predicted_and_nsti.tsv.gz --observed_trait_table gtdb_picrust_ref/bac_annotations/16S.txt --hsp_method mp --edge_exponent 0.5 --seed 100 --calculate_NSTI --processes 1 --verbose   
# 
# trait_tab = pd.read_csv('gtdb_picrust_ref/bac_annotations/16S.txt', sep="\t", dtype={'assembly': str})
# trait_tab.set_index('assembly', drop=True, inplace=True)
# 
# #nsti_values = castor_nsti(tree_path, trait_tab.index.values, verbose)
# #castor_nsti(tree_path, known_tips, verbose)
# known_tips = trait_tab.index.values
# tree_path = 'picrust_test/picrust2_out_pipeline_bac_fix/out.tre'
# castor_nsti_script = path.join('/home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_nsti.R')
# known_tips_out = path.join('picrust_test/picrust2_out_pipeline_bac_fix/intermediate', "known_tips.txt")
# known_tips.tofile(known_tips_out, sep="\n")
# nsti_tmp_out = path.join('picrust_test/picrust2_out_pipeline_bac_fix/intermediate', "nsti_out.txt")
# system_call_check(" ".join(["Rscript", castor_nsti_script, tree_path, known_tips_out, nsti_tmp_out]), print_command=True, print_stdout=True, print_stderr=True)
# nsti_out = pd.read_csv(nsti_tmp_out, sep="\t", dtype={'sequence': str})
# nsti_out.set_index('sequence', drop=True, inplace=True)
# if len(known_tips) != nsti_out.shape[0]:
#   print("Number of rows in returned NSTI table is incorrect.")
#   
# print(len(known_tips))

castor_hsp_workflow('picrust_test/picrust2_out_pipeline_bac_fix/out.tre', 'gtdb_picrust_ref/bac_annotations/16S.txt', 'mp', calc_nsti=True, num_proc=12, verbose=True, check_input=True)

def castor_hsp_workflow(tree_path,
                        trait_table_path,
                        hsp_method,
                        edge_exponent=0.5,
                        chunk_size=500,
                        calc_nsti=False,
                        calc_ci=False,
                        check_input=False,
                        num_proc=1,
                        ran_seed=None,
                        verbose=False):
    '''Runs full HSP workflow. Main purpose is to read in trait table and run
    HSP on subsets of column at a time to be more memory efficient. Will return
    a single table of predictions and also a table of CIs (if specified).'''
    # Check edge exponent input.
    if edge_exponent < 0:
        sys.exit("Stopping - edge_exponent setting must be >= 0.")
    # Read in trait table as pandas dataframe.
    trait_tab = pd.read_csv(trait_table_path, sep="\t", dtype={'assembly': str})
    trait_tab.set_index('assembly', drop=True, inplace=True)
    # Calculate NSTI values if option set.
    if calc_nsti:
        nsti_values = castor_nsti(tree_path, trait_tab.index.values, verbose)
    # Create output directory for writing trait table subsets.
    with TemporaryDirectory() as temp_dir:
        num_chunks = int(trait_tab.shape[1]) / (chunk_size + 1)
        # Get all table subsets and write to file.
        # Also keep list of all temporary filenames.
        file_subsets = []
        for i in range(ceil(num_chunks)):
            subset_file = path.join('picrust_test/picrust2_out_pipeline_bac_fix/intermediate/', "subset_tab_" + str(i))
            subset_tab = trait_tab.iloc[:, i * chunk_size:(i + 1) * chunk_size]
            subset_tab.to_csv(path_or_buf=subset_file,
                              index_label="assembly",
                              sep="\t")
            file_subsets.append(subset_file)
        castor_out_raw = Parallel(n_jobs=num_proc)(delayed(
                                    castor_hsp_wrapper)(tree_path,
                                                        trait_in,
                                                        hsp_method,
                                                        edge_exponent,
                                                        calc_ci,
                                                        check_input,
                                                        ran_seed,
                                                        verbose)
                                    for trait_in in file_subsets)
    # Get lists of predictions and CIs for all chunks.
    predict_out_chunks = []
    ci_out_chunks = []
    for i in range(len(castor_out_raw)):
        predict_out_chunks.append(castor_out_raw[i][0])
        ci_out_chunks.append(castor_out_raw[i][1])
    predict_out_combined = pd.concat(predict_out_chunks, axis=1, sort=True)
    # Add NSTI as column as well if option specified.
    if calc_nsti:
        predict_out_combined = pd.concat([predict_out_combined, nsti_values],
                                         axis=1, sort=True)
    ci_out_combined = None
    if calc_ci:
        ci_out_combined = pd.concat(ci_out_chunks, axis=1, sort=True)
    return(predict_out_combined, ci_out_combined)


def castor_hsp_wrapper(tree_path, trait_tab, hsp_method, edge_exponent=0.5,
                       calc_ci=False, check_input=False, ran_seed=None,
                       verbose=False):
    '''Wrapper for making system calls to castor_hsp.py Rscript.'''
    castor_hsp_script = path.join('/home/robyn/tools/picrust2-2.5.3/picrust2/',
                                  'Rscripts', 'castor_hsp.R')
    # Need to format boolean setting as string for R to read in as argument.
    if calc_ci:
        calc_ci_setting = "TRUE"
    else:
        calc_ci_setting = "FALSE"
    if check_input:
        check_input_setting = "TRUE"
    else:
        check_input_setting = "FALSE"
    # Create temporary directory for writing output files of castor_hsp.R
    with TemporaryDirectory() as temp_dir:
        output_count_path = path.join('picrust_test/picrust2_out_pipeline_bac_fix/intermediate/', "predicted_counts.txt")
        output_ci_path = path.join('picrust_test/picrust2_out_pipeline_bac_fix/intermediate/', "predicted_ci.txt")
        hsp_cmd = " ".join(["Rscript",
                            castor_hsp_script,
                            tree_path,
                            trait_tab,
                            hsp_method,
                            str(edge_exponent),
                            calc_ci_setting,
                            check_input_setting,
                            output_count_path,
                            output_ci_path,
                            str(ran_seed)])
        # Run castor_hsp.R
        system_call_check(hsp_cmd, print_command=verbose,
                          print_stdout=verbose, print_stderr=verbose)
        # Load the output into Table objects
        try:
            asr_table = pd.read_csv(filepath_or_buffer=output_count_path,
                                    sep="\t", dtype={'sequence': str})
            asr_table.set_index('sequence', drop=True, inplace=True)
        except IOError:
            raise print("Cannot read in expected output file" +
                             output_ci_path)
        if calc_ci:
            asr_ci_table = pd.read_csv(filepath_or_buffer=output_ci_path,
                                       sep="\t", dtype={'sequence': str})
            asr_ci_table.set_index('sequence', drop=True, inplace=True)
        else:
            asr_ci_table = None
    # Return list with predicted counts and CIs.
    return [asr_table, asr_ci_table]


def castor_nsti(tree_path,
                known_tips,
                verbose):
    '''Will calculate distance from each study sequence to the closest
    reference sequence. Takes in the path to treefile and the known tips
    (i.e. the rownames in the trait table - the reference genome ids).'''
    castor_nsti_script = path.join('/home/robyn/tools/picrust2-2.5.3/picrust2/',
                                   'Rscripts', 'castor_nsti.R')
    # Create temporary directory for working in.
    with TemporaryDirectory() as temp_dir:
        # Output known tip names to temp file
        # (note this object is a numpy.ndarray)
        known_tips_out = path.join('picrust_test/picrust2_out_pipeline_bac_fix/intermediate/', "known_tips.txt")
        known_tips.tofile(known_tips_out, sep="\n")
        nsti_tmp_out = path.join('picrust_test/picrust2_out_pipeline_bac_fix/intermediate/', "nsti_out.txt")
        # Run Rscript.
        system_call_check(" ".join(["Rscript",
                                    castor_nsti_script,
                                    tree_path,
                                    known_tips_out,
                                    nsti_tmp_out]),
                          print_command=verbose,
                          print_stdout=verbose,
                          print_stderr=verbose)
        # Read in calculated NSTI values.
        nsti_out = pd.read_csv(nsti_tmp_out, sep="\t", dtype={'sequence': str})
        nsti_out.set_index('sequence', drop=True, inplace=True)
    # Make sure that the table has the correct number of rows.
    if len(known_tips) != nsti_out.shape[0]:
        print("Number of rows in returned NSTI table is incorrect.")
    return(nsti_out)
  

```

Run regular PICRUSt2 but with verbose option on:
```{bash, eval=FALSE}
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_default -p 12 --no_pathways --no_regroup --verbose

#This worked fine so now I'll try replacing default files with my files:
mv /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic_default
mkdir /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/
mkdir /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/pro_ref
cp gtdb_picrust_ref/bac_ref/bac_ref.fna /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/pro_ref/pro_ref.fna
cp gtdb_picrust_ref/bac_ref/bac_ref.hmm /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/pro_ref/pro_ref.hmm
cp gtdb_picrust_ref/bac_ref/bac_ref.model /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/pro_ref/pro_ref.model
cp gtdb_picrust_ref/bac_ref/bac_ref.tre /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/pro_ref/pro_ref.tre
cp gtdb_picrust_ref/bac_annotations/EC_fixed.txt /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/ec.txt
cp gtdb_picrust_ref/bac_annotations/16S.txt /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/16S.txt
cp gtdb_picrust_ref/bac_annotations/KEGG_ko.txt /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/ko.txt
gzip /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/ec.txt
gzip /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/16S.txt
gzip /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/prokaryotic/ko.txt

cd /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/
cp prokaryotic_default/cog.txt.gz prokaryotic/
cp prokaryotic_default/pfam.txt.gz prokaryotic/
cp prokaryotic_default/pheno.txt.gz prokaryotic/
cp prokaryotic_default/tigrfam.txt.gz prokaryotic/
cd /bigpool/robyn/picrust2_database

picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_default_replaced -p 12 --no_pathways --no_regroup --verbose
#still get the same issue! So obviously not this.
cd /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/
#seems like ec has saved badly. I'll try running only ko as this seems to be OK
cd /bigpool/robyn/picrust2_database
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_default_replaced_ko -p 12 --no_pathways --no_regroup --verbose --in_traits KO
```

There is obviously something wrong with the trait tables that I have made... try looking at both:
```{python, eval=FALSE}
#cd /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/
#gunzip prokaryotic_default/16S.txt.gz
#gunzip prokaryotic/16S.txt.gz
import pandas as pd
from ete3 import Tree

original = 'prokaryotic_default/16S.txt'
new = 'prokaryotic/16S.txt'

count = 0
rows_orig = []
for row in open(original, 'r'):
  rows_orig.append(row)
  count += 1
  
count = 0
rows_new = []
for row in open(new, 'r'):
  rows_new.append(row)
  count += 1

new_df = pd.read_csv(new, index_col=0, header=0, sep='\t')
orig_df = pd.read_csv(original, index_col=0, header=0, sep='\t')
print(new_df)
print(new_df[new_df.max(axis=1) > 0])

tree = Tree('prokaryotic_default/pro_ref/pro_ref.tre', format=1, quoted_node_names=True)
for node in tree.traverse("postorder"):
  if node.name not in orig_df.index.values:
    print([node.name])

```

Looking at the two trees, the default one appears to have no node names aside from the genomes. Maybe I need to do this?

```{python, eval=FALSE}
#mv prokaryotic/pro_ref/pro_ref.tre prokaryotic/pro_ref/pro_ref_original.tre 
from ete3 import Tree
import pandas as pd

new = 'prokaryotic/16S.txt'
new_df = pd.read_csv(new, index_col=0, header=0, sep='\t')
tree = Tree('prokaryotic/pro_ref/pro_ref_original.tre', format=1, quoted_node_names=True)
for node in tree.traverse("postorder"):
  if node.name not in new_df.index.values:
    node.name = ''
    
tree.write(outfile='prokaryotic/pro_ref/pro_ref.tre', format=1)
```

```{bash, eval=FALSE}
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_default_replaced_tree_rename -p 12 --no_pathways --no_regroup --verbose --in_traits KO

mv castor_hsp.R castor_hsp_original.R
cp castor_hsp_original.R castor_hsp.R
vi castor_hsp.R
#edited castor_hsp.R so that it gives a warning rather than stopping... at least if I get an output of the file then I can hopefully look and see what the issue is!
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_default_replaced_tree_rename_R_edit -p 12 --no_pathways --no_regroup --verbose --in_traits KO
#changing this does make it run, and the column with 16S copy numbers is completely empty. So it obviously struggles somehow with this... lets check the archaea one?

picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_arc -p 12 --ref_dir gtdb_picrust_ref/arc_ref/ --custom_trait_tables gtdb_picrust_ref/arc_annotations/KEGG_ko.txt --marker_gene_table gtdb_picrust_ref/arc_annotations/16S.txt --no_pathways --no_regroup --verbose

#for archaea this works fine... maybe to do with the large 16S copy numbers?
```

Move back the files to how they were:
```{bash, eval=FALSE}
cd /home/robyn/tools/picrust2-2.5.3/picrust2/default_files/
mv prokaryotic /bigpool/robyn/picrust2_database/prokaryotic_default_replaced_didnt_work
mv prokaryotic_default prokaryotic

cd /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts
rm castor_hsp.R
mv castor_hsp_original.R castor_hsp.R
cd /bigpool/robyn/picrust2_database/
```

Modify the 16S file to just have 1 copy per genome:
```{python, eval=FALSE}
import pandas as pd

copy_16S = pd.read_csv('gtdb_picrust_ref/bac_annotations/16S.txt', index_col=0, header=0, sep='\t')
for row in copy_16S.index.values:
  try: 
    n = float(copy_16S.loc[row, '16S_rRNA_Count'])
  except:
    print(row, copy_16S.loc[row, '16S_rRNA_Count'])
  #nothing printed here, so they clearly are all numbers
  copy_16S.loc[row, '16S_rRNA_Count'] = 1

copy_16S.to_csv('gtdb_picrust_ref/bac_annotations/16S_single_copy.txt', sep='\t')
```

Try again:
```{python, eval=FALSE}
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_single_16S -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/KEGG_ko.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/16S_single_copy.txt --no_pathways --no_regroup --verbose

#got a different error this time...
# Finished placing sequences on output tree: picrust_test/picrust2_out_pipeline_bac_single_16S/out.tre                                       
# hsp.py --tree picrust_test/picrust2_out_pipeline_bac_single_16S/out.tre --output picrust_test/picrust2_out_pipeline_bac_single_16S/marker_p
# redicted_and_nsti.tsv.gz --observed_trait_table gtdb_picrust_ref/bac_annotations/16S_single_copy.txt --hsp_method mp --edge_exponent 0.5 --
# seed 100 --calculate_NSTI --processes 1 --verbose                                                                                          
#                                                                                                                                            
# Error running this command:                                                                                                                
# hsp.py --tree picrust_test/picrust2_out_pipeline_bac_single_16S/out.tre --output picrust_test/picrust2_out_pipeline_bac_single_16S/marker_p
# redicted_and_nsti.tsv.gz --observed_trait_table gtdb_picrust_ref/bac_annotations/16S_single_copy.txt --hsp_method mp --edge_exponent 0.5 --seed 100 --calculate_NSTI --processes 1 --verbose
# 
# Standard output of the above failed command:
# 
# 
# 
# Standard error of the above failed command:
# Rscript /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_nsti.R picrust_test/picrust2_out_pipeline_bac_single_16S/out.tre /tmp/tmp6jcwijy9/known_tips.txt /tmp/tmp6jcwijy9/nsti_out.txt
# Warning messages:
# 1: package ‘castor’ was built under R version 4.4.1 
# 2: package ‘Rcpp’ was built under R version 4.4.1 
# 
# Rscript /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R picrust_test/picrust2_out_pipeline_bac_single_16S/out.tre /tmp/tmpo
# 290vbpg/subset_tab_0 mp 0.5 FALSE FALSE /tmp/tmpbw6zwe3l/predicted_counts.txt /tmp/tmpbw6zwe3l/predicted_ci.txt 100
# 
# Error running this command:
# Rscript /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R picrust_test/picrust2_out_pipeline_bac_single_16S/out.tre /tmp/tmpo
# 290vbpg/subset_tab_0 mp 0.5 FALSE FALSE /tmp/tmpbw6zwe3l/predicted_counts.txt /tmp/tmpbw6zwe3l/predicted_ci.txt 100
# 
# Standard error of the above failed command:
# Warning messages:
# 1: package ‘castor’ was built under R version 4.4.1 
# 2: package ‘Rcpp’ was built under R version 4.4.1 
# Error in matrix(0, nrow = Nclades, ncol = Nstates - common_state) : 
#   invalid 'ncol' value (too large or NA)
# Calls: lapply ... mp_study_probs -> hsp_max_parsimony -> cbind -> matrix
# Execution halted
```

## Using 0.8 sequences

```{bash, eval=FALSE}
mv picrust_test picrust_0.5_test
mkdir picrust_test
cp -r picrust_0.5_test/chemerin_16S picrust_test/
```

Try the tree insertion:
```{bash, eval=FALSE}
conda activate picrust-v2.5.3
place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/placed_seqs_bac.tre -p 12 --intermediate picrust_test/placement_working_bac --ref_dir gtdb_picrust_ref/bac_ref --min_align 0.8

place_seqs.py -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/placed_seqs_arc.tre -p 12 --intermediate picrust_test/placement_working_arc --ref_dir gtdb_picrust_ref/arc_ref --min_align 0.8
```
This works!!!!

## Filter trait tables for each of bacteria and archaea

```{bash, eval=FALSE}
mkdir gtdb_picrust_ref/bac_annotations
mkdir gtdb_picrust_ref/arc_annotations
conda deactivate
```

First filter the metadata again and make these files:
```{python, eval=FALSE}
import pandas as pd
from Bio import SeqIO

metadata = ['archaea_metadata_clusters_ssu_align_centroids_0.8.csv', 'bacteria_metadata_clusters_ssu_align_centroids_0.8.csv']
cluster_information = ['genomes_to_search_barrnap/archaea_16S_clusters_processed_0.8.txt', 'genomes_to_search_barrnap/bacteria_16S_clusters_processed_0.8.txt']
refs = ['gtdb_picrust_ref/arc_ref/arc_ref.fna', 'gtdb_picrust_ref/bac_ref/bac_ref.fna']
annotations = ['gtdb_picrust_ref/arc_annotations/', 'gtdb_picrust_ref/bac_annotations/']
copies_16S = ['genomes_to_search_barrnap/archaea_16S_copies_0.8.txt', 'genomes_to_search_barrnap/bacteria_16S_copies_0.8.txt']

for d in range(len(metadata)):
  md = pd.read_csv(metadata[d], index_col=0, header=0)
  genomes = []
  for record in SeqIO.parse(refs[d], "fasta"):
    genomes.append(record.id)
  md = md.loc[genomes, :]
  md = md.loc[:, ['checkm_completeness', 'checkm_contamination', 'checkm_marker_count', 'checkm_marker_lineage', 'gc_percentage', 'genome_size', 'gtdb_taxonomy', 'ncbi_organism_name', 'ncbi_taxid', 'ncbi_taxonomy', 'ncbi_taxonomy_unfiltered', 'ncbi_total_length', 'ssu_gg_taxonomy', 'ssu_silva_taxonomy']]
  md['16S copies'] = ''
  md['Other genomes within this 16S cluster'] = ''
  md['Cluster centroid 16S sequences'] = ''
  copy_number = pd.read_csv(copies_16S[d], sep='\t', index_col=0, header=None)
  copy_number.columns = ['16S copies']
  for row in md.index.values:
    if row in copy_number.index.values:
      md.loc[row, '16S copies'] = copy_number.loc[row, '16S copies']
  cluster = pd.read_csv(cluster_information[d], sep='\t', header=0, index_col=0)
  for row in cluster.index.values:
    if cluster.loc[row, 'Best'] in md.index.values:
      md.loc[cluster.loc[row, 'Best'], 'Cluster centroid 16S sequences'] = row
      md.loc[cluster.loc[row, 'Best'], 'Other genomes within this 16S cluster'] = cluster.loc[row, 'All genomes']
  md.to_csv(annotations[d]+'metadata.csv')

for d in range(len(metadata)):
  md = pd.read_csv(annotations[d]+'metadata.csv', index_col=0, header=0)
  md = md.loc[:, ['16S copies']]
  md = md.reset_index()
  md.columns = ['assembly', '16S_rRNA_Count']
  md = md.set_index('assembly')
  md.to_csv(annotations[d]+'16S.txt', sep='\t')
```

### Fix annotations

Seems like I need to fix the annotations first. Some in the ECs clearly are gene names (or similar), and some in the KOs appear to be ECs, etc. 
Going to redo some collation. Can do this for only the genomes in the references though?
```{python, eval=FALSE}
import os
import pandas as pd
import pickle
from multiprocessing import Pool
from multiprocessing import freeze_support
from multiprocessing import Process, Manager

bacteria = pd.read_csv('gtdb_picrust_ref/bac_annotations/metadata.csv', index_col=0, header=0)
archaea = pd.read_csv('gtdb_picrust_ref/arc_annotations/metadata.csv', index_col=0, header=0)
genomes = list(set(bacteria.index.values))+list(set(archaea.index.values))

eggnog_out = os.listdir('eggnog_out_consolidated/')
eggnog_out = [e for e in eggnog_out if e.replace('.emapper.annotations', '') in genomes]

column_names = ['Preferred_name', 'GOs', 'EC', 'KEGG_ko', 'CAZy', 'BiGG_Reaction', 'PFAMs']
columns = [8, 9, 10, 11, 18, 19, 20]
lists = {'Preferred_name':set(), 'GOs':set(), 'EC':set(), 'KEGG_ko':set(), 'CAZy':set(), 'BiGG_Reaction':set(), 'PFAMs':set()}

# count = 0
# for genome in eggnog_out:
#   if count % 10 == 0: print(count)
#   count2 = 0
#   for row in open('eggnog_out_consolidated/'+genome, 'r'):
#     if count2 > 4: 
#       row = row.replace('\n', '').split('\t')
#       try:
#         for c in range(len(columns)):
#           #lists[column_names[c]] += row[columns[c]]+','
#           #lists[column_names[c]].append(row[columns[c]])
#           val = row[columns[c]]
#           if ',' in val:
#             val = val.split(',')
#             for v in val:
#               lists[column_names[c]].add(v)
#           else:
#             lists[column_names[c]].add(val)
#       except:
#         couldnt_do = True
#       #print(len(row.split('\t')), row.split('\t'))
#     #if count2 > 10: break
#     count2 += 1
#   count += 1
#   
# with open('gtdb_picrust_ref/eggnog_lists.dict', 'wb') as f:
#     pickle.dump(lists, f)
with open('gtdb_picrust_ref/eggnog_lists.dict', 'rb') as f:
    lists = pickle.load(f)

#check format of lists
for l in lists:
  print(l, len(lists[l]), list(lists[l])[:10])
    
for l in lists:
  if l == 'GOs':
    for gene in lists[l]:
      if 'GO:' not in gene: 
        print(gene)
  if l == 'EC':
    for gene in lists[l]:
      if gene.count('.') < 3:
        print(gene)
  if l == 'KEGG_ko':
    for gene in lists[l]:
      if 'ko:' not in gene:
        print(gene)

#this is only giving '-' for each of these, so I think that I'm safe to use the lists

# for l in lists:
#   with open('gtdb_picrust_ref/'+l+'_present_in_database.txt', 'w') as f:
#     for gene in lists[l]:
#       if gene != '-':
#         w = f.write(gene+'\n')

# def add_to_file(fn, line):
#   with open(fn, 'a') as f:
#     w = f.write('\t'.join(line)+'\n')
#   return
# 
# def make_files(fn, line):
#   with open(fn, 'w') as f:
#     w = f.write('\t'.join(line)+'\n')
#   return
def add_to_file(fn, line):
  with open(fn, 'a') as f:
    w = f.write(line)
  return

def make_files(fn, line):
  with open(fn, 'w') as f:
    w = f.write(line)
  return

def process_genome(genome):
  print(genome)
  this_genome = {}
  for l in lists:
    this_genome[l] = []
  count2 = 0
  for row in open('eggnog_out_consolidated/'+genome, 'r'):
    if count2 > 4:
      row = row.replace('\n', '').split('\t')
      try:
        for c in range(len(columns)):
          val = row[columns[c]]
          if ',' in val:
            val = val.split(',')
            for v in val:
              this_genome[column_names[c]].append(v)
          else:
            this_genome[column_names[c]].append(val)
      except:
        couldnt_do = True
    count2 += 1
  for l in lists:
    this_genome_counts = []
    for gene in lists[l]:
      this_genome_counts.append(this_genome[l].count(gene))
    this_genome_counts = [str(int(s)) for s in this_genome_counts]
    this_genome_counts = genome.replace('.emapper.annotations', '')+'\t'+'\t'.join(this_genome_counts)+'\n'
    add_to_file('gtdb_picrust_ref/'+l+'.txt', this_genome_counts)
    #add_to_file('gtdb_picrust_ref/'+l+'.txt', [genome.replace('.emapper.annotations', '')]+[str(int(s)) for s in this_genome_counts])
    #print(len(this_genome_counts), this_genome_counts[-10:])
    #print([genome.replace('.emapper.annotations', '')+'\t'.join(this_genome_counts[-100:])+'\n'])
  return

for l in lists:
  make_files('gtdb_picrust_ref/'+l+'.txt', 'assembly\t'+'\t'.join(list(lists[l]))+'\n')
  
def run_multiprocessing(func, i, n_processors):
    with Pool(processes=n_processors) as pool:
        return pool.map(func, i)

def main():
    print('Starting processing')
    run_multiprocessing(process_genome, eggnog_out, 24)

if __name__ == "__main__":
    freeze_support()   # required to use multiprocessing
    main()

###CHECK!!!
ec = pd.read_csv('gtdb_picrust_ref/EC.txt', index_col=0, header=0, sep='\t')
ec.isnull().values.any()
#False
```

### Try PICRUSt2

See whether we need separate tables for bacteria and archaea:
```{bash, eval=FALSE}
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/EC.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/16S.txt --no_pathways --no_regroup --verbose
#again get the "at least one trait in the prediction table was missing values" error

picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_arc -p 12 --ref_dir gtdb_picrust_ref/arc_ref/ --custom_trait_tables gtdb_picrust_ref/EC.txt --marker_gene_table gtdb_picrust_ref/arc_annotations/16S.txt --no_pathways --no_regroup --verbose
#Error: Fewer than 10% of reference ids in the function abundance table are in the tree. This could be because the ids are slightly different between the table and the tree.
```

### Make separate bacteria and archaea annotation tables

So I guess we do need separate tables:
```{python}
import pandas as pd

for function in ['BiGG_Reaction.txt', 'CAZy.txt', 'EC.txt', 'GOs.txt', 'KEGG_ko.txt', 'PFAMs.txt', 'Preferred_name.txt']:
  print(function)
  function_df = pd.read_csv('gtdb_picrust_ref/'+function, header=0, index_col=0, sep='\t')
  for domain in ['bac', 'arc']:
    print(domain)
    metadata = pd.read_csv('gtdb_picrust_ref/'+domain+'_annotations/metadata.csv', index_col=0, header=0)
    function_df_domain = function_df.copy(deep=True)
    function_df_domain = function_df_domain.loc[metadata.index.values, :]
    function_df_domain = function_df_domain.loc[:, (function_df_domain != 0).any(axis=0)]
    function_df_domain.to_csv('gtdb_picrust_ref/'+domain+'_annotations/'+function, sep='\t')
```

Double check the tables:
```{python, eval=FALSE}
import pandas as pd
from ete3 import Tree

tree_file = 'picrust_test/picrust2_out_pipeline_bac/out.tre'
copies_16S = pd.read_csv('gtdb_picrust_ref/bac_annotations/16S.txt', index_col=0, header=0, sep='\t')
genomes = set(copies_16S.index.values)
print(len(genomes))
#26868

genomes_in_tree = []
tree = Tree(tree_file, format=1, quoted_node_names=True)
for node in tree.traverse("postorder"):
  if '_' in node.name:
    genomes_in_tree.append(node.name)

print(len(genomes_in_tree))
#26868
```

### Remove internal nodes from tree

Maybe the trees want to have their internal nodes removed.
```{bash, eval=FALSE}
mv gtdb_picrust_ref/bac_ref/bac_ref.tre gtdb_picrust_ref/bac_ref/bac_ref_internal.tre
mv gtdb_picrust_ref/arc_ref/arc_ref.tre gtdb_picrust_ref/arc_ref/arc_ref_internal.tre
```

```{python, eval=FALSE}
from ete3 import Tree

bac_tree = 'gtdb_picrust_ref/bac_ref/bac_ref_internal.tre'
tree = Tree(bac_tree, format=1, quoted_node_names=True)
for node in tree.traverse("postorder"):
  if '_' not in node.name:
    node.name = ''

tree.write(outfile='gtdb_picrust_ref/bac_ref/bac_ref.tre', format=1)

arc_tree = 'gtdb_picrust_ref/arc_ref/arc_ref_internal.tre'
tree = Tree(arc_tree, format=1, quoted_node_names=True)
for node in tree.traverse("postorder"):
  if '_' not in node.name:
    node.name = ''

tree.write(outfile='gtdb_picrust_ref/arc_ref/arc_ref.tre', format=1)
```

Try PICRUSt2 again:
```{bash, eval=FALSE}
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_arc_2 -p 12 --ref_dir gtdb_picrust_ref/arc_ref/ --custom_trait_tables gtdb_picrust_ref/arc_annotations/EC.txt --marker_gene_table gtdb_picrust_ref/arc_annotations/16S.txt --no_pathways --no_regroup --verbose
#new error!
# Error running this command:
# Rscript /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R picrust_test/picrust2_out_pipeline_arc_2/out.tre /tmp/tmpc_phcdqb/subset_tab_4 mp 0.5 FALSE FALSE /tmp/tmplvugesc0/predicted_counts.txt /tmp/tmplvugesc0/predicted_ci.txt 100
# 
# Standard error of the above failed command:
# Warning messages:
# 1: package ‘castor’ was built under R version 4.4.1 
# 2: package ‘Rcpp’ was built under R version 4.4.1 
# Error in `rownames<-`(`*tmp*`, value = tree_tips[study_tips_i]) : 
#   attempt to set 'rownames' on an object with no dimensions
# Calls: lapply ... FUN -> mp_study_probs -> get_sorted_prob -> rownames<-
# Execution halted
#
#And we have the marker_predicted_and_nsti file!!!
#But maybe the issue is that all of the NSTI's are >3?

picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_2 -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/EC.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/16S.txt --no_pathways --no_regroup --verbose
#same error here :(
# Error - at least one trait in the prediction table was entirely missing values.
# Execution halted
```

### Troubleshoot bacteria

Troubleshoot bacteria:
```{python, eval=FALSE}
import pandas as pd
import math

copies_16S = pd.read_csv('bac_annotations/16S.txt', index_col=0, header=0, sep='\t')
copies_16S.isnull().values.any()
#False
#so none of the values aren't numbers, so they all should be fine??
#see if the same is true for the EC numbers, although I don't think that we got to that point

EC = pd.read_csv('bac_annotations/EC.txt', index_col=0, header=0, sep='\t')
print(EC)
#it does look like there are some nans in here!
EC.isnull().values.any()
#True
for col in EC.columns:
  if EC.loc[:, [col]].isnull().values.any():
    print(col)
    #2.1.1.20 is the only one printed

print(EC['2.1.1.20'])
#Looks like they are all nan. Maybe an issue with reading in?

count = 0
for row in open('bac_annotations/EC.txt', 'r'):
  if count == 0:
    row = row.replace('\n', '').split('\t')
    for r in range(len(row)):
      if row[r] == '2.1.1.20':
        loc = r
  else:
    row = row.split('\t')
    print([row[loc]])
  #if count > 1: break
  count += 1

#basically it always seems to just have the line break and be empty. See if this is the case for other annotations.
KO = pd.read_csv('bac_annotations/KEGG_ko.txt', index_col=0, header=0, sep='\t')
print(KO)
#it does look like there are some nans in here!
#same thing as the previous one - all of the values in the last column are nan :(

  

```

I re-ran some of the earlier steps and now coming back to test PICRUSt2:
```{bash, eval=FALSE}
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_arc_3 -p 12 --ref_dir gtdb_picrust_ref/arc_ref/ --custom_trait_tables gtdb_picrust_ref/arc_annotations/EC.txt --marker_gene_table gtdb_picrust_ref/arc_annotations/16S.txt --no_pathways --no_regroup --verbose

picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_3 -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/EC.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/16S.txt --no_pathways --no_regroup --verbose
#Error - at least one trait in the prediction table was entirely missing values.
#Execution halted :(

picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_4 -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/KEGG_ko.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/16S.txt --no_pathways --no_regroup --verbose
#Error running this command:
#Rscript /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R picrust_test/picrust2_out_pipeline_bac_4/out.tre /tmp/tmp1vkf__vm/subset_tab_0 mp 0.5 FALSE FALSE /tmp/tmpvjmji_1j/predicted_counts.txt /tmp/tmpvjmji_1j/predicted_ci.txt 100
#Error - at least one trait in the prediction table was entirely missing values.
#Execution halted


```

Modified the 16S.txt for bacteria by just resaving it because I don't know what's wrong with it.
```{bash, eval=FALSE}
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_5 -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/KEGG_ko.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt --no_pathways --no_regroup --verbose
#still doesnt work
```

Try editing some scripts:
```{bash, eval=FALSE}
cp /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp_original.R
vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
#added in some printing steps and cat instead of stop to print the warning but not stop the whole script
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_6 -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/KEGG_ko.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt --no_pathways --no_regroup --verbose
#so the nsti step can run, but for some reason the 16S counts aren't being added
#print different step
vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
#printed the trait table at the start
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_7 -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/KEGG_ko.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt --no_pathways --no_regroup --verbose
#so the table appears to read in fine... why doesn't it still work after?!
#printing next part of trait_values df
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_8 -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/KEGG_ko.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt --no_pathways --no_regroup --verbose
#we have NAs for the input sequences here, but is this the case for archeae at that step, too?
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_arc_4 -p 12 --ref_dir gtdb_picrust_ref/arc_ref/ --custom_trait_tables gtdb_picrust_ref/arc_annotations/EC.txt --marker_gene_table gtdb_picrust_ref/arc_annotations/16S.txt --no_pathways --no_regroup --verbose
#yes... so obviously for bacteria they just don't get filled in at some point when they should
#now printing predicted_values when it is first made
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_arc_5 -p 12 --ref_dir gtdb_picrust_ref/arc_ref/ --custom_trait_tables gtdb_picrust_ref/arc_annotations/EC.txt --marker_gene_table gtdb_picrust_ref/arc_annotations/16S.txt --no_pathways --no_regroup --verbose
#this prints predictions
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_9 -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/KEGG_ko.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt --no_pathways --no_regroup --verbose
#ok, so issue seems to be with the prediction step. Maybe a different one will work?
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_10 -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/KEGG_ko.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt --no_pathways --no_regroup --verbose -m subtree_average
#this works!!! so the issue is something specifically with mp (maybe others too?)
```

Now run pipeline parts separately:
```{bash, eval=FALSE}
#first insertion
mkdir picrust_test/picrust2_steps
place_seqs.py --ref_dir gtdb_picrust_ref/bac_ref/ -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/picrust2_steps/out.tre -p 12 --intermediate picrust_test/picrust2_steps/intermediate

parallel -j 1 'hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt -t picrust_test/picrust2_steps/out.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_{}.tsv.gz -p 12 -n -m {} --verbose' ::: emp_prob subtree_average pic scp mp
#so it only seems to be mp that doesn't work

#now change back to original script and see if it works
mv /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp_modified.R
cp /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp_original.R /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
```

Test whether mp works after 16S step:
```{bash, eval=FALSE}
hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/KEGG_ko.txt -t picrust_test/picrust2_steps/out.tre -o picrust_test/picrust2_steps/KEGG_ko.txt -p 12 --verbose
#it doesnt, but it runs a few of the subset_tables before it fails. So maybe it is happening with some specific IDs? And with the 16S I guess there aren't subset_tables!
#Lets print things out again
#already have castor_hsp_original.R saved
vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
#change temporary director for castor
vi wrap_hsp.py
mkdir picrust_test/picrust2_steps/temporary_directory
#this didn't even work
vi /home/robyn/tools/picrust2-2.5.3/picrust2/wrap_hsp.py #put back to normal
vi /home/robyn/tools/picrust2-2.5.3/picrust2/bin/hsp.py #changed nothing here
#castor_hsp_workflow in wrap_hsp.py
#just print out more? 
vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R

hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt -t picrust_test/picrust2_steps/out.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_mp.tsv -p 12 -n --verbose

hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt -t picrust_test/picrust2_steps/out.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_emp_prob.tsv -p 12 -n --verbose -m emp_prob

#so I can see nan for the hsp_out_models_unknown_lik table for mp, but it works for emp_prob
vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
#print(trait_values)
#print(full_tree)
#print(unknown_tips_index)
#print(check_input_set)

#print all input to hsp_out_models_unknown_lik <- lapply()

hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt -t picrust_test/picrust2_steps/out.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_mp.tsv -p 12 -n --verbose
#check_input_set change to TRUE
#made no difference

#try previous version of PICRUSt2
#If not, maybe try printing out tree for archaea and seeing if edges are still showing as null?
conda activate picrust2
#unknown version, but probably 2.5.2
hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt -t picrust_test/picrust2_steps/out.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_mp.tsv -p 12 -n --verbose
#still get the error, but no warnings about R version the packages were made with
#try with 1 processor before emailing Gavin
conda activate picrust-v2.5.3
hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt -t picrust_test/picrust2_steps/out.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_mp.tsv -p 1 -n --verbose
#nope :(
#just print things out within the mp_study_probs function before emailing Gavin
vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R

hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/bacteria_16S_new.txt -t picrust_test/picrust2_steps/out.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_mp.tsv -p 12 -n --verbose

#try with archaea
hsp.py --observed_trait_table gtdb_picrust_ref/arc_annotations/16S.txt -tpicrust_test/picrust2_out_pipeline_arc/out.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_mp_arc.tsv -p 12 -n --verbose
#here we have a $total_cost, where we don't with bacteria. We don't use that though, it's the likelihoods that we need

vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
#when we check input, there are NAs in what it outputs. Don't really know how this is generated though?
[1]  2168  2239  3864  4252  4258 11068 24686 24688 24690 24691 24693 24694
[13] 24695 24696 24699 24702 24703 24704 24706 24708 24709 24712 24713 24715
[25] 24717 24719 24721 24723 24724 24728 24730 24731 24732 25004 25012 25166
[37] 26069

#I think it is maybe because of the edge lengths?
hsp.py --observed_trait_table gtdb_picrust_ref/arc_annotations/16S.txt -tpicrust_test/picrust2_out_pipeline_arc/out.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_mp_arc.tsv -p 12 -n --verbose
#$edge.length - check for NAs in "edge"        "edge.length"
#archaea
[1] "Nnode"       "tip.label"   "node.label"  "edge"        "edge.length"
[6] "edge.label"  "edge.number" "root"        "root.edge"  
[1] FALSE
[1] FALSE
[1] FALSE
[1] TRUE
[1] FALSE
#bacteria 
[1] "Nnode"       "tip.label"   "node.label"  "edge"        "edge.length"
[6] "edge.label"  "edge.number" "root"        "root.edge"  
[1] FALSE
[1] FALSE
[1] FALSE
[1] TRUE
[1] TRUE

#print(in_tree$edge) - all numbers, but no NA
#print(in_tree$Nnode) - 26904
#print(in_tree$tip.label) - list of tip names for bacteria and archaea, and I can see the ASV names amongst the others
#print(in_tree$node.label) - no node labels for bacteria!!
#so this is where the issue must be!!!
#so is it because I took away the node labels?!?!

```

Lets try using the internal node names again:
```{bash, eval=FALSE}
#for archaea, even though I removed my internal node names, they still seem to be in the output trees? Maybe I just accidentally used a different output
mv bac_ref/bac_ref.tre bac_ref/bac_ref_no_internal_nodes.tre
mv bac_ref/bac_ref_internal.tre bac_ref/bac_ref.tre
mv arc_ref/arc_ref.tre arc_ref/arc_ref_no_internal_nodes.tre
mv arc_ref/arc_ref_internal.tre arc_ref/arc_ref.tre

place_seqs.py --ref_dir gtdb_picrust_ref/bac_ref/ -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/picrust2_steps/out_internal_bac.tre -p 12

place_seqs.py --ref_dir gtdb_picrust_ref/bac_ref/ -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/picrust2_steps/out_internal_arc.tre -p 12

hsp.py --observed_trait_table gtdb_picrust_ref/arc_annotations/16S.txt -t picrust_test/picrust2_steps/out_internal_arc.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_internal_arc.tsv -p 12 -n --verbose
#Error: None of the reference ids within the function abundance table are found within the input tree. This can occur when malformed or mismatched custom reference files are used.

hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/16S.txt -t picrust_test/picrust2_steps/out_internal_bac.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_internal_bac.tsv -p 12 -n --verbose
#Error - at least one trait in the prediction table was entirely missing values.
vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
#there are no edge labels
#are there edge labels for archaea in the tree that worked?
#no edge labels for archaea that works either
print(in_tree$node.label)
```

First troubleshoot archaea with internal labels:
```{bash, eval=FALSE}
hsp.py --observed_trait_table gtdb_picrust_ref/arc_annotations/16S.txt -t picrust_test/picrust2_steps/out_internal_arc.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_internal_arc.tsv -p 12 -n --verbose
#issue here is that the archaea tree has somehow become the bacteria tree
cp gtdb_picrust_files/archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre gtdb_picrust_ref/arc_ref/arc_ref.tre
#it actually wasn't, I just had the step wrong above lol

place_seqs.py --ref_dir gtdb_picrust_ref/arc_ref/ -s picrust_test/chemerin_16S/seqs.fna -o picrust_test/picrust2_steps/out_internal_arc.tre -p 12

hsp.py --observed_trait_table gtdb_picrust_ref/arc_annotations/16S.txt -t picrust_test/picrust2_steps/out_internal_arc.tre -o picrust_test/picrust2_steps/marker_nsti_predicted_internal_arc.tsv -p 12 -n --verbose
#this still works fine

#now try again with bacteria and printing out edge labels
#[1] "Nnode"       "tip.label"   "node.label"  "edge"        "edge.length"
#[6] "edge.label"  "edge.number" "root"        "root.edge"  
#will run all with archaea and then bacteria each time
vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
#Nnode - 1038 and NA=FALSE for archaea (diff of 36 vs input genomes), 26904 and NA=FALSE for bacteria (diff of 36 vs input genomes)
#tip.label - list of 1039 incl. ASVs, NA=FALSE for archaea, list of 26905 incl. ASVs, NA=FALSE for bacteria
#node.label - prints 1 and blank, NA=FALSE for archaea and bacteria
#edge - table of 2076 going up to 1039, NA=FALSE for archaea, probably similar for bacteria - df of 49999, 3809 rows omitted, going up to 24991
#edge.length - list of 2076, NA=FALSE for archaea, list of 53808, NA=FALSE for bacteria
#edge.label - NULL for both, NA=FALSE
#edge.number - NULL and NA=FALSE for archaea, same for bacteria
#root - 1040, NA=FALSE for archaea, 26906 and NA=FALSE for bacteria
#root.edge - NULL and NA=FALSE for archaea, NULL and NA=FALSE for bacteria

#so it seems like the likelihoods are the only thing that is different.
print(attr(x=mp_hsp_out$success, which="names")) #after changing check_input to TRUE
#NULL

hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/KEGG_ko.txt -t picrust_test/picrust2_steps/out_internal_bac.tre -o picrust_test/picrust2_steps/KEGG_ko_predicted_internal_bac.tsv -p 12 --verbose
#see if all NA for the likelihoods that fails for KO
```

Put together some files for troubleshooting. Just check that I get the same error locally:
```{bash, eval=FALSE}
conda activate picrust-v2.5.3

place_seqs.py --ref_dir troubleshoot_files/archaea/arc_ref/ -s troubleshoot_files/chemerin_16S/seqs.fna -o troubleshoot_files/out_arc.tre -p 12

hsp.py --observed_trait_table troubleshoot_files/archaea/16S.txt -t troubleshoot_files/out_arc.tre -o troubleshoot_files/marker_nsti_predicted_arc.tsv -p 12 -n --verbose

place_seqs.py --ref_dir troubleshoot_files/bacteria/bac_ref/ -s troubleshoot_files/chemerin_16S/seqs.fna -o troubleshoot_files/out_bac.tre -p 12

hsp.py --observed_trait_table troubleshoot_files/bacteria/16S.txt -t troubleshoot_files/out_bac.tre -o troubleshoot_files/marker_nsti_predicted_bac.tsv -p 12 -n --verbose

```

## Fixed???

Fixed with some help from Gavin! Some edge lengths were 0.
```{bash, eval=FALSE}
mv /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp_modified_2.R
mv /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp_original.R /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
vi /home/robyn/tools/picrust2-2.5.3/picrust2/Rscripts/castor_hsp.R
#and just added this line to just after command line arguments are read in:
#full_tree$edge.length[which(full_tree$edge.length == 0)] <- 0.00001
```

```{bash, eval=FALSE}
picrust2_pipeline.py -s picrust_test/chemerin_16S/seqs.fna -i picrust_test/chemerin_16S/table.biom -o picrust_test/picrust2_out_pipeline_bac_fix -p 12 --ref_dir gtdb_picrust_ref/bac_ref/ --custom_trait_tables gtdb_picrust_ref/bac_annotations/EC.txt --marker_gene_table gtdb_picrust_ref/bac_annotations/16S.txt --no_pathways --no_regroup --verbose
#Error: std::bad_alloc
#Execution halted

hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/KEGG_ko.txt -t picrust_test/picrust2_out_pipeline_bac_fix/out.tre -o picrust_test/picrust2_out_pipeline_bac_fix/KEGG_ko_predicted.txt -p 12 --verbose

#getting this std::bad_alloc again??? Maybe just because I'm running checkm at the same time??
```

Try locally:
```{bash, eval=FALSE}
picrust2_pipeline.py -s chemerin_16S/seqs.fna -i chemerin_16S/table.biom -o picrust2_out_pipeline_bac_fix -p 12 --ref_dir bacteria/bac_ref/ --custom_trait_tables bacteria/KEGG_ko.txt --marker_gene_table bacteria/16S.txt --no_pathways --no_regroup --verbose
```

OK, so this isn't working because of two reasons: (1) we still have '-' in each of the files which is giving very large numbers of ECs etc to predict, and (2) we do have very large numbers of copies of some genes in them. First I'll just try dropping the '-' and see how this then does.
```{python}
import pandas as pd

files = ['BiGG_Reaction.txt', 'CAZy.txt', 'EC.txt', 'GOs.txt', 'KEGG_ko.txt', 'PFAMs.txt', 'Preferred_name.txt']
files_folders = []
for f in files:
  files_folders.append('gtdb_picrust_ref/'+f)
  files_folders.append('gtdb_picrust_ref/arc_annotations/'+f)
  files_folders.append('gtdb_picrust_ref/bac_annotations/'+f)
  
for f in files_folders:
  df = pd.read_csv(f, index_col=0, header=0, sep='\t')
  init_df = df.shape[1]
  df = df.drop('-', axis=1)
  print(f, init_df, df.shape[1], max(df.max(axis=0)))
  df.to_csv(f, sep='\t')
  
for f in files_folders:
  print(f)
  df = pd.read_csv(f, index_col=0, header=0, sep='\t')
  df[df > 10] = 10
  df.to_csv(f.replace('.txt', '_max10.txt'), sep='\t')
```

Try single:
```{bash, eval=FALSE}
hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/BiGG_Reaction.txt -t picrust_test/picrust2_out_pipeline_bac_fix/out.tre -o picrust_test/picrust2_out_pipeline_bac_fix/BiGG_Reaction_predicted.txt -p 12 --verbose
#this works!

hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/EC_max10.txt -t picrust_test/picrust2_out_pipeline_bac_fix/out.tre -o picrust_test/picrust2_out_pipeline_bac_fix/EC_predicted.txt -p 12 --verbose
```

### Make copies of new reference files

```{bash, eval=FALSE}
mkdir new_reference
mkdir new_reference/bacteria
mkdir new_reference/archaea
cp -r gtdb_picrust_ref/bac_ref new_reference/bacteria/
cp -r gtdb_picrust_ref/arc_ref new_reference/archaea/
cp gtdb_picrust_ref/arc_annotations/16S.txt new_reference/archaea/
cp gtdb_picrust_ref/arc_annotations/BiGG_Reaction_max10.txt new_reference/archaea/bigg_reaction
cp gtdb_picrust_ref/arc_annotations/CAZy_max10.txt new_reference/archaea/cazy.txt
cp gtdb_picrust_ref/arc_annotations/EC_max10.txt new_reference/archaea/ec.txt
cp gtdb_picrust_ref/arc_annotations/GOs_max10.txt new_reference/archaea/go.txt
cp gtdb_picrust_ref/arc_annotations/KEGG_ko_max10.txt new_reference/archaea/ko.txt
cp gtdb_picrust_ref/arc_annotations/PFAMs_max10.txt new_reference/archaea/pfam.txt
cp gtdb_picrust_ref/arc_annotations/Preferred_name_max10.txt new_reference/archaea/preferred_name.txt
cp gtdb_picrust_ref/arc_annotations/metadata.csv new_reference/archaea/

cp gtdb_picrust_ref/bac_annotations/16S.txt new_reference/bacteria/
cp gtdb_picrust_ref/bac_annotations/BiGG_Reaction_max10.txt new_reference/bacteria/bigg_reaction.txt
cp gtdb_picrust_ref/bac_annotations/CAZy_max10.txt new_reference/bacteria/cazy.txt
cp gtdb_picrust_ref/bac_annotations/EC_max10.txt new_reference/bacteria/ec.txt
cp gtdb_picrust_ref/bac_annotations/GOs_max10.txt new_reference/bacteria/go.txt
cp gtdb_picrust_ref/bac_annotations/KEGG_ko_max10.txt new_reference/bacteria/ko.txt
cp gtdb_picrust_ref/bac_annotations/PFAMs_max10.txt new_reference/bacteria/pfam.txt
cp gtdb_picrust_ref/bac_annotations/Preferred_name_max10.txt new_reference/bacteria/preferred_name.txt
cp gtdb_picrust_ref/bac_annotations/metadata.csv new_reference/bacteria/

gzip new_reference/bacteria/*.txt
gzip new_reference/archaea/*.txt

#missing = cog, pheno, tigrfam

#previous: 16S.txt.gz  cog.txt.gz  ec.txt.gz  ko.txt.gz  pfam.txt.gz  pheno.txt.gz  pro_ref  tigrfam.txt.gz
```


## Test datasets for sequence insertion 

```{bash, eval=FALSE}
cp /home/storage/gavin/projects/picrust_folders/picrust2_manuscript.tar.gz /bigpool/robyn/picrust2_database/
cd  /bigpool/robyn/picrust2_database/
tar -xvf picrust2_manuscript.tar.gz

mkdir picrust_datasets_test
mkdir picrust_datasets_test/data
parallel -j 1 'cp picrust2_manuscript/data/16S_datasets/{}/*.biom.tsv picrust_datasets_test/data/' ::: blueberry cameroon hmp indian mammal ocean primate
parallel -j 1 'cp picrust2_manuscript/data/16S_datasets/{}/*.fasta picrust_datasets_test/data/' ::: blueberry cameroon hmp indian mammal ocean primate
parallel -j 1 'cp picrust2_manuscript/data/16S_datasets/{}/*.fna picrust_datasets_test/data/' ::: blueberry cameroon hmp indian mammal ocean primate

#rename
cd picrust_datasets_test/data/
mv cameroon_16S.renamed.biom.tsv cameroon_16S.biom.tsv
mv indian_16S.renamed.with_extra_header.biom.tsv indian_16S.biom.tsv
mv blueberry_16S_rep_seqs.fna blueberry_16S_rep_seqs.fasta
mv primate_asvs.fna primate_16S_rep_seqs.fasta
cd ..
cd ..

mkdir picrust_datasets_test/original_nsti
parallel -j 1 'cp picrust2_manuscript/data/16S_datasets/{}/marker_predicted_and_nsti.tsv picrust_datasets_test/original_nsti/{}_marker_predicted_and_nsti.tsv' ::: blueberry cameroon hmp indian mammal ocean primate
```

Place sequences in new database:
```{bash, eval=FALSE}
mkdir picrust_datasets_test/trees
parallel -j 2 --progress --eta 'place_seqs.py --ref_dir gtdb_picrust_ref/{1}_ref/ -s picrust_datasets_test/data/{2}_16S_rep_seqs.fasta -o picrust_datasets_test/trees/{2}_{1}.tre -p 12' ::: bac arc ::: blueberry cameroon hmp indian mammal ocean primate

#get nsti
mkdir picrust_datasets_test/marker_nsti
parallel -j 2 --progress --eta 'hsp.py --observed_trait_table gtdb_picrust_ref/bac_annotations/16S.txt -t picrust_datasets_test/trees/{2}_{1}.tre -o picrust_datasets_test/marker_nsti/{2}_{1}_{3}.tsv -p 12 -n -m {3}' ::: bac ::: blueberry cameroon hmp indian mammal ocean primate ::: mp
emp_prob subtree_average pic scp

parallel -j 2 --progress --eta 'hsp.py --observed_trait_table gtdb_picrust_ref/arc_annotations/16S.txt -t picrust_datasets_test/trees/{2}_{1}.tre -o picrust_datasets_test/marker_nsti/{2}_{1}_{3}.tsv -p 12 -n -m {3}' ::: arc ::: blueberry cameroon hmp indian mammal ocean primate ::: emp_prob subtree_average pic scp mp
```

Place sequences in original database:
```{bash, eval=FALSE}
parallel -j 2 --progress --eta 'place_seqs.py -s picrust_datasets_test/data/{2}_16S_rep_seqs.fasta -o picrust_datasets_test/trees/{2}_{1}.tre -p 12' ::: def ::: blueberry cameroon hmp indian mammal ocean primate

parallel -j 2 --progress --eta 'hsp.py -i 16S -t picrust_datasets_test/trees/{2}_{1}.tre -o picrust_datasets_test/marker_nsti/{2}_{1}_{3}.tsv -p 12 -n -m {3}' ::: def ::: blueberry cameroon hmp indian mammal ocean primate ::: emp_prob subtree_average pic scp mp
```

Process output - get best options between bacteria and archaea:
```{python, eval=FALSE}
#mkdir picrust_datasets_test/nsti_comparison/
from ete3 import Tree
import pandas as pd
import os

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']
methods = ['emp_prob', 'subtree_average', 'pic', 'scp', 'mp']
domains = ['bac', 'arc', 'def']
for ds in datasets:
  print(ds)
  in_df = pd.read_csv('picrust_datasets_test/data/'+ds+'_16S.biom.tsv', header=1, index_col=0, sep='\t')
  asvs = set(in_df.index.values)
  new_df = in_df.copy(deep=True)
  new_df['Placed in bacteria tree'] = False
  new_df['Placed in archaea tree'] = False
  new_df['Placed in default tree'] = False
  new_df = new_df.loc[:, ['Placed in bacteria tree', 'Placed in archaea tree']]
  bac_tree = Tree('picrust_datasets_test/trees/'+ds+'_bac.tre', format=1, quoted_node_names=True)
  arc_tree = Tree('picrust_datasets_test/trees/'+ds+'_arc.tre', format=1, quoted_node_names=True)
  def_tree = Tree('picrust_datasets_test/trees/'+ds+'_def.tre', format=1, quoted_node_names=True)
  placed_in_bac_tree, placed_in_arc_tree, placed_in_def_tree = [], [], []
  for node in bac_tree.traverse("postorder"):
    if node.name in asvs:
      placed_in_bac_tree.append(node.name)
      new_df.loc[node.name, 'Placed in bacteria tree'] = True
  for node in arc_tree.traverse("postorder"):
    if node.name in asvs:
      placed_in_arc_tree.append(node.name)
      new_df.loc[node.name, 'Placed in archaea tree'] = True
  for node in def_tree.traverse("postorder"):
    if node.name in asvs:
      placed_in_def_tree.append(node.name)
      new_df.loc[node.name, 'Placed in default tree'] = True
  for dom in domains:
    for method in methods:
      if os.path.exists('picrust_datasets_test/marker_nsti/'+ds+'_'+dom+'_'+method+'.tsv'):
        new_df[dom+'_'+method+'_nsti'] = ''
        nsti = pd.read_csv('picrust_datasets_test/marker_nsti/'+ds+'_'+dom+'_'+method+'.tsv', index_col=0, header=0, sep='\t')
        for row in nsti.index.values:
          new_df.loc[row, dom+'_'+method+'_nsti'] = nsti.loc[row, 'metadata_NSTI']
  new_df.to_csv('picrust_datasets_test/nsti_comparison/'+ds+'_bac_arc.csv')
  # print(ds, len(asvs), len(placed_in_bac_tree), len(placed_in_arc_tree))
  # blueberry 3333 3333 3333
  # cameroon 4277 4277 3847
  # hmp 1865 1865 1633
  # indian 2397 2384 40
  # mammal 323 323 323
  # ocean 1148 1148 1148
  # primate 8251 8222 8138
```

## Run CheckM on previous genomes

```{bash, eval=FALSE}
wget https://figshare.com/ndownloader/files/22494503
mv 22494503 JGI_PICRUSt_genomes.tar.bz2
tar -xvjf JGI_PICRUSt_genomes.tar.bz2
conda activate checkm
wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz
tar -xvf checkm_data_2015_01_16.tar.gz
#then moved everything to checkm_data
export CHECKM_DATA_PATH=/bigpool/robyn/picrust2_database/checkm_data/

mkdir JGI_PICRUSt_genomes
```

Looks like I put them all in a single fasta...

```{python, eval=FALSE}
from Bio import SeqIO

for record in SeqIO.parse('JGI_PICRUSt_genomes.fasta', "fasta"):
  this_record = [record]
  SeqIO.write(this_record, 'JGI_PICRUSt_genomes/'+str(record.id)+'.fasta', "fasta")
```

Run CheckM:
```{bash, eval=FALSE}
#checkm lineage_wf -t 8 -x fa bin_folder output_folder
mkdir 
checkm lineage_wf --reduced_tree -t 24 -x fasta JGI_PICRUSt_genomes checkm_out -f JGI-CHECKM.txt --tab_table
#didnt have all of the files in the folders
checkm tree_qa checkm_out/ -o 2 -f JGI-CHECKM.txt --tab_table
checkm analyze checkm_out/lineage.ms JGI_PICRUSt_genomes checkm_out -t 24 -x fasta
checkm qa checkm_out/lineage.ms checkm_out -o 1 -f JGI-CHECKM-qa.txt --tab_table
```

## Taxonomically classify dataset sequences

```{bash, eval=FALSE}
cd picrust_datasets_test
mkdir qiime2_classification

conda activate qiime2-amplicon-2024.5

parallel -j 7 'qiime tools import --input-path {} --output-path qiime2_classification/{/.}.qza --type "FeatureData[Sequence]"' ::: data/*.fasta
parallel -j 1 'qiime feature-classifier classify-sklearn --i-reads qiime2_classification/{}_16S_rep_seqs.qza --i-classifier /home/shared/taxa_classifiers/qiime2-amplicon-2024.5_classifiers/greengenes_2022.10.backbone.full-length.nb.sklearn-1.4.2.qza --p-n-jobs 12 --output-dir qiime2_classification/{}' ::: blueberry cameroon hmp indian mammal ocean primate
parallel -j 7 'qiime tools export --input-path qiime2_classification/{}/classification.qza --output-path qiime2_classification/{}' ::: blueberry cameroon hmp indian mammal ocean primate
```

## Get lowest classification for each  ASV and then match genomes

```{python}
import pandas as pd
import numpy as np

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']
all_species_level = set()

for ds in datasets:
  classification = pd.read_csv('qiime2_classification/'+ds+'/taxonomy.tsv', sep='\t', index_col=0, header=0)
  all_species_level = all_species_level.union(set(classification['Taxon'].values))

tax_using = {}
tax_for_genomes = []
for tax in all_species_level:
  tax = tax.split('; ')
  new_tax = []
  for t in tax:
    if len(t) > 3:
      new_tax.append(t)
  if 's__' in new_tax[-1]:
    new_tax = new_tax[:-1]
  tax_for_genomes.append('; '.join(new_tax))
  tax_using['; '.join(tax)] = '; '.join(new_tax)

with open('genomes_needed.txt', 'w') as f:
  for tax in set(tax_for_genomes):
    w = f.write(tax+'\n')

with open('tax_names.txt', 'w') as f:
  for tax in tax_using:
    w = f.write(tax+'\t'+tax_using[tax]+'\n')
```

```{bash, eval=FALSE}
cd ..
```

Also get genbank assemble summary:
```{bash, eval=FALSE}
wget https://ftp.ncbi.nlm.nih.gov/genomes/genbank/assembly_summary_genbank.txt
```

```{python}
import pandas as pd
import random
from multiprocessing import Pool
from multiprocessing import freeze_support
from multiprocessing import Process, Manager
import pickle

bac = pd.read_csv('GTDB_r214/bac120_metadata_r214.tsv', index_col=0, header=0, sep='\t')
arc = pd.read_csv('GTDB_r214/ar53_metadata_r214.tsv', index_col=0, header=0, sep='\t')
genomes_needed = []
for row in open('picrust_datasets_test/genomes_needed.txt', 'r'):
  row = row.replace('\n', '')
  row = row.split('; ')
  row = ';'.join(row)
  genomes_needed.append(row)
  
bac = bac[bac['checkm_completeness'] >= 90]
bac = bac[bac['checkm_contamination'] <= 10]
bac = bac[bac['gtdb_representative'] == 'f']
arc = arc[arc['checkm_completeness'] >= 90]
arc = arc[arc['checkm_contamination'] <= 10]
arc = arc[arc['gtdb_representative'] == 'f']
gtdb = pd.concat([bac, arc])

possible_genomes = set(list(gtdb.index.values))
genome_matches = {}
no_matches = []

def check_genomes(checking):
  tax_genomes = []
  for genome in possible_genomes:
    tax = gtdb.loc[genome, 'gtdb_taxonomy']
    if checking in tax:
      tax_genomes.append([genome, tax])
  return(tax_genomes)

def write_match(tax, genome):
  with open('genome_matches.txt', 'a') as f:
    w = f.write(tax+'\t'+genome+'\n')
  return

def get_match(tax_needed):
  try:
    for n in [-1, -2, -3, -4, -5, -6, -7]:
      lowest = tax_needed.split(';')[n]
      if '__' not in lowest: continue
      tax_genomes = check_genomes(lowest)
      if len(tax_genomes) == 0: 
        l = str(lowest[0])
        c_ = lowest.split('__')[1].count('_')
        if c_ > 0:
          if c_ == 1:
            lowest = l+'__'+lowest.split('__')[1].split('_')[0]
            tax_genomes = check_genomes(lowest)
          elif c_ > 1:
            lowest = lowest.split('__')[1].split('_')
            try:
              num = int(lowest[1])
              lowest = lowest[0]
            except:
              lowest = lowest[0]+'_'+lowest[1]
            lowest = l+'__'+lowest
            tax_genomes = check_genomes(lowest)
        else:
          continue
      if len(tax_genomes) == 0: continue
      print(tax_needed, lowest, len(tax_genomes))
      if len(tax_genomes) == 1: num = 0
      else: num = random.randint(0, len(tax_genomes)-1)
      global genome_matches, possible_genomes
      globals()["genome_matches"][tax_needed] = tax_genomes[num][0]
      write_match(tax_needed, tax_genomes[num][0])
      globals()["possible_genomes"].remove(tax_genomes[num][0])
      print(len(globals()["possible_genomes"]), len(globals()["genome_matches"]))
      return
  except:
    global no_matches
    no_matches.append(tax_needed)
    return

count = 0
for tax in genomes_needed:
  if tax in genome_matches: continue
  else: print(tax)
  get_match(tax)
  print(count)
  count += 1
  
#was trying to use multiprocessing here, but essentially you can't write to global variables because each process is effectively a new instance of python

# def run_multiprocessing(func, i, n_processors):
#     with Pool(processes=n_processors) as pool:
#         return pool.map(func, i)
# 
# def main():
#     print('Starting processing')
#     run_multiprocessing(get_match, genomes_needed, 24)
# 
# if __name__ == "__main__":
#     freeze_support()   # required to use multiprocessing
#     main()   

import pickle
with open('genome_matches.dict', 'wb') as f:
  pickle.dump(genome_matches, f)
# 
# with open('possible_genomes.set', 'wb') as f:
#   pickle.dump(possible_genomes, f)
 
bac_sum = pd.read_csv('test_genomes/bacteria_assembly_summary.txt', index_col=0, header=1, sep='\t')
arc_sum = pd.read_csv('test_genomes/archaea_assembly_summary.txt', index_col=0, header=1, sep='\t')
gb_sum = pd.read_csv('test_genomes/assembly_summary_genbank.txt', index_col=0, header=1, sep='\t')
ncbi = pd.concat([bac_sum, arc_sum, gb_sum])
ncbi_genomes = set(ncbi.index.values)

genome_info = []
for tax in genome_matches:
  genome = genome_matches[tax].split('_', 1)[1]
  if genome in ncbi_genomes:
    genome_info.append([tax, genome, genome, ncbi.loc[genome, 'ftp_path']])
  elif genome.replace('GCA', 'GCF') in ncbi_genomes:
    genome_info.append([tax, genome, genome.replace('GCA', 'GCF'), ncbi.loc[genome.replace('GCA', 'GCF'), 'ftp_path']])
  elif genome.replace('GCF', 'GCA') in ncbi_genomes:
    genome_info.append([tax, genome, genome.replace('GCF', 'GCA'), ncbi.loc[genome.replace('GCF', 'GCA'), 'ftp_path']])
  elif genome.replace('.1', '.2') in ncbi_genomes:
    genome_info.append([tax, genome, genome.replace('.1', '.2'), ncbi.loc[genome.replace('.1', '.2'), 'ftp_path']])
  elif genome.replace('.1', '.3') in ncbi_genomes:
    genome_info.append([tax, genome, genome.replace('.1', '.3'), ncbi.loc[genome.replace('.1', '.3'), 'ftp_path']])
  else:
    print(genome)

ftp_links_bacteria, ftp_links_archaea = [], []
for genome in genome_info:
  link = genome[3].replace('https', 'ftp')
  link = link+'/'+link.split('/')[-1]+'_genomic.fna.gz'
  if 'Bacteria' in genome[0]:
    ftp_links_bacteria.append(link)
  else:
    ftp_links_archaea.append(link)
print(len(ftp_links_bacteria), len(ftp_links_archaea))
#1446 34
  
with open('test_genomes/tax_genome_info.txt', 'w') as f:
  for row in genome_info:
    w = f.write('\t'.join(row)+'\n')

with open('test_genomes/genome_ftp_bacteria.txt', 'w') as f:
  for row in ftp_links_bacteria:
    w = f.write(row+'\n')
    
with open('test_genomes/genome_ftp_archaea.txt', 'w') as f:
  for row in ftp_links_archaea:
    w = f.write(row+'\n')
```

## Download genomes

Download genomes:
```{bash, eval=FALSE}
cd test_genomes
parallel -j 12 --progress --eta -a genome_ftp_archaea.txt 'wget {}'
mkdir genomes_archaea
mv *.fna.gz genomes_archaea
parallel -j 12 --progress --eta -a genome_ftp_bacteria.txt 'wget {}'
mkdir genomes_bacteria
mv *.fna.gz genomes_bacteria
```

### Run Barrnap

```{bash, eval=FALSE}
conda activate picrust-v2.5.3
parallel -j 24 --progress --eta 'gunzip {}' ::: genomes_archaea/*.gz
parallel -j 24 --progress --eta 'gunzip {}' ::: genomes_bacteria/*.gz
mkdir barrnap_archaea
mkdir barrnap_bacteria

parallel -j 8 --progress --eta 'barrnap -q -k arc {} --outseq barrnap_archaea/{/} --reject 0.8 --threads 6' ::: genomes_archaea/*.fna

parallel -j 8  --progress --eta 'barrnap -q -k bac {} --outseq barrnap_bacteria/{/} --reject 0.8 --threads 6' ::: genomes_bacteria/*.fna

mkdir 16S_archaea
mkdir 16S_bacteria
mkdir rerun_archaea
mkdir rerun_bacteria
```

Get 16S sequences:
```{python, eval=FALSE}
import os
from Bio import SeqIO

dom = 'bacteria'
genomes = os.listdir('barrnap_'+dom)
for genome in genomes:
  seqs = []
  for record in SeqIO.parse('barrnap_'+dom+'/'+genome, "fasta"):
    if '16S' in record.id:
      seqs.append(record)
  if len(seqs) != 0:
    SeqIO.write(seqs, '16S_'+dom+'/'+genome, "fasta")
  else:
    os.system('mv genomes_'+dom+'/'+genome+' rerun_'+dom)
```

Run barrnap again:
```{bash, eval=FALSE}
parallel -j 8 --progress --eta 'barrnap -q -k arc {} --outseq barrnap_archaea/{/.}_nolim.fna --threads 6' ::: rerun_archaea/*.fna

parallel -j 8  --progress --eta 'barrnap -q -k bac {} --outseq barrnap_bacteria/{/}_nolim.fna --threads 6' ::: rerun_bacteria/*.fna

cp rerun_bacteria/* genomes_bacteria
cp rerun_archaea/* genomes_archaea
```

Check how many genomes have 16S now:
```{python, eval=FALSE}
import os
from Bio import SeqIO

dom = 'archaea'
genomes = os.listdir('barrnap_'+dom)
genomes = [g for g in genomes if 'nolim' in g]
count = 0
for genome in genomes:
  seqs = []
  for record in SeqIO.parse('barrnap_'+dom+'/'+genome, "fasta"):
    if '16S' in record.id:
      seqs.append(record)
  if len(seqs) != 0:
    SeqIO.write(seqs, '16S_'+dom+'/'+genome, "fasta")
  else:
    count += 1

print(count)
#bacteria = 653
#archaea = 16
```

Look in ssu file:
```{bash, eval=FALSE}
wget https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_all/ssu_all_r214.tar.gz
tar -xvf ssu_all_r214.tar.gz


```

### Run EggNog

```{bash, eval=FALSE}
conda activate eggnog
mkdir eggnog_data
download_eggnog_data.py --data_dir eggnog_data
create_dbs.py -m diamond --dbname bacteria --taxa Bacteria --data_dir eggnog_data
create_dbs.py -m diamond --dbname archaea --taxa Archaea --data_dir eggnog_data
cd test_genomes

mkdir eggnog_out_bacteria
mkdir eggnog_out_archaea

parallel -j 2 --progress --eta 'emapper.py -m diamond --itype genome --genepred prodigal --data_dir /bigpool/robyn/picrust2_database/eggnog_data -i {} -o {/} --output_dir eggnog_out_archaea/ --cpu 12 --scratch_dir /scratch/robyn/' ::: genomes_archaea/*.fna
#not quite all done - need to rerun

parallel -j 2 --progress --eta 'emapper.py -m diamond --itype genome --genepred prodigal --data_dir /bigpool/robyn/picrust2_database/eggnog_data -i {} -o {/} --output_dir eggnog_out_bacteria/ --cpu 12 --scratch_dir /scratch/robyn/' ::: genomes_bacteria/*.fna
#missing at least one!
```

## Put together mock samples

```{python}
import pandas as pd

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']
folder = '/bigpool/robyn/picrust2_database/'

tax_using = {}
asv_classifications = {}

for row in open(folder+'picrust_datasets_test/tax_names.txt', 'r'):
  row = row.replace('\n', '').split('\t')
  tax_using[row[0]] = row[1]

for ds in datasets:
  classification = pd.read_csv(folder+'picrust_datasets_test/qiime2_classification/'+ds+'/taxonomy.tsv', sep='\t', index_col=0, header=0)
  for row in classification.index.values:
    asv_classifications[row] = tax_using[classification.loc[row, 'Taxon']]

genome_tax = {}
for row in open(folder+'test_genomes/tax_genome_info.txt', 'r'):
  row = row.replace('\n', '').split('\t')
  genome_tax[row[0].replace(';', '; ')] = row[1]

for ds in datasets:
  #if ds != 'blueberry': continue
  ft = pd.read_csv(folder+'picrust_datasets_test/data/'+ds+'_16S.biom.tsv', sep='\t', index_col=0, header=1)
  ft = ft.rename(index=asv_classifications)
  ft = ft.groupby(by=ft.index, axis=0).sum()
  ft['Tax'] = ft.index.values
  ft = ft.rename(index=genome_tax)
  ft.to_csv(folder+'test_genomes/feature_tables/'+ds+'_16S.csv')
```

```{python}
import pandas as pd
import os
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
import random

genomes_with_16S = os.listdir('16S_bacteria')+os.listdir('16S_archaea')
#811 genomes
genomes_with_16S = [g.split('_')[1].split('.')[0] for g in genomes_with_16S]
genes_16S = {}
for folder in ['16S_bacteria', '16S_archaea']:
  for genome in os.listdir(folder):
    gen_name = genome.split('_')[1].split('.')[0]
    for record in SeqIO.parse(folder+'/'+genome, "fasta"):
      if gen_name not in genes_16S:
        genes_16S[gen_name] = [record.seq]
      else:
        genes_16S[gen_name].append(record.seq)

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']
for ds in datasets:
  ft = pd.read_csv('feature_tables/'+ds+'_16S.csv', index_col=0, header=0)
  if 'Unassigned' in ft.index.values:
    ft = ft.drop('Unassigned', axis=0)
  dropping = []
  for row in ft.index.values:
    if row.split('_')[1].split('.')[0] not in genomes_with_16S:
      dropping.append(row)
  drop_ra = ft.copy(deep=True)
  drop_ra = drop_ra.drop('Tax', axis=1)
  drop_ra = drop_ra.divide(drop_ra.sum(axis=0), axis=1).multiply(100)
  drop_ra = drop_ra.loc[dropping, :]
  ft = ft.drop(dropping, axis=0)
  ft.to_csv('feature_tables/'+ds+'_16S_mock.csv')
  ft = ft.drop('Tax', axis=1)
  print(ds, ft.shape[0])
  for col in ft.columns:
    print(col)
    this_sample = []
    count = 0
    for row in ft.index.values:
      genome = row.split('_')[1].split('.')[0]
      for i in range(int(ft.loc[row, col])):
        count += 1
        name = 'Sequence_'+str(count)+'_'+row
        name = col+'_'+name.replace('_', '-')
        if len(genes_16S[genome]) == 1:
          this_sample.append(SeqRecord(genes_16S[genome][0], id=name, description=''))
        else:
          this_sample.append(SeqRecord(random.choice(genes_16S[genome]), id=name, description=''))
    SeqIO.write(this_sample, "sample_fasta/"+ds+"/"+col+".fasta", "fasta")
          
  
```

Get actual tax for mocks:
```{python, eval=FALSE}
import pandas as pd

# bac_sum = pd.read_csv('test_genomes/bacteria_assembly_summary.txt', index_col=0, header=1, sep='\t')
# arc_sum = pd.read_csv('test_genomes/archaea_assembly_summary.txt', index_col=0, header=1, sep='\t')
# gb_sum = pd.read_csv('test_genomes/assembly_summary_genbank.txt', index_col=0, header=1, sep='\t')
# ncbi = pd.concat([bac_sum, arc_sum, gb_sum])
# ncbi_genomes = set(ncbi.index.values)

bac = pd.read_csv('/bigpool/robyn/picrust2_database/GTDB_r214/bac120_metadata_r214.tsv', index_col=0, header=0, sep='\t')
arc = pd.read_csv('/bigpool/robyn/picrust2_database/GTDB_r214/ar53_metadata_r214.tsv', index_col=0, header=0, sep='\t')
gtdb = pd.concat([bac, arc])
rename = {}
for row in gtdb.index.values:
  rename[row] = row.split('_', 1)[1]

gtdb = gtdb.rename(index=rename)


datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']
all_genome_tax = {}

for ds in datasets:
  ft = pd.read_csv('feature_tables/'+ds+'_16S.csv', index_col=0, header=0)
  for row in ft.index.values:
    if row in gtdb.index.values:
      if row in all_genome_tax: continue
      else:
        all_genome_tax[row] = gtdb.loc[row, 'gtdb_taxonomy']
    else:
      print(row)

with open('genome_full_tax.csv', 'w') as f:
  w = f.write('Genome,Taxonomy\n')
  for genome in all_genome_tax:
    w = f.write(genome+','+all_genome_tax[genome]+'\n')
```

## Processing of mock samples

### Full length

```{bash, eval=FALSE}
parallel -j 2 'cat {}/* > {}.fasta' ::: blueberry cameroon hmp indian mammal ocean primate

vsearch --cluster_fast blueberry.fasta --id 1 --uc blueberry_clusters.uc --threads 12
vsearch --cluster_fast cameroon.fasta --id 1 --uc cameroon_clusters.uc --threads 12
vsearch --cluster_fast hmp.fasta --id 1 --uc hmp_clusters.uc --threads 12
vsearch --cluster_fast indian.fasta --id 1 --uc indian_clusters.uc --threads 12
vsearch --cluster_fast mammal.fasta --id 1 --uc mammal_clusters.uc --threads 12
vsearch --cluster_fast ocean.fasta --id 1 --uc ocean_clusters.uc --threads 12
vsearch --cluster_fast primate.fasta --id 1 --uc primate_clusters.uc --threads 12
```

Now make the files with representative sequences:
```{python}
import pandas as pd
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
import os

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']

for ds in datasets:
  if ds != 'indian': continue
  clusters = {}
  count = 0
  for row in open(ds+'_clusters.uc', 'r'):
    #if count % 1000 == 0: print(ds, count)
    row = row.replace('\n', '').split('\t')
    seq1, seq2 = row[8], row[9]
    if seq2 == '*':
      if seq1 not in clusters:
        clusters[seq1] = set([])
      continue
    if seq1 in clusters:
      clusters[seq1].add(seq2)
    elif seq2 in clusters:
      clusters[seq2].add(seq1)
    else:
      clusters[seq1] = set([seq2])
    count += 1
  seqs = {}
  for record in SeqIO.parse(ds+'.fasta', "fasta"):
    seqs[record.id] = [record.seq, len(str(record.seq))]
  samples = [s.replace('.fasta', '') for s in os.listdir(ds)]
  rep_seqs_list, rep_seqs_count = [], []
  for cluster in clusters:
    clusters[cluster].add(cluster)
    best_seq, length = cluster, seqs[cluster][1]
    cluster_samples = []
    for seq in clusters[cluster]:
      if seqs[seq][1] > length:
        best_seq = seq
        length = seqs[seq][1]
      cluster_samples.append(seq.split('_Sequence')[0])
    rep_seqs_list.append(SeqRecord(seqs[best_seq][0], id=best_seq, description=''))
    #rep_seqs_count.append([best_seq, len(clusters[cluster])])
    this_cluster_count = [best_seq]
    for sample in samples:
      this_cluster_count.append(cluster_samples.count(sample))
    rep_seqs_count.append(this_cluster_count)
  SeqIO.write(rep_seqs_list, ds+'_rep_seqs.fna', "fasta")
  rep_seqs_count = pd.DataFrame(rep_seqs_count, columns=['Representative']+samples).set_index('Representative')
  rep_seqs_count.to_csv(ds+'_feature_table.tsv', sep='\t')
        
```

### V4-V5

```{bash, eval=FALSE}
cd sample_fasta
parallel -j 2 --progress 'cutadapt -a GTGYCAGCMGCCGCGGTAA -g CCGYCAATTYMTTTRAGTTT -o trimmed_{}.fasta {}.fasta' ::: blueberry cameroon hmp indian mammal ocean primate
parallel -j 2 --progress 'cutadapt -a GTGYCAGCMGCCGCGGTAA -g CCGYCAATTYMTTTRAGTTT -o trimmed_min200_{}.fasta -m 200 --rc {}.fasta' ::: blueberry cameroon hmp indian mammal ocean primate

mkdir cutadapt_first
mv trimmed_* cutadapt_first/

parallel -j 2 --progress 'cutadapt --action trim -b GTGYCAGCMGCCGCGGTAA -o trim1_{}.fasta --rc {}.fasta' ::: blueberry cameroon hmp indian mammal ocean primate
parallel -j 2 --progress 'cutadapt --action trim -b CCGYCAATTYMTTTRAGTTT -o trim2_{}.fasta --rc -m 200 trim1_{}.fasta' ::: blueberry cameroon hmp indian mammal ocean primate
```

```{bash, eval=FALSE}
parallel -j 1 'vsearch --cluster_fast trim2_{}.fasta --id 1 --uc {}_trimmed_clusters.uc --threads 12' ::: blueberry cameroon hmp indian mammal ocean primate
```

Now make the files with representative sequences:
```{python}
import pandas as pd
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
import os

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']

for ds in datasets:
  if ds == 'indian': continue
  clusters = {}
  count = 0
  for row in open(ds+'_trimmed_clusters.uc', 'r'):
    #if count % 1000 == 0: print(ds, count)
    row = row.replace('\n', '').split('\t')
    seq1, seq2 = row[8], row[9]
    if seq2 == '*':
      if seq1 not in clusters:
        clusters[seq1] = set([])
      continue
    if seq1 in clusters:
      clusters[seq1].add(seq2)
    elif seq2 in clusters:
      clusters[seq2].add(seq1)
    else:
      clusters[seq1] = set([seq2])
    count += 1
  seqs = {}
  for record in SeqIO.parse(ds+'.fasta', "fasta"):
    seqs[record.id] = [record.seq, len(str(record.seq))]
  samples = [s.replace('.fasta', '') for s in os.listdir(ds)]
  rep_seqs_list, rep_seqs_count = [], []
  for cluster in clusters:
    clusters[cluster].add(cluster)
    best_seq, length = cluster, seqs[cluster][1]
    cluster_samples = []
    for seq in clusters[cluster]:
      if seqs[seq][1] > length:
        best_seq = seq
        length = seqs[seq][1]
      cluster_samples.append(seq.split('_Sequence')[0])
    rep_seqs_list.append(SeqRecord(seqs[best_seq][0], id=best_seq, description=''))
    #rep_seqs_count.append([best_seq, len(clusters[cluster])])
    this_cluster_count = [best_seq]
    for sample in samples:
      this_cluster_count.append(cluster_samples.count(sample))
    rep_seqs_count.append(this_cluster_count)
  SeqIO.write(rep_seqs_list, ds+'_rep_seqs_trimmed.fna', "fasta")
  rep_seqs_count = pd.DataFrame(rep_seqs_count, columns=['Representative']+samples).set_index('Representative')
  rep_seqs_count.to_csv(ds+'_feature_table_trimmed.tsv', sep='\t')
        
```

## Make metagenome predictions

Get 16S copies for each genome:
```{python}
import pandas as pd
from Bio import SeqIO
import os

genomes = ['16S_archaea/'+s for s in os.listdir('16S_archaea')]+['16S_bacteria/'+s for s in os.listdir('16S_bacteria')]
counts = []
for genome in genomes:
  seqs = 0
  for record in SeqIO.parse(genome, "fasta"):
    seqs += 1
  counts.append([genome.split('/')[1], seqs])

with open('counts_16S.csv', 'w') as f:
  w = f.write('Genome,16S_copies\n')
  for row in counts:
    genome = row[0].split('_')
    genome = genome[0]+'_'+genome[1]
    w = f.write(genome+','+str(row[1])+'\n')

```

Divide the feature tables by the 16S copies:
```{python}
import pandas as pd

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']
counts_16S = pd.read_csv('counts_16S.csv', index_col=0, header=0)
rename = {}
for row in counts_16S.index.values:
  rename[row] = row.split('_')[1].split('.')[0]

counts_16S = counts_16S.rename(index=rename)


for ds in datasets:
  mock = pd.read_csv('feature_tables/'+ds+'_16S_mock.csv', index_col=0, header=0).drop('Tax', axis=1)
  #print(mock)
  count = 0
  for row in mock.index.values:
    try:
      copy = counts_16S.loc[row.split('_')[1].split('.')[0], '16S_copies']
      mock.loc[row, :] = mock.loc[row, :].values/copy
    except:
      print(row)
      count += 1
  print(count)
  mock.to_csv('feature_tables/'+ds+'_16S_mock_normalised.csv')
```

Collate predictions for all genomes included:
```{python}
import os
import pandas as pd
import pickle
from multiprocessing import Pool
from multiprocessing import freeze_support
from multiprocessing import Process, Manager

eggnog_out = ['eggnog_out_archaea/'+gen for gen in os.listdir('eggnog_out_archaea/')]+['eggnog_out_bacteria/'+gen for gen in os.listdir('eggnog_out_bacteria/')]
eggnog_out = [e for e in eggnog_out if '.annotations' in e]

column_names = ['Preferred_name', 'GOs', 'EC', 'KEGG_ko', 'CAZy', 'BiGG_Reaction', 'PFAMs']
columns = [8, 9, 10, 11, 18, 19, 20]
lists = {'Preferred_name':set(), 'GOs':set(), 'EC':set(), 'KEGG_ko':set(), 'CAZy':set(), 'BiGG_Reaction':set(), 'PFAMs':set()}

count = 0
for genome in eggnog_out:
  if count % 10 == 0: print(count)
  count2 = 0
  for row in open(genome, 'r'):
    if count2 > 4:
      row = row.replace('\n', '').split('\t')
      try:
        for c in range(len(columns)):
          #lists[column_names[c]] += row[columns[c]]+','
          #lists[column_names[c]].append(row[columns[c]])
          val = row[columns[c]]
          if ',' in val:
            val = val.split(',')
            for v in val:
              lists[column_names[c]].add(v)
          else:
            lists[column_names[c]].add(val)
      except:
        couldnt_do = True
      #print(len(row.split('\t')), row.split('\t'))
    #if count2 > 10: break
    count2 += 1
  count += 1

with open('eggnog_lists.dict', 'wb') as f:
    pickle.dump(lists, f)
# with open('eggnog_lists.dict', 'rb') as f:
#     lists = pickle.load(f)

#check format of lists
for l in lists:
  print(l, len(lists[l]), list(lists[l])[:10])
    
for l in lists:
  if l == 'GOs':
    for gene in lists[l]:
      if 'GO:' not in gene: 
        print(gene)
  if l == 'EC':
    for gene in lists[l]:
      if gene.count('.') < 3:
        print(gene)
  if l == 'KEGG_ko':
    for gene in lists[l]:
      if 'ko:' not in gene:
        print(gene)

#this is only giving '-' for each of these, so I think that I'm safe to use the lists

# for l in lists:
#   with open('gtdb_picrust_ref/'+l+'_present_in_database.txt', 'w') as f:
#     for gene in lists[l]:
#       if gene != '-':
#         w = f.write(gene+'\n')

# def add_to_file(fn, line):
#   with open(fn, 'a') as f:
#     w = f.write('\t'.join(line)+'\n')
#   return
# 
# def make_files(fn, line):
#   with open(fn, 'w') as f:
#     w = f.write('\t'.join(line)+'\n')
#   return
def add_to_file(fn, line):
  with open(fn, 'a') as f:
    w = f.write(line)
  return

def make_files(fn, line):
  with open(fn, 'w') as f:
    w = f.write(line)
  return

def process_genome(genome):
  print(genome)
  this_genome = {}
  for l in lists:
    this_genome[l] = []
  count2 = 0
  for row in open(''+genome, 'r'):
    if count2 > 4:
      row = row.replace('\n', '').split('\t')
      try:
        for c in range(len(columns)):
          val = row[columns[c]]
          if ',' in val:
            val = val.split(',')
            for v in val:
              this_genome[column_names[c]].append(v)
          else:
            this_genome[column_names[c]].append(val)
      except:
        couldnt_do = True
    count2 += 1
  for l in lists:
    this_genome_counts = []
    for gene in lists[l]:
      if gene == '-': continue
      this_genome_counts.append(this_genome[l].count(gene))
    this_genome_counts = [str(int(s)) for s in this_genome_counts]
    this_genome_counts = genome.replace('.emapper.annotations', '')+'\t'+'\t'.join(this_genome_counts)+'\n'
    add_to_file(''+l+'.txt', this_genome_counts)
    #add_to_file('gtdb_picrust_ref/'+l+'.txt', [genome.replace('.emapper.annotations', '')]+[str(int(s)) for s in this_genome_counts])
    #print(len(this_genome_counts), this_genome_counts[-10:])
    #print([genome.replace('.emapper.annotations', '')+'\t'.join(this_genome_counts[-100:])+'\n'])
  return

for l in lists:
  make_files(''+l+'.txt', 'assembly\t'+'\t'.join(list(lists[l]))+'\n')
  
def run_multiprocessing(func, i, n_processors):
    with Pool(processes=n_processors) as pool:
        return pool.map(func, i)

eggnog_out = ['eggnog_out_bacteria/GCA_000169055.1_ASM16905v1_genomic.fna.emapper.annotations', 'eggnog_out_bacteria/GCA_000162175.1_ASM16217v1_genomic.fna.emapper.annotations', 'eggnog_out_archaea/GCA_903836875.1_freshwater_MAG_---_MJ120620B_bin-020_genomic.fna.emapper.annotations']

def main():
    print('Starting processing')
    run_multiprocessing(process_genome, eggnog_out, 12)

if __name__ == "__main__":
    freeze_support()   # required to use multiprocessing
    main()

genomes = list(pd.read_csv('counts_16S.csv', index_col=0, header=0).index.values)
###CHECK!!!
traits = ['KEGG_ko', 'EC']
for l in traits:
  print(l)
  trait = pd.read_csv(l+'.txt', index_col=0, header=0, sep='\t')
  rename = {}
  for row in trait.index.values:
    new_name = row.split('/')[1].split('_')
    new_name = new_name[0]+'_'+new_name[1]
    rename[row] = new_name
  trait = trait.rename(index=rename)
  # for g in genomes:
  #   if g not in trait.index.values:
  #     print(g)
  trait = trait.loc[genomes, :]
  trait = trait.transpose()
  trait = trait[trait.max(axis=1) > 0]
  trait = trait.transpose()
  trait.to_csv(l+'_genomes.txt', sep='\t')
  print(trait.isnull().values.any())
#False
#False
```

Rerun eggnog on the missing ones:
```{bash, eval=FALSE}
conda activate eggnog

parallel -j 2 --progress --eta 'emapper.py -m diamond --itype genome --genepred prodigal --data_dir /bigpool/robyn/picrust2_database/eggnog_data -i {} -o {/} --output_dir eggnog_out_archaea/ --cpu 12 --scratch_dir /scratch/robyn/' ::: genomes_archaea/GCA_903836875.1_freshwater_MAG_---_MJ120620B_bin-020_genomic.fna
#not quite all done - need to rerun

parallel -j 2 --progress --eta 'emapper.py -m diamond --itype genome --genepred prodigal --data_dir /bigpool/robyn/picrust2_database/eggnog_data -i {} -o {/} --output_dir eggnog_out_bacteria/ --cpu 12 --scratch_dir /scratch/robyn/' ::: genomes_bacteria/GCA_000169055.1_ASM16905v1_genomic.fna genomes_bacteria/GCA_000162175.1_ASM16217v1_genomic.fna
```
Then I reran the script above.

Turn these into metagenome predictions by multiplying abundances:
Note that I initially did this with 16S normalisation (dividing by copy number), but I think I shouldn't have done that as PICRUSt2 should be normalising for that anyway?
```{python}
import pandas as pd
import os

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']

for trait in ['EC', 'KEGG_ko']:
  if trait != 'EC': continue
  trait_df = pd.read_csv(trait+'_genomes.txt', sep='\t', index_col=0, header=0)
  trait_df[trait_df > 10] = 10
  for ds in datasets:
    #if ds != 'blueberry': continue
    if os.path.exists('feature_tables/'+ds+'_mock_'+trait+'not_16S_norm_metagenome.csv'): continue
    ft = pd.read_csv('feature_tables/'+ds+'_16S_mock.csv', index_col=0, header=0).drop('Tax', axis=1)
    # for genome in ft.index.values:
    #   if genome not in trait_df.index.values:
    #     if genome.replace('GCA', 'GCF').replace('GCF', 'GCA') not in trait_df.index.values:
    #       print(genome)
    sample_mg = []
    for sample in ft.columns:
      sample_genomes = {}
      for row in ft.index.values:
        if ft.loc[row, sample] > 0:
          if row not in trait_df.index.values:
            if row.replace('GCF', 'GCA') in trait_df.index.values:
              name = row.replace('GCF', 'GCA')
            elif row.replace('.1', '.3') in trait_df.index.values:
              name = row.replace('.1', '.3')
            elif row.replace('.1', '.2') in trait_df.index.values:
              name = row.replace('.1', '.2')
          else: name = row
          sample_genomes[name] = [ft.loc[row, sample], row]
      trait_red = trait_df.copy(deep=True).loc[sample_genomes, :]
      for genome in sample_genomes:
        trait_red.loc[genome, :] = trait_red.loc[genome, :].values*sample_genomes[genome][0]
      trait_red = trait_red.transpose()
      trait_red[sample] = trait_red.sum(axis=1)
      trait_red = trait_red.loc[:, [sample]]
      sample_mg.append(trait_red)
    ds_mg = pd.concat(sample_mg).fillna(value=0)
    ds_mg = ds_mg.groupby(by=ds_mg.index, axis=0).sum()
    ds_mg.to_csv('feature_tables/'+ds+'_mock_'+trait+'_not_16S_norm_metagenome.csv')
```

# Incorporating changes into code

- Made v2.6.0 branch
- Added new reference files to the default_files folder

First:
- Fix check files (in pipeline.py)

Order of calls:
- scripts/picrust2_pipeline.py: calls picrust2.default (for default files) and then picrust2.pipeline full_pipeline
- picrust2/default.py: currently gets default ref_dir, etc. for all necessary files
- picrust2/pipeline.py: runs picrust2.place_seqs, hsp.py for each of the functions, run metagenome_pipeline.py

So, we'll need to first of all add the new default files, and then make sure that we first run hsp.py twice for each of the default files, make a new script to get the best matches, and then run the next HSP with only the ASVs that match best, before making a final combined file for each of the annotations, and then running the metagenome_pipeline on the single combined file

## Install this version of PICRUSt2

```{bash, eval=FALSE}
cd /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2
conda env create -n picrust-v2.6.0-dev -f picrust2-env.yaml
conda activate picrust-v2.6.0-dev
pip install --editable .
conda install r-essentials
```

## Added check for duplicated IDs

Test:
```{bash, eval=FALSE}
cd /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting/
#just copied two of the ASV ids, but didn't change the feature table
picrust2_pipeline.py -s chemerin_16S/seqs.fna -i chemerin_16S/table.biom -o picrust2_out_pipeline_dup_test -p 8
```

This works fine - have pushed the commit to the 2.6.0 branch

## Mapping files

Taking from Humann3 data: https://github.com/biobakery/humann/tree/master/humann/data/utility_DEMO

Need:
1. ec_level4_to_metacyc_rxn.tsv
2. KEGG_modules_to_KO.tsv
3. KEGG_pathways_to_KO.tsv
4. metacyc_path2rxn_struc_filt_pro.txt
5. metacyc_pathways_structured_filtered
6. metacyc_rxn_to_level4ec.tsv

So these are: 
1. Opposite of 6?
2. Find
3. Find
4. Filter based on metacyc website
5. HUMAnN3: metacyc_pathways_structured_filtered_v24_subreactions.txt
6. HUMAnN3: metacyc_reactions_level4ec_only.uniref

Will have to:
- Get file 1, mapping EC level 4 to metacyc - done
- Filter EC level 4 to metacyc to only included ECs
- Subset functions to only those included in PICRUSt2
- Find mapping from KEGG pathways to KO
- Find mapping from KEGG modules to KO
- Probably just update the ones there? Make sure that whatever is in them remains in them, or keep them the same for pro_ref?

Running regular just to test:
```{bash, eval=FALSE}
cd /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting
picrust2_pipeline.py -s chemerin_16S/seqs.fna -i chemerin_16S/table.biom -o picrust2_out_pipeline_def -p 8
```

So default is to run KO and EC and then use the ECs for pathway prediction.

Get file 1, EC level 4 to metacyc reaction:
```{python}
reaction_to_ec = {}
count = 0
rxn_to_ec_no_uniref = []
for row in open('/Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/new_database/new_reference/HUMAnN3/metacyc_reactions_level4ec_only.uniref', 'r'):
  row = row.replace('\n', '').split('\t')
  this_row = []
  for r in row[1:]:
    if 'UniRef' not in r:
      this_row.append(r)
  if len(this_row) == 1:
    reaction_to_ec[row[0]] = this_row[0]
  rxn_to_ec_no_uniref.append(row[0]+'\t'+'\t'.join(this_row)+'\n')
  count += 1

with open('/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/pathway_mapfiles/metacyc_rxn_to_level4ec_new.tsv', 'w') as f:
  for row in rxn_to_ec_no_uniref:
    w = f.write(row)

print(len(reaction_to_ec))

ec_to_rxn = {}
for rxn in reaction_to_ec:
  ec = reaction_to_ec[rxn]
  if ec in ec_to_rxn:
    ec_to_rxn[ec].append(rxn)
  else:
    ec_to_rxn[ec] = [rxn]

with open('/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/pathway_mapfiles/ec_level4_to_metacyc_rxn_new.tsv', 'w') as f:
  for ec in ec_to_rxn:
    w = f.write(ec+'\t'+','.join(ec_to_rxn[ec])+'\n')

finished = True
```

## Run and make separate split pipelines (no pathways)

```{bash, eval=FALSE}
cd /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting

picrust2_pipeline.py \
    -s chemerin_16S/seqs.fna \
    -i chemerin_16S/table.biom \
    -o picrust2_out_pipeline_bac \
    -p 8 \
    --ref_dir /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/bac_ref     --custom_trait_tables /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/ec.txt.gz,/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/go.txt.gz,/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/ko.txt.gz,/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/pfam.txt.gz \
    --marker_gene_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/16S.txt.gz \
    --no_pathways \
    --no_regroup \
    --verbose

picrust2_pipeline.py \
    -s chemerin_16S/seqs.fna \
    -i chemerin_16S/table.biom \
    -o picrust2_out_pipeline_arc \
    -p 8 \
    --ref_dir /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/arc_ref     --custom_trait_tables /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/ec.txt.gz,/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/go.txt.gz,/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/ko.txt.gz,/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/pfam.txt.gz \
    --marker_gene_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/16S.txt.gz \
    --no_pathways \
    --no_regroup \
    --verbose

#these ran fine, only all of the sequences were > NSTI 2 for archaea so it stopped before the metagenome pipeline

#add the other databases that I didn't use yet, just to test that the tables work
hsp.py --observed_trait_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/bigg_reaction.txt.gz -t /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting/picrust2_out_pipeline_bac/out.tre -o /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting/picrust2_out_pipeline_bac/bigg_reaction.txt_predicted.tsv.gz -p 8

hsp.py --observed_trait_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/cazy.txt.gz -t /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting/picrust2_out_pipeline_bac/out.tre -o /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting/picrust2_out_pipeline_bac/cazy.txt_predicted.tsv.gz -p 8

hsp.py --observed_trait_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/preferred_name.txt.gz -t /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting/picrust2_out_pipeline_bac/out.tre -o /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting/picrust2_out_pipeline_bac/preferred_name.txt_predicted.tsv.gz -p 8
```

Now we'll work on what this would look like for the full pipeline:
Replaced two of the ASVs in seqs_arc.fna with archaeal sequences, changed one bp.
```{bash, eval=FALSE}
mkdir picrust2_out_pipeline_combined
place_seqs.py -s chemerin_16S/seqs_arc.fna -o picrust2_out_pipeline_combined/bac_out.tre -p 4 --intermediate picrust2_out_pipeline_combined/intermediate_bac --ref_dir /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/bac_ref

place_seqs.py -s chemerin_16S/seqs_arc.fna -o picrust2_out_pipeline_combined/arc_out.tre -p 4 --intermediate picrust2_out_pipeline_combined/intermediate_arc --ref_dir /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/arc_ref

hsp.py --observed_trait_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/16S.txt.gz -t picrust2_out_pipeline_combined/bac_out.tre -o picrust2_out_pipeline_combined/bac_marker_nsti_predicted.tsv.gz -p 8 -n

hsp.py --observed_trait_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/16S.txt.gz -t picrust2_out_pipeline_combined/arc_out.tre -o picrust2_out_pipeline_combined/arc_marker_nsti_predicted.tsv.gz -p 8 -n
```

### picrust2/split_domains.py

Make new script called "picrust2/split_domains.py"
We'll also make a wrapper script for this in "scripts/pick_best_domain.py", and then we'll make another one for when combining the predictions after HSP ("scripts/combine_domains.py"), but the functions needed will all be in split_domains.py

I've been running this now to get the output files, so lets try the next step:
```{bash, eval=FALSE}
hsp.py --observed_trait_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/ec.txt.gz -t picrust2_out_pipeline_combined/bac_best_out.tre -o picrust2_out_pipeline_combined/bac_best_ec_predicted.tsv.gz -p 8

hsp.py --observed_trait_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/ec.txt.gz -t picrust2_out_pipeline_combined/arc_best_out.tre -o picrust2_out_pipeline_combined/arc_best_ec_predicted.tsv.gz -p 8
```

Used the combine_domain_functions function to combine these. Now do metagenome prediction:
```{bash, eval=FALSE}
metagenome_pipeline.py -i chemerin_16S/table.biom \
                       -m picrust2_out_pipeline_combined/marker_nsti_predicted.tsv.gz \
                       -f picrust2_out_pipeline_combined/ec_predicted.tsv.gz \
                       -o picrust2_out_pipeline_combined/EC_metagenome_out
```

It works!!

### Now we'll try the pathways

```{bash, eval=FALSE}
pathway_pipeline.py -i picrust2_out_pipeline_combined/EC_metagenome_out/pred_metagenome_unstrat.tsv.gz \
                    -o picrust2_out_pipeline_combined/pathways_out \
                    --intermediate picrust2_out_pipeline_combined/minpath_working \
                    -m /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/new_database/new_reference/HUMAnN3/metacyc_pathways_structured_filtered_v24_subreactions.txt \
                    --regroup_map /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/pathway_mapfiles/ec_level4_to_metacyc_rxn_new.tsv \
                    -p 8
```

And this also works!!

### Intermediate wrapping scripts

So now I just need to write the intermediate wrapping scripts and then it can be run all the way through.

Try the pick_best_domain.py script:
```{bash, eval=FALSE}
cd picrust2_out_pipeline_combined

python /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/pick_best_domain.py \
              --tree_dom1 bac_out.tre \
              --tree_dom2 arc_out.tre \
              --ref_fasta_dom1 /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/bac_ref/bac_ref.fna \
              --ref_fasta_dom2 /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/archaea/arc_ref/arc_ref.fna \
              --tree_out_dom1 bac_out_best_2.tre \
              --tree_out_dom2 arc_out_best_2.tre \
              --nsti_table_dom1 bac_marker_nsti_predicted.tsv.gz \
              --nsti_table_dom2 arc_marker_nsti_predicted.tsv.gz \
              --nsti_table_out_dom1 bac_best_2_marker_nsti_predicted.tsv.gz \
              --nsti_table_out_dom2 arc_best_2_marker_nsti_predicted.tsv.gz \
              --nsti_table_out_combined combined_best_2_marker_nsti_predicted.tsv.gz \
              -n1 'bac' \
              -n2 'arc'
```

This all works!

### combine_domains.py

So now make the scripts/combine_domains.py script.

Try running this:
```{bash, eval=FALSE}
python /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/combine_domains.py --table_dom1 arc_best_ec_predicted.tsv.gz --table_dom2 bac_best_ec_predicted.tsv.gz -o combined_ec_predicted.tsv.gz
```

This also works!! So tomorrow will be joining these together into one big pipeline.

### One big pipeline

Made one big pipeline. Time to test it:
```{bash, eval=FALSE}
python /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/picrust2_pipeline_split.py -s chemerin_16S/seqs_arc.fna -i chemerin_16S/table.biom -o picrust2_out_split_pipeline_4 -p 10 --verbose
```
Had to make small fixes to get it to run, but it all eventually worked! 

And test now that I've added the genome output:
```{bash, eval=FALSE}
python /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/picrust2_pipeline_split.py -s chemerin_16S/seqs_arc.fna -i chemerin_16S/table.biom -o picrust2_out_split_pipeline_genome -p 10 --verbose
```

### Clone to vulcan

Now clone this branch to vulcan:
```{bash, eval=FALSE}
git clone --single-branch -b PICRUSt2-v2.6.0 https://github.com/picrust/picrust2
mv picrust2 picrust2-v2.6.0
cd picrust2-v2.6.0
conda env create -n picrust2-v2.6.0 -f picrust2-env.yaml

pip install --editable .
#pytest

conda rename -n picrust2-v2.6.0 picrust2-v2.6.0-old
mv picrust2-v2.6.0 picrust2-v2.6.0-old
git clone --single-branch -b PICRUSt2-v2.6.0 https://github.com/picrust/picrust2
mv picrust2 picrust2-v2.6.0
cd picrust2-v2.6.0
conda env create -n picrust2-v2.6.0 -f picrust2-env.yaml

pip install --editable .
```

Test!!!
```{bash, eval=FALSE}
mkdir picrust_test_v2.6.0
cp -r picrust_test/chemerin_16S/ picrust_test_v2.6.0/
cd picrust_test_v2.6.0/
picrust2_pipeline_split.py -s chemerin_16S/seqs.fna -i chemerin_16S/table.biom -o picrust2_out_split_pipeline -p 12 --verbose
```
A few small fixed, but it works!!

Remove first lines of files:
```{python}
import pandas as pd

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']
for ds in datasets:
  ft = pd.read_csv('/bigpool/robyn/picrust2_database/picrust_datasets_test/data/'+ds+'_16S.biom.tsv', index_col=0, header=1, sep='\t')
  ft.to_csv('/bigpool/robyn/picrust2_database/picrust_datasets_test/data/'+ds+'_16S.tsv', sep='\t')
```

### Now run the test data from the previous paper

```{bash, eval=FALSE}
parallel -j 1 'picrust2_pipeline_split.py -s /bigpool/robyn/picrust2_database/picrust_datasets_test/data/{}_16S_rep_seqs.fasta -i /bigpool/robyn/picrust2_database/picrust_datasets_test/data/{}_16S.tsv -o picrust2_out_split_{} -p 12 --verbose' ::: blueberry cameroon hmp indian mammal ocean primate
```

Now get the outputs from the previous paper:
```{bash, eval=FALSE}
cd ..
mkdir picrust_test_v2.6.0/picrust2_default
parallel -j 1 'cp picrust2_manuscript/data/16S_validation/picrust2_out/{1}_picrust2_{2}_nsti2.0.tsv picrust_test_v2.6.0/picrust2_default/' ::: blueberry cameroon hmp indian mammal ocean primate ::: ko ec path
```

Copied this all across locally:
```{bash, eval=FALSE}
gunzip picrust_test_v2.6.0/picrust2_out_split_*/combined_EC_predicted.tsv.gz
gunzip picrust_test_v2.6.0/picrust2_out_split_*/combined_KO_predicted.tsv.gz
gzip picrust_test_v2.6.0/picrust2_out_split_*/combined_EC_predicted.tsv
gzip picrust_test_v2.6.0/picrust2_out_split_*/combined_KO_predicted.tsv
gunzip picrust_test_v2.6.0/picrust2_out_split_*/EC_metagenome_out/pred_metagenome_unstrat.tsv.gz
gunzip picrust_test_v2.6.0/picrust2_out_split_*/KO_metagenome_out/pred_metagenome_unstrat.tsv.gz
gunzip picrust_test_v2.6.0/picrust2_out_split_*/pathways_out/path_abun_unstrat.tsv.gz
```

### Run default pipeline on datasets

```{bash, eval=FALSE}
cd picrust_test_v2.6.0
parallel -j 1 'picrust2_pipeline.py -s /bigpool/robyn/picrust2_database/picrust_datasets_test/data/{}_16S_rep_seqs.fasta -i /bigpool/robyn/picrust2_database/picrust_datasets_test/data/{}_16S.tsv -o picrust2_out_default_{} -p 12 --verbose' ::: blueberry cameroon hmp indian mammal ocean primate
```

## Run PICRUSt2 on mock samples

Full-length:
```{bash, eval=FALSE}
mkdir picrust2_out
parallel -j 1 'picrust2_pipeline_split.py -s sample_fasta/{}_rep_seqs_trimmed.fna -i sample_fasta/{}_feature_table.tsv -o picrust2_out/picrust2_out_updated_{} -p 12 --verbose' ::: blueberry cameroon hmp indian mammal ocean primate

parallel -j 1 'picrust2_pipeline.py -s sample_fasta/{}_rep_seqs.fna -i sample_fasta/{}_feature_table.tsv -o picrust2_out/picrust2_out_default_{} -p 12 --verbose' ::: blueberry cameroon hmp indian mammal ocean primate
```

Re-run with genome output:
```{bash, eval=FALSE}
conda remove -n picrust2-v2.6.0 --all
git clone --single-branch -b PICRUSt2-v2.6.0 https://github.com/picrust/picrust2
mv picrust2 picrust2-v2.6.0
cd picrust2-v2.6.0
conda env create -n picrust2-v2.6.0 -f picrust2-env.yaml
conda activate picrust2-v2.6.0
pip install --editable .

parallel -j 3 'picrust2_pipeline_split.py -s sample_fasta/{}_rep_seqs_trimmed.fna -i sample_fasta/{}_feature_table.tsv -o picrust2_out/picrust2_out_updated_genome_{} -p 12 --verbose' ::: blueberry cameroon hmp indian mammal ocean primate
```

Trimmed:
```{bash, eval=FALSE}
mkdir picrust2_out_trim
parallel -j 1 'picrust2_pipeline_split.py -s sample_fasta/{}_rep_seqs_trimmed.fna -i sample_fasta/{}_feature_table_trimmed.tsv -o picrust2_out_trim/picrust2_out_updated_{} -p 12 --verbose' ::: blueberry cameroon hmp indian mammal ocean primate

parallel -j 1 'picrust2_pipeline.py -s sample_fasta/{}_rep_seqs_trimmed.fna -i sample_fasta/{}_feature_table_trimmed.tsv -o picrust2_out_trim/picrust2_out_default_{} -p 12 --verbose' ::: blueberry cameroon hmp indian mammal ocean primate
```

## Check EC/KO in PICRUSt2 vs in mock metagenome

```{python, eval=FALSE}
import pandas as pd

bac = pd.read_csv(filepath_or_buffer='picrust2-v2.6.0/picrust2/default_files/bacteria/ko.txt.gz', index_col=0, header=0, sep='\t')
arc = pd.read_csv(filepath_or_buffer='picrust2-v2.6.0/picrust2/default_files/archaea/ko.txt.gz', index_col=0, header=0, sep='\t')
mock_genomes = pd.read_csv('test_genomes/KEGG_ko_genomes.txt', index_col=0, header=0, sep='\t')

trait_mock = list(mock_genomes.columns)
bac_arc_traits = list(bac.columns)+list(arc.columns)
traits_unique = [t for t in trait_mock if t not in bac_arc_traits]
print(len(traits_unique), len(trait_mock), len(bac_arc_traits))

```

## Correlation between PICRUSt2 database and mock genomes

Get the database genomes in the same format as the mock genomes:
```{python, eval=FALSE}
import pandas as pd
import os

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']
pic_folder = '/bigpool/robyn/picrust2_database/picrust2-v2.6.0/picrust2/default_files/'
traits = ['EC', 'KEGG_ko']
traits_def = ['ec', 'ko']

for t in range(len(traits)):
  #if traits[t] != 'KEGG_ko': continue
  trait_df_bac = pd.read_csv(filepath_or_buffer=pic_folder+'bacteria/'+traits_def[t]+'.txt.gz', index_col=0, header=0, sep='\t')
  copy_num_bac = pd.read_csv(filepath_or_buffer=pic_folder+'bacteria/16S.txt.gz', index_col=0, header=0, sep='\t')
  trait_df_arc = pd.read_csv(filepath_or_buffer=pic_folder+'archaea/'+traits_def[t]+'.txt.gz', index_col=0, header=0, sep='\t')
  copy_num_arc = pd.read_csv(filepath_or_buffer=pic_folder+'archaea/16S.txt.gz', index_col=0, header=0, sep='\t')
  trait_df = pd.concat([trait_df_bac, trait_df_arc]).fillna(value=0)
  copy_num = pd.concat([copy_num_bac, copy_num_arc])
  for ds in datasets:
    #if ds != 'blueberry': continue
    if os.path.exists('feature_tables/'+ds+'_mock_'+traits[t]+'_metagenome_picrust2_genomes.csv'): continue
    print(traits[t], ds)
    pic_out = pd.read_csv(filepath_or_buffer='picrust2_out/picrust2_out_updated_genome_'+ds+'/combined_marker_predicted_and_nsti.tsv.gz', sep='\t', index_col=0, header=0)
    genome_dict = {}
    for row in pic_out.index.values:
      genome = row.replace('GCA-', 'GCA_').replace('GCF-', 'GCF_').split('-')[-1]
      if isinstance(pic_out.loc[row, 'closest_reference_genome'], str):
        genome_dict[genome] = pic_out.loc[row, 'closest_reference_genome']
    ft = pd.read_csv('feature_tables/'+ds+'_16S_mock.csv', index_col=0, header=0).drop('Tax', axis=1)
    drop = []
    for row in ft.index.values:
      if row not in genome_dict:
        drop.append(row)
    ft = ft.drop(drop)
    ft = ft.rename(index=genome_dict)
    ft = ft.groupby(by=ft.index).sum()
    trait_df_red = trait_df.copy(deep=True).loc[ft.index.values, :]
    copy_num_red = copy_num.copy(deep=True).loc[ft.index.values, :]
    for row in ft.index.values:
      try:
        ft.loc[row, :] = ft.loc[row, :].values/copy_num_red.loc[row, '16S_rRNA_Count'].values[0]
      except:
        ft.loc[row, :] = ft.loc[row, :].values/copy_num_red.loc[row, '16S_rRNA_Count']
    sample_mg = []
    for sample in ft.columns:
      print(sample)
      this_sample = ft.copy(deep=True).loc[:, [sample]]
      this_sample = this_sample[this_sample.max(axis=1) > 0]
      trait_sample = trait_df_red.copy(deep=True).loc[this_sample.index.values, :]
      trait_sample = trait_sample.astype(object)
      trait_sample = trait_sample.mul(this_sample.loc[:, sample].values, axis=0)
      trait_sample = trait_sample.transpose()
      trait_sample[sample] = trait_sample.sum(axis=1)
      trait_sample = trait_sample.loc[:, [sample]]
      sample_mg.append(trait_sample)
    ds_mg = pd.concat(sample_mg).fillna(value=0)
    ds_mg = ds_mg.groupby(by=ds_mg.index).sum()
    print(ds_mg)
    ds_mg.to_csv('feature_tables/'+ds+'_mock_'+traits[t]+'_metagenome_picrust2_genomes.csv')
    
    
```

Correlations between genomes:
```{python}
import pandas as pd
import os
from scipy.stats import spearmanr
from scipy.spatial import distance
import numpy as np

datasets = ['blueberry', 'cameroon', 'hmp', 'indian', 'mammal', 'ocean', 'primate']
pic_folder = '/bigpool/robyn/picrust2_database/picrust2-v2.6.0/picrust2/default_files/'
traits = ['EC', 'KEGG_ko']
traits_def = ['ec', 'ko']

for t in range(len(traits)):
  #if traits[t] == 'KEGG_ko': continue
  trait_df_bac = pd.read_csv(filepath_or_buffer=pic_folder+'bacteria/'+traits_def[t]+'.txt.gz', index_col=0, header=0, sep='\t')
  copy_num_bac = pd.read_csv(filepath_or_buffer=pic_folder+'bacteria/16S.txt.gz', index_col=0, header=0, sep='\t')
  trait_df_arc = pd.read_csv(filepath_or_buffer=pic_folder+'archaea/'+traits_def[t]+'.txt.gz', index_col=0, header=0, sep='\t')
  copy_num_arc = pd.read_csv(filepath_or_buffer=pic_folder+'archaea/16S.txt.gz', index_col=0, header=0, sep='\t')
  trait_df = pd.concat([trait_df_bac, trait_df_arc]).fillna(value=0)
  copy_num = pd.concat([copy_num_bac, copy_num_arc])
  mock_genomes = pd.read_csv(traits[t]+'_genomes.txt', index_col=0, header=0, sep='\t')
  for ds in datasets:
    #if ds != 'blueberry': continue
    if os.path.exists('correlations/'+traits_def[t]+'_'+ds+'.csv'): continue
    print(traits[t], ds)
    pic_out = pd.read_csv(filepath_or_buffer='picrust2_out/picrust2_out_updated_genome_'+ds+'/combined_marker_predicted_and_nsti.tsv.gz', sep='\t', index_col=0, header=0)
    genome_dict = {}
    comparisons = []
    for row in pic_out.index.values:
      genome = row.replace('GCA-', 'GCA_').replace('GCF-', 'GCF_').split('-')[-1]
      if not isinstance(pic_out.loc[row, 'closest_reference_genome'], str): continue
      ref_genome = pic_out.loc[row, 'closest_reference_genome']
      try:
        ref_trait = trait_df.loc[[ref_genome], :]
        mock_trait = mock_genomes.loc[[genome], :]
      except:
        #print(pic_out.loc[row, :])
        continue
      intersection = [trait for trait in ref_trait.columns if trait in mock_trait.columns]
      combined = pd.concat([ref_trait, mock_trait]).fillna(value=0).transpose()
      combined_relabun = combined.divide(combined.sum(axis=0), axis=1).multiply(100)
      X = combined_relabun.transpose().iloc[0:].values
      bc_dist_relabun = np.nan_to_num(distance.cdist(X, X, 'braycurtis'))[0][1]
      X = combined.transpose().iloc[0:].values
      bc_dist = np.nan_to_num(distance.cdist(X, X, 'braycurtis'))[0][1]
      jac_dist = np.nan_to_num(distance.cdist(X, X, 'jaccard'))[0][1]
      ref_trait_vals = ref_trait.loc[ref_genome, intersection].values
      mock_trait_vals = mock_trait.loc[genome, intersection].values
      stat, p = spearmanr(ref_trait_vals, mock_trait_vals)
      comparisons.append([genome, ref_genome, bc_dist, bc_dist_relabun, jac_dist, stat, pic_out.loc[row, 'metadata_NSTI']])
    comp = pd.DataFrame(comparisons, columns=['Genome', 'Reference genome', 'Bray-Curtis', 'Bray-Curtis relative abundance', 'Jaccard', 'Spearman', 'NSTI'])
    print(comp)
    comp.to_csv('correlations/'+traits_def[t]+'_'+ds+'.csv')
    #     genome_dict[genome] = pic_out.loc[row, 'closest_reference_genome']
    # print(pic_out)
  
```

## Troubleshooting other people's files for new version

```{bash, eval=FALSE}
conda activate picrust-v2.6.0-dev
cd /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/troubleshooting/picrust2-v2.6.0_troubleshooting

/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/picrust2_pipeline_split.py -s dna-sequences-KTU.fasta -i KTU.table.summary_relative_abundance.biom -o picrust2_out_pipeline_split -p 8 --verbose
#for some reason, a lot of the genomes are missing nearest reference genomes... troubleshoot this
```

```{bash, eval=FALSE}
/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/hsp.py --observed_trait_table /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/bacteria/16S.txt.gz -t picrust2_out_pipeline_split/bac.tre -o picrust2_out_pipeline_split/bac_nsti_test.tsv.gz -p 8 -n

#troubleshooted this and made edits needed

#now try running whole thing
picrust2_pipeline.py -s dna-sequences-KTU.fasta -i KTU.table.summary_relative_abundance.biom -o picrust2_out_pipeline_default -p 8 --verbose
#this worked fine.

#and run the split pipeline again
/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/picrust2_pipeline_split.py -s dna-sequences-KTU.fasta -i KTU.table.summary_relative_abundance.biom -o picrust2_out_pipeline_split_2 -p 8 --verbose
```

Made add_descriptions_split.py. Test:
```{bash, eval=FALSE}
#add_descriptions_split.py

/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/add_descriptions_split.py \
  -i picrust2_out_pipeline_split_2/EC_metagenome_out/pred_metagenome_unstrat.tsv.gz \
  -m EC \
  -o picrust2_out_pipeline_split_2/EC_metagenome_out/pred_metagenome_unstrat_descrip.tsv.gz
  
/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/add_descriptions_split.py \
  -i picrust2_out_pipeline_split_2/pathways_out/path_abun_unstrat.tsv.gz \
  -m METACYC \
  -o picrust2_out_pipeline_split_2/pathways_out/path_abun_unstrat_descrip.tsv.gz
  
/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/add_descriptions_split.py \
  -i picrust2_out_pipeline_split_2/KO_metagenome_out/pred_metagenome_unstrat.tsv.gz \
  -m KO \
  -o picrust2_out_pipeline_split_2/KO_metagenome_out/pred_metagenome_unstrat_descrip.tsv.gz
```

Modify KO:
```{python}
import pandas as pd
ko = pd.read_csv(filepath_or_buffer='/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/description_mapfiles/ko_name.txt.gz', index_col=0, header=None, sep='\t')
rename_ko = {}
for row in ko.index.values:
  rename_ko[row] = 'ko:'+row

ko = ko.rename(index=rename_ko)
ko.to_csv(path_or_buf='/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/picrust2/default_files/description_mapfiles/ko_name.txt.gz', sep='\t', header=None)
```

They'd actually run the split pipeline before, so try that?
```{bash, eval=FALSE}
#they had skipped the normalisation step - try that?
picrust2_pipeline.py -s dna-sequences-KTU.fasta -i KTU.table.summary_relative_abundance.biom -o picrust2_out_pipeline_default_skip_norm -p 8 --skip_norm --verbose

#error seems to be in metagenome step
metagenome_pipeline.py -i KTU.table.summary_relative_abundance.biom \
                       -f picrust2_out_pipeline_default_skip_norm/EC_predicted_and_NSTI.tsv.gz \
                       -o picrust2_out_pipeline_default_skip_norm/EC_metagenome_out \
                       --skip_norm
                       
                       -m picrust2_out_pipeline_default_skip_norm/EC_predicted_and_NSTI.tsv.gz \
                       
picrust2_pipeline.py -s dna-sequences-KTU.fasta -i KTU.table.summary_relative_abundance.biom -o picrust2_out_pipeline_default_skip_norm_2 -p 8 --skip_norm --verbose
```


# Test all split files

Testing what is on this page: https://github.com/picrust/picrust2/wiki/Workflow-GTDB-database

```{bash, eval=FALSE}
export PATH=/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/:$PATH
place_seqs_split.py -s chemerin_16S/seqs_arc.fna -o bac_placed_seqs.tre -p 8 \
                    --ref_dir bacteria \
                    --intermediate placement_working_bac

place_seqs_split.py -s chemerin_16S/seqs_arc.fna -o arc_placed_seqs.tre -p 8 \
                    --ref_dir archaea \
                    --intermediate placement_working_arc      

hsp_split.py -i 16S -r bacteria -t bac_placed_seqs.tre -o bac_marker_nsti_predicted.tsv.gz -p 1 -n

hsp_split.py -i 16S -r archaea -t arc_placed_seqs.tre -o arc_marker_nsti_predicted.tsv.gz -p 1 -n  

pick_best_domain.py \
                    -n1 bacteria \
                    -n2 archaea \
                    --tree_dom1 bac_placed_seqs.tre \
                    --tree_dom2 arc_placed_seqs.tre \
                    --tree_out_dom1 bac_reduced_placed_seqs.tre \
                    --tree_out_dom2 arc_reduced_placed_seqs.tre \
                    --nsti_table_dom1 bac_marker_nsti_predicted.tsv.gz \
                    --nsti_table_dom2 arc_marker_nsti_predicted.tsv.gz \
                    --nsti_table_out_dom1 bac_reduced_marker_nsti_predicted.tsv.gz \
                    --nsti_table_out_dom2 arc_reduced_marker_nsti_predicted.tsv.gz \
                    --nsti_table_out_combined combined_marker_nsti_predicted.tsv.gz

hsp_split.py -i EC -r archaea -t arc_reduced_placed_seqs.tre -o arc_EC_predicted.tsv.gz -p 8 --verbose

hsp_split.py -i KO -r archaea -t arc_reduced_placed_seqs.tre -o arc_KO_predicted.tsv.gz -p 8

hsp_split.py -i EC -r bacteria -t bac_reduced_placed_seqs.tre -o bac_EC_predicted.tsv.gz -p 8 --verbose

hsp_split.py -i KO -r bacteria -t bac_reduced_placed_seqs.tre -o bac_KO_predicted.tsv.gz -p 8

combine_domains.py --table_dom1 bac_EC_predicted.tsv.gz --table_dom2 arc_EC_predicted.tsv.gz -o combined_EC_predicted.tsv.gz

combine_domains.py --table_dom1 bac_KO_predicted.tsv.gz --table_dom2 arc_KO_predicted.tsv.gz -o combined_KO_predicted.tsv.gz

metagenome_pipeline.py -i chemerin_16S/table.biom \
                       -m combined_marker_nsti_predicted.tsv.gz \
                       -f combined_EC_predicted.tsv.gz \
                       -o EC_metagenome_out

metagenome_pipeline.py -i chemerin_16S/table.biom \
                       -m combined_marker_nsti_predicted.tsv.gz \
                       -f combined_KO_predicted.tsv.gz \
                       -o KO_metagenome_out

pathway_pipeline_split.py -i EC_metagenome_out/pred_metagenome_unstrat.tsv.gz \
                          -o pathways_out \
                          --intermediate pathways_working \
                          -p 8

add_descriptions_split.py -i EC_metagenome_out/pred_metagenome_unstrat.tsv.gz -m EC \
                          -o EC_metagenome_out/pred_metagenome_unstrat_descrip.tsv.gz

add_descriptions_split.py -i KO_metagenome_out/pred_metagenome_unstrat.tsv.gz -m KO \
                          -o KO_metagenome_out/pred_metagenome_unstrat_descrip.tsv.gz

add_descriptions_split.py -i pathways_out/path_abun_unstrat.tsv.gz -m METACYC \
                          -o pathways_out/path_abun_unstrat_descrip.tsv.gz
```

## Test other annotations

```{bash, eval=FALSE}
conda activate /opt/miniconda3/envs/picrust-v2.6.0-dev
/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/picrust2_pipeline_split.py -s chemerin_16S/seqs_arc.fna -i chemerin_16S/table.biom -o picrust2_out_pipeline_default_other_annotations -p 8 --verbose --in_traits BIGG,CAZY,GO,PFAM,GENE_NAMES

/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/add_descriptions_split.py -i picrust2_out_pipeline_default_other_annotations/GO_metagenome_out/pred_metagenome_unstrat.tsv.gz -m GO \
                          -o picrust2_out_pipeline_default_other_annotations/GO_metagenome_out/pred_metagenome_unstrat_descrip.tsv.gz
                          
/Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2/scripts/add_descriptions_split.py -i picrust2_out_pipeline_default_other_annotations/PFAM_metagenome_out/pred_metagenome_unstrat.tsv.gz -m PFAM \
                          -o picrust2_out_pipeline_default_other_annotations/PFAM_metagenome_out/pred_metagenome_unstrat_descrip.tsv.gz
```

## Test installation

```{bash, eval=FALSE}
mkdir picrust2-v2.6.0-test
cd picrust2-v2.6.0-test 
git clone --single-branch -b PICRUSt2-v2.6.0 https://github.com/picrust/picrust2
cd picrust2
conda env create --platform osx-64 -n picrust2-v2.6.0 -f picrust2-env.yaml
conda activate picrust2-v2.6.0
pip install --editable .
#note that this installation was done after making default database oldIMG, but before removing _split from new ones

conda env create --platform osx-64 -n picrust2-v2.6.0-dev -f picrust2-env.yaml
```

Test:
```{bash, eval=FALSE}
mkdir test_again
cp -r test_steps_split/chemerin_16S test_again 
cd test_again

picrust2_pipeline_split.py -s chemerin_16S/seqs_arc.fna -i chemerin_16S/table.biom -o picrust2_out_pipeline -p 8 --verbose
picrust2_pipeline.py -s chemerin_16S/seqs_arc.fna -i chemerin_16S/table.biom -o picrust2_out_pipeline_old -p 8 --verbose


#split
place_seqs.py -s chemerin_16S/seqs_arc.fna -o placed_seqs.tre -p 8 \
                    --intermediate placement_working

hsp.py -i 16S -t placed_seqs.tre -o marker_nsti_predicted.tsv.gz -p 1 -n

hsp.py -i EC -t placed_seqs.tre -o EC_predicted.tsv.gz -p 8
hsp.py -i KO -t placed_seqs.tre -o KO_predicted.tsv.gz -p 8

metagenome_pipeline.py -i chemerin_16S/table.biom \
                       -m marker_nsti_predicted.tsv.gz \
                       -f EC_predicted.tsv.gz \
                       -o EC_metagenome_out

metagenome_pipeline.py -i chemerin_16S/table.biom \
                       -m marker_nsti_predicted.tsv.gz \
                       -f KO_predicted.tsv.gz \
                       -o KO_metagenome_out

pathway_pipeline.py -i EC_metagenome_out/pred_metagenome_unstrat.tsv.gz \
                          -o pathways_out \
                          --intermediate pathways_working \
                          -p 8
                          
picrust2_pipeline_oldIMG.py -s chemerin_16S/seqs_arc.fna -i chemerin_16S/table.biom -o picrust2_out_pipeline_oldIMG -p 8 --verbose
cd /Users/robynwright/Dropbox/Langille_Lab_postdoc/Github/picrust2 #remove egg_info folder
pip install --editable . #rerun when editing scripts
cd /Users/robynwright/Dropbox/Langille_Lab_postdoc/microbiome_helper/PICRUSt2/test_again
picrust2_pipeline.py -s chemerin_16S/seqs_arc.fna -i chemerin_16S/table.biom -o picrust2_out_pipeline_split_not_split_script -p 8 --verbose

pathway_pipeline.py -i picrust2_out_pipeline_split_not_split_script/EC_metagenome_out/pred_metagenome_unstrat.tsv.gz \
                          -o picrust2_out_pipeline_split_not_split_script/pathways_out_new \
                          --intermediate pathways_working_new \
                          -p 8
                          
place_seqs.py -s chemerin_16S/seqs_arc.fna -o bac_placed_seqs.tre -p 8 \
                    --intermediate placement_working_bac
place_seqs.py -s chemerin_16S/seqs_arc.fna -o arc_placed_seqs.tre -p 8 -r arc \
                    --intermediate placement_working_arc
place_seqs.py -s chemerin_16S/seqs_arc.fna -o old_placed_seqs.tre -p 8 -r oldIMG \
                    --intermediate placement_working_old
                    
hsp.py -i 16S -t bac_placed_seqs.tre -o bac_marker_nsti_predicted.tsv.gz -p 1 -n -r bac
hsp.py -i 16S -t arc_placed_seqs.tre -o arc_marker_nsti_predicted.tsv.gz -p 1 -n -r arc
hsp.py -i 16S -t old_placed_seqs.tre -o old_marker_nsti_predicted.tsv.gz -p 1 -n -db oldIMG

pick_best_domain.py \
                    -n1 bacteria \
                    -n2 archaea \
                    --tree_dom1 bac_placed_seqs.tre \
                    --tree_dom2 arc_placed_seqs.tre \
                    --tree_out_dom1 bac_reduced_placed_seqs.tre \
                    --tree_out_dom2 arc_reduced_placed_seqs.tre \
                    --nsti_table_dom1 bac_marker_nsti_predicted.tsv.gz \
                    --nsti_table_dom2 arc_marker_nsti_predicted.tsv.gz \
                    --nsti_table_out_dom1 bac_reduced_marker_nsti_predicted.tsv.gz \
                    --nsti_table_out_dom2 arc_reduced_marker_nsti_predicted.tsv.gz \
                    --nsti_table_out_combined combined_marker_nsti_predicted.tsv.gz

hsp.py -i EC -t bac_reduced_placed_seqs.tre -o bac_EC_predicted.tsv.gz -p 8 -r bac
hsp.py -i EC -t arc_reduced_placed_seqs.tre -o arc_EC_predicted.tsv.gz -p 8 -r arc
hsp.py -i EC -t old_placed_seqs.tre -o old_EC_predicted.tsv.gz -p 8 -db oldIMG

combine_domains.py --table_dom1 bac_EC_predicted.tsv.gz --table_dom2 arc_EC_predicted.tsv.gz -o combined_EC_predicted.tsv.gz

metagenome_pipeline.py -i chemerin_16S/table.biom \
                       -m combined_marker_nsti_predicted.tsv.gz \
                       -f combined_EC_predicted.tsv.gz \
                       -o EC_metagenome_out

metagenome_pipeline.py -i chemerin_16S/table.biom \
                       -m old_marker_nsti_predicted.tsv.gz \
                       -f old_EC_predicted.tsv.gz \
                       -o EC_metagenome_out_old

pathway_pipeline.py -i EC_metagenome_out/pred_metagenome_unstrat.tsv.gz \
                    -o pathways_out \
                    --intermediate pathways_working \
                    -p 8
                    
pathway_pipeline.py -i EC_metagenome_out_old/pred_metagenome_unstrat.tsv.gz \
                    -o pathways_out_old \
                    --intermediate pathways_working \
                    -p 8 \
                    -db oldIMG
                    
add_descriptions.py -i EC_metagenome_out/pred_metagenome_unstrat.tsv.gz -m EC \
                          -o EC_metagenome_out/pred_metagenome_unstrat_descrip.tsv.gz
```

Now test SEPP:
```{bash, eval=FALSE}
mkdir test_sepp
conda activate picrust2-v2.6.0
cp -r picrust_test_v2.6.0/chemerin_16S/ test_sepp/
cd test_sepp

place_seqs.py -s chemerin_16S/seqs_arc.fna -o arc_placed_seqs.tre -p 24 -r arc \
                    --intermediate placement_working_sepp_arc -t sepp
                    
place_seqs.py -s chemerin_16S/seqs_arc.fna -o bac_placed_seqs.tre -p 8 \
                    --intermediate placement_working_sepp_bac -t sepp
                    

```

# Repeat make database

## Get the genomes

These were downloaded from [The GTDB website](https://data.gtdb.ecogenomic.org/releases/release214/214.1/genomic_files_reps/). You should check for an updated version of GTDB before downloading them. 

Download all files:
```{bash, eval=FALSE}
mkdir picrust2_database
cd picrust2_database/
mkdir GTDB_r220
mkdir GTDB_r220/database_files
cd GTDB_r220/database_files
wget https://data.gtdb.ecogenomic.org/releases/release220/220.0/genomic_files_reps/gtdb_genomes_reps_r220.tar.gz https://data.gtdb.ecogenomic.org/releases/release220/220.0/ar53_r220.tree https://data.gtdb.ecogenomic.org/releases/release220/220.0/ar53_metadata_r220.tsv.gz https://data.gtdb.ecogenomic.org/releases/release220/220.0/ar53_taxonomy_r220.tsv.gz https://data.gtdb.ecogenomic.org/releases/release220/220.0/bac120_metadata_r220.tsv.gz https://data.gtdb.ecogenomic.org/releases/release220/220.0/bac120_r220.tree https://data.gtdb.ecogenomic.org/releases/release220/220.0/bac120_taxonomy_r220.tsv.gz 

tar -xvf gtdb_genomes_reps_r220.tar.gz
mkdir gtdb_genomes
find gtdb_genomes_reps_r220/ -type f -print0 | xargs -0 mv -t gtdb_genomes/
mkdir genomes_to_search_barrnap
mkdir genomes_to_search_barrnap/bacteria
mkdir genomes_to_search_barrnap/archaea
```

### First get bacterial and archaeal genomes separately using tree files

```{python}
from ete3 import Tree
import os

tree_name_bac = 'bac120_r220.tree'
tree_name_arc = 'ar53_r220.tree'
  
tree_bac = Tree(tree_name_bac, format=1, quoted_node_names=True)
bac_genomes_in_tree = []
for node in tree_bac.traverse("postorder"):
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    bac_genomes_in_tree.append(node.name)
  
tree_arc = Tree(tree_name_arc, format=1, quoted_node_names=True)
arc_genomes_in_tree = []
for node in tree_arc.traverse("postorder"):
  if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
    arc_genomes_in_tree.append(node.name)

genomes = os.listdir('gtdb_genomes/')
genomes = [g for g in genomes if '.decomp' not in g]
bac_genomes_in_tree = [g.split('_', 1)[1] for g in bac_genomes_in_tree]
arc_genomes_in_tree = [g.split('_', 1)[1] for g in arc_genomes_in_tree]
bac_genomes_in_tree = set(bac_genomes_in_tree)
arc_genomes_in_tree = set(arc_genomes_in_tree)
bac_genomes = []
arc_genomes = []

bac_count = 0
arc_count = 0
for genome in genomes:
  if genome.replace('_genomic.fna.gz', '') in bac_genomes_in_tree:
    m = os.system('mv gtdb_genomes/'+genome+' genomes_to_search_barrnap/bacteria')
    bac_count += 1
  elif genome.replace('_genomic.fna.gz', '') in arc_genomes_in_tree:
    m = os.system('mv gtdb_genomes/'+genome+' genomes_to_search_barrnap/archaea')
    arc_count += 1
  if bac_count % 1000 == 0:
    print('Bacterial genomes moved: '+str(bac_count))
  if arc_count % 1000 == 0:
    print('Archaeal genomes moved: '+str(arc_count))
    
print(bac_count, arc_count)
#107235 5869
```

Filter to only >90% completion and <10% redundancy:
```{python, eval=FALSE}
import pandas as pd
import os

barrnap_bacteria = os.listdir('genomes_to_search_barrnap/bacteria')
barrnap_archaea = os.listdir('genomes_to_search_barrnap/archaea')

bac_md = pd.read_csv('bac120_metadata_r220.tsv', index_col=0, header=0, sep='\t')
arc_md = pd.read_csv('ar53_metadata_r220.tsv', index_col=0, header=0, sep='\t')
md = pd.concat([bac_md, arc_md]) #596859 genomes
md = md[md['checkm_completeness'] >= 90]
md = md[md['checkm_contamination'] <= 10] #474470 genomes
genomes = [g.replace('RS_', '').replace('GB_', '') for g in md.index.values]
barrnap_bacteria = [g.replace('_genomic.fna.gz', '') for g in barrnap_bacteria]
barrnap_archaea = [g.replace('_genomic.fna.gz', '') for g in barrnap_archaea]
genomes = set(genomes)
barrnap_bacteria = set(barrnap_bacteria)
barrnap_archaea = set(barrnap_archaea)
bac_count = 0
for genome in barrnap_bacteria:
  if genome not in genomes:
    m = os.system('mv genomes_to_search_barrnap/bacteria/'+genome+'_genomic.fna.gz gtdb_genomes/')
  else:
    bac_count += 1
#67813
    
arc_count = 0
for genome in barrnap_archaea:
  if genome not in genomes:
    m = os.system('mv genomes_to_search_barrnap/archaea/'+genome+'_genomic.fna.gz gtdb_genomes/')
  else:
    arc_count += 1
#2473
```

List of bacteria:
```{python, eval=FALSE}
#cd genomes_to_search_barrnap/
import os

genomes = os.listdir('bacteria')
with open('bacteria.txt', 'w') as f:
  for genome in genomes:
    f.write(genome.replace('.gz', '')+'\n')
```

### Run barrnap

```{bash, eval=FALSE}
#cd genomes_to_search_barrnap
mkdir barrnap_bacteria
mkdir barrnap_archaea

parallel -j 24 --progress --eta 'gunzip {}' ::: archaea/*.gz
parallel -j 24 --progress --eta -a bacteria.txt 'gunzip bacteria/{}.gz'

parallel -j 8 --progress --eta 'barrnap -q -k arc {} --outseq barrnap_archaea/{/} --reject 0.8 --threads 6' ::: archaea/*.fna
parallel -j 8  --progress --eta -a bacteria.txt 'barrnap -q -k bac bacteria/{} --outseq barrnap_bacteria/{} --reject 0.8 --threads 6'

mkdir barrnap_bacteria_16S_single
mkdir barrnap_archaea_16S_single
mkdir barrnap_bacteria_16S_multiple
mkdir barrnap_archaea_16S_multiple
```

### Count copies per genome and get files with multiple 16S copies only

```{python, eval=FALSE}
from Bio import SeqIO
import os
import pandas as pd
import numpy as np

kingdom = 'archaea'
genomes = os.listdir('barrnap_'+kingdom)

copies = []
for genome in genomes:
  count = 0
  sequences = []
  for record in SeqIO.parse('barrnap_'+kingdom+'/'+genome, "fasta"):
    if '16S' in record.id:
      sequences.append(record)
      count += 1
  copies.append([genome, count])
  if sequences != []:
    if len(sequences) == 1:
      w = SeqIO.write(sequences, 'barrnap_'+kingdom+'_16S_single/'+genome, "fasta")
    else:
      w = SeqIO.write(sequences, 'barrnap_'+kingdom+'_16S_multiple/'+genome, "fasta")

with open(kingdom+'_16S_copies.txt', 'w') as f:
  for copy in copies:
    w = f.write(copy[0].replace('_genomic.fna', '')+'\t'+str(copy[1])+'\n')
    
k08 = pd.read_csv(kingdom+'_16S_copies.txt', sep='\t', header=None, index_col=0)
k08 = k08[k08.max(axis=1) > 0]
print(kingdom)
print('80%: ', k08.shape[0], np.mean(k08.iloc[:, 0].values), np.median(k08.iloc[:, 0].values), np.max(k08.iloc[:, 0].values))
# archaea
# 80% r214:  1553 1.2137797810688988 1.0
# 80% r220:  1320 1.2833333333333334 1.0 6
# bacteria
# 80% r214:  32595 1.8404663291915937 1.0 37
# 80% r220:  33229 1.962773480995516 1.0 37
```

### Now cluster the ones with multiple 16S copies

```{bash, eval=FALSE}
mkdir barrnap_bacteria_16S_clustered
parallel -j 8 --eta --progress 'vsearch --cluster_fast {} --id 0.9 --centroids barrnap_bacteria_16S_clustered/{/} --threads 6' ::: barrnap_bacteria_16S_multiple/*

mkdir barrnap_archaea_16S_clustered
parallel -j 8 --progress --eta 'vsearch --cluster_fast {} --id 0.9 --centroids barrnap_archaea_16S_clustered/{/} --threads 6' ::: barrnap_archaea_16S_multiple/*
```

### Make files with single 16S for each genome

```{python, eval=FALSE}
import os
from Bio import SeqIO
import random
from Bio.SeqRecord import SeqRecord

kingdoms = ['archaea', 'bacteria']
for kingdom in kingdoms:
  clusters = os.listdir('barrnap_'+kingdom+'_16S_clustered')
  singles = []
  ids = []
  for genome in clusters:
    records = []
    for record in SeqIO.parse('barrnap_'+kingdom+'_16S_clustered/'+genome, "fasta"):
      this_record = SeqRecord(record.seq, id=genome.replace('_genomic.fna', ''), description='')
      records.append(this_record)
    if len(records) > 1: 
      num = int(random.choice(range(len(records))))
      singles.append(records[num])
      ids.append(records[num].id)
    else:
      singles.append(records[0])
      ids.append(records[0].id)
  single_copies = os.listdir('barrnap_'+kingdom+'_16S_single/')
  for genome in single_copies:
    for record in SeqIO.parse('barrnap_'+kingdom+'_16S_single/'+genome, "fasta"):
      this_record = SeqRecord(record.seq, id=genome.replace('_genomic.fna', ''), description='')
      singles.append(this_record)
      ids.append(this_record.id)
  SeqIO.write(singles, kingdom+"_16S_genes.fasta", "fasta")

#1320
#33229
```

Look at these sequence lengths:
```{python, eval=FALSE}
from Bio import SeqIO
import numpy as np

kingdom = 'bacteria'
lengths = []
for record in SeqIO.parse(kingdom+"_16S_genes.fasta", "fasta"):
  lengths.append(len(str(record.seq)))

print(min(lengths), max(lengths), np.mean(lengths), np.median(lengths))
#bacteria
#1268 1808 1510.6717626169911 1521.0
#archaea
#1270 2410 1476.7363636363636 1472.0
```

### Cluster these single 16S genes for all genomes

```{bash, eval=FALSE}
vsearch --cluster_fast archaea_16S_genes.fasta --id 1 --centroids archaea_16S_centroids.fasta --uc archaea_16S_clusters.uc --threads 24
# Reading file archaea_16S_genes.fasta 100%  
# 1949292 nt in 1320 seqs, min 1270, max 2410, avg 1477
# Masking 100%  
# Sorting by length 100%
# Counting unique k-mers 100%
# Clustering 100%  
# Sorting clusters 100%
# Writing clusters 100%  
# Clusters: 1309 Size min 1, max 3, avg 1.0
# Singletons: 1299, 98.4% of seqs, 99.2% of clusters

vsearch --cluster_fast bacteria_16S_genes.fasta --id 1 --centroids bacteria_16S_centroids.fasta --uc bacteria_16S_clusters.uc --threads 24
# 50198112 nt in 33229 seqs, min 1268, max 1808, avg 1511
# Masking 100%  
# Sorting by length 100%
# Counting unique k-mers 100%  
# Clustering 100%  
# Sorting clusters 100%
# Writing clusters 100%  
# Clusters: 31938 Size min 1, max 26, avg 1.0
# Singletons: 31119, 93.7% of seqs, 97.4% of clusters
```

### Align sequences

ssu-align:
```{bash, eval=FALSE}
conda activate clustalo
export SSUALIGNDIR="/usr/local/share/ssu-align-0.1.1"
ssu-align --rfonly archaea_16S_centroids.fasta archaea_16S_centroids_ssu_align

ssu-align --rfonly bacteria_16S_centroids.fasta bacteria_16S_centroids_ssu_align

########################

esl-reformat -o archaea_16S_centroids_ssu_align.fna afa archaea_16S_centroids_ssu_align/archaea_16S_centroids_ssu_align.archaea.stk

esl-reformat -o bacteria_16S_centroids_ssu_align.fna afa bacteria_16S_centroids_ssu_align/bacteria_16S_centroids_ssu_align.bacteria.stk

# cd ..
```

Look at these sequence lengths (unaligned):
```{python, eval=FALSE}
from Bio import SeqIO
import numpy as np

kingdom = 'bacteria'
lengths = []
for record in SeqIO.parse(kingdom+'_16S_centroids_ssu_align_0.8.fna', "fasta"):
  lengths.append(len(str(record.seq)))

print(min(lengths), max(lengths), np.mean(lengths), np.median(lengths))
#bacteria
#1508 1508 1508.0 1508.0
#archaea
#1582 1582 1582.0 1582.0
```

### Now look at choosing best genome for each cluster

Archaea:
```{python, eval=FALSE}
import os
import pandas as pd
from Bio import SeqIO
import numpy as np
import random

clusters = 'genomes_to_search_barrnap/archaea_16S_clusters_0.8.uc'
aligned_fasta = 'genomes_to_search_barrnap/archaea_16S_centroids_ssu_align_0.8.fna'
md = pd.read_csv('GTDB_r214/ar53_metadata_r214.tsv', index_col=0, header=0, sep='\t')
md = md[md['gtdb_representative'] == 't']

genes_16S = []
for record in SeqIO.parse(aligned_fasta, "fasta"):
  genes_16S.append(record.id)
  
rename_md = {}
for row in md.index.values:
  rename_md[row] = row.split('_', 1)[1]

md = md.rename(index=rename_md)

clusters_all_genomes, clusters_centroid = {}, []
for row in open(clusters, 'r'):
  row = row.replace('\n', '').split('\t')
  if row[-1] != '*':
    genomes = row[-2:]
    if genomes[0] in genes_16S:
      #print('First one', genomes)
      clusters_centroid.append(genomes[0])
      if genomes[0] in clusters_all_genomes:
        clusters_all_genomes[genomes[0]].append(genomes[1])
      else:
        clusters_all_genomes[genomes[0]] = [genomes[1]]
    elif genomes[1] in genes_16S:
      clusters_centroid.append(genomes[1])
      if genomes[1] in clusters_all_genomes:
        clusters_all_genomes[genomes[1]].append(genomes[0])
      else:
        clusters_all_genomes[genomes[1]] = [genomes[0]]
    else:
      print('Neither', genomes)

#now choose best genome from all clusters
cluster_bests = {}
for cluster in clusters_all_genomes:
  md_cluster = md.loc[[cluster]+clusters_all_genomes[cluster], :]
  completeness = md_cluster['checkm_completeness'].values
  contamination = md_cluster['checkm_contamination'].values
  best = ''
  if len(set(completeness)) == 1:
    if len(set(contamination)) == 1:
      num = int(random.choice(range(md_cluster.shape[0])))
      best = md_cluster.index.values[num]
    else:
      lowest, best = 100, ''
      for row in md_cluster.index.values:
        if md_cluster.loc[row, 'checkm_contamination'] < lowest:
          lowest, best = md_cluster.loc[row, 'checkm_contamination'], row
  else:
    highest, best = 0, ''
    for row in md_cluster.index.values:
      if md_cluster.loc[row, 'checkm_completeness'] > highest:
        highest, best = md_cluster.loc[row, 'checkm_completeness'], row
  cluster_bests[cluster] = best

#rename fasta file with the best of the cluster
new_records = []
all_ids = []
for record in SeqIO.parse(aligned_fasta, "fasta"):
  if record.id in cluster_bests:
    print('Changing', record.id, 'to', cluster_bests[record.id])
    record.id = cluster_bests[record.id]
  all_ids.append(record.id)
  new_records.append(record)

SeqIO.write(new_records, aligned_fasta.replace('.fna', '_best.fna'), "fasta")

#make file with name of best genome in cluster and other genomes within the cluster
with open('genomes_to_search_barrnap/archaea_16S_clusters_processed_0.8.txt', 'w') as f:
  w = f.write('Centroid\tBest\tAll genomes\n')
  for cluster in clusters_all_genomes:
    w = f.write(cluster+'\t'+cluster_bests[cluster]+'\t'+cluster+','+','.join(clusters_all_genomes[cluster])+'\n')
  
#get reduced metadata file including only those genomes that are the best
md = md.loc[all_ids, :]
md.to_csv('archaea_metadata_clusters_ssu_align_centroids_0.8.csv')
```

Bacteria:
```{python, eval=FALSE}
import os
import pandas as pd
from Bio import SeqIO
import numpy as np
import random

clusters = 'genomes_to_search_barrnap/bacteria_16S_clusters_0.8.uc'
aligned_fasta = 'genomes_to_search_barrnap/bacteria_16S_centroids_ssu_align_0.8.fna'
md = pd.read_csv('GTDB_r214/bac120_metadata_r214.tsv', index_col=0, header=0, sep='\t')
md = md[md['gtdb_representative'] == 't']

genes_16S = []
for record in SeqIO.parse(aligned_fasta, "fasta"):
  genes_16S.append(record.id)
  
rename_md = {}
for row in md.index.values:
  rename_md[row] = row.split('_', 1)[1]

md = md.rename(index=rename_md)

clusters_all_genomes, clusters_centroid = {}, []
for row in open(clusters, 'r'):
  row = row.replace('\n', '').split('\t')
  if row[-1] != '*':
    #print(row)
    genomes = row[-2:]
    if genomes[0] in genes_16S:
      #print('First one', genomes)
      clusters_centroid.append(genomes[0])
      if genomes[0] in clusters_all_genomes:
        clusters_all_genomes[genomes[0]].append(genomes[1])
      else:
        clusters_all_genomes[genomes[0]] = [genomes[1]]
    elif genomes[1] in genes_16S:
      clusters_centroid.append(genomes[1])
      if genomes[1] in clusters_all_genomes:
        clusters_all_genomes[genomes[1]].append(genomes[0])
      else:
        clusters_all_genomes[genomes[1]] = [genomes[0]]
    else:
      neither = True #note that this means that the sequences were removed from this step because they fitted the model of a different domain the best

#now choose best genome from all clusters
cluster_bests = {}
for cluster in clusters_all_genomes:
  md_cluster = md.loc[[cluster]+clusters_all_genomes[cluster], :]
  completeness = md_cluster['checkm_completeness'].values
  contamination = md_cluster['checkm_contamination'].values
  best = ''
  if len(set(completeness)) == 1:
    if len(set(contamination)) == 1:
      num = int(random.choice(range(md_cluster.shape[0])))
      best = md_cluster.index.values[num]
    else:
      lowest, best = 100, ''
      for row in md_cluster.index.values:
        if md_cluster.loc[row, 'checkm_contamination'] < lowest:
          lowest, best = md_cluster.loc[row, 'checkm_contamination'], row
  else:
    highest, best = 0, ''
    for row in md_cluster.index.values:
      if md_cluster.loc[row, 'checkm_completeness'] > highest:
        highest, best = md_cluster.loc[row, 'checkm_completeness'], row
  cluster_bests[cluster] = best

#rename fasta file with the best of the cluster
new_records = []
all_ids = []
for record in SeqIO.parse(aligned_fasta, "fasta"):
  if record.id in cluster_bests:
    print('Changing', record.id, 'to', cluster_bests[record.id])
    record.id = cluster_bests[record.id]
  all_ids.append(record.id)
  new_records.append(record)

SeqIO.write(new_records, aligned_fasta.replace('.fna', '_best.fna'), "fasta")

#make file with name of best genome in cluster and other genomes within the cluster
with open('genomes_to_search_barrnap/bacteria_16S_clusters_processed_0.8.txt', 'w') as f:
  w = f.write('Centroid\tBest\tAll genomes\n')
  for cluster in clusters_all_genomes:
    w = f.write(cluster+'\t'+cluster_bests[cluster]+'\t'+cluster+','+','.join(clusters_all_genomes[cluster])+'\n')
  
#get reduced metadata file including only those genomes that are the best
md = md.loc[all_ids, :]
md.to_csv('bacteria_metadata_clusters_ssu_align_centroids_0.8.csv')
```

```{bash, eval=FALSE}
mv gtdb_picrust_files gtdb_picrust_files_first_try
mkdir gtdb_picrust_files
```

### And filter the files to only include genomes <= 10% contamination and >= 90% completion

```{python}
import pandas as pd
from Bio import SeqIO
from ete3 import Tree

domains = ['bacteria', 'archaea']
trees = ['GTDB_r214/bac120_r214.tree', 'GTDB_r214/ar53_r214.tree']

for d in range(len(domains)):
  domain = domains[d]
  tree = Tree(trees[d], format=1, quoted_node_names=True)
  aligned_fasta = 'genomes_to_search_barrnap/'+domain+'_16S_centroids_ssu_align_0.8_best.fna'
  md = pd.read_csv(domain+'_metadata_clusters_ssu_align_centroids_0.8.csv', index_col=0, header=0)
  md = md[md['checkm_contamination'] <= 10]
  md = md[md['checkm_completeness'] >= 90]
  new_records = []
  for record in SeqIO.parse(aligned_fasta, "fasta"):
    if record.id in md.index.values:
      new_records.append(record)
  SeqIO.write(new_records, 'gtdb_picrust_files/'+domain+'_16S_centroids_ssu_align_best_reduced_0.8.fna', "fasta")
  pruning = []
  names = []
  for node in tree.traverse("postorder"):
    if 'GB_' in node.name or 'GCA_' in node.name or 'GCF_' in node.name:
      new_name = str(node.name).split('_', 1)[1]
      node.name = new_name
      names.append(new_name)
      if new_name in md.index.values:
        pruning.append(new_name)
  tree.prune(pruning)
  tree.write(outfile='gtdb_picrust_files/'+domain+'_16S_centroids_ssu_align_best_reduced_0.8.tre')
  
```

### Run the raxml check

```{bash, eval=FALSE}
cd gtdb_picrust_files
raxml-ng --check --msa archaea_16S_centroids_ssu_align_best_reduced_0.8.fna --model GTR+G --prefix archaea_0.8_raxml-check
#this didn't need fixing!
#esl-reformat -o archaea_raxml-check.raxml.reduced.phy phylip archaea_16S_centroids_ssu_align_best_reduced.fna
#but for some reason, the place_seqs step didn't work when I'd used this so I've just made the steps the same for both of them

raxml-ng --check --msa bacteria_16S_centroids_ssu_align_best_reduced_0.8.fna --model GTR+G --prefix bacteria_0.8_raxml-check
#this had some duplicates removed still apparently (copied this from previously, but outcome is the same!!)
```

Convert archaea alignment to phylip so we have the same for both:
```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment

seq_records = []
seqs, lengths = 0, []
for record in SeqIO.parse('archaea_16S_centroids_ssu_align_best_reduced_0.8.fna', 'fasta'):
  seq_records.append(record)
  seqs += 1
  lengths.append(len(str(record.seq)))
  
# count = 0
# for row in open('bacteria_0.8_raxml-check.raxml.reduced.phy', 'r'):
#   if count < 2: print([row])
#   count += 1
  
with open('archaea_0.8_raxml-check.raxml.reduced.phy', 'w') as f:
  f.write(str(seqs)+' '+str(int(lengths[0]))+'\n')
  for record in seq_records:
    f.write(record.id+' '+str(record.seq)+'\n')
  
```

### Filter the tree files to include only these

```{python, eval=FALSE}
from ete3 import Tree
import os
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
#from Bio.Alphabet import IUPAC

fixed_alignments = ['bacteria_0.8_raxml-check.raxml.reduced.phy', 'archaea_0.8_raxml-check.raxml.reduced.phy']
tree_files = ['bacteria_16S_centroids_ssu_align_best_reduced_0.8.tre', 'archaea_16S_centroids_ssu_align_best_reduced_0.8.tre']

for f in range(len(fixed_alignments)):
  fixed_alignment = fixed_alignments[f]
  tree_file = tree_files[f]
  genomes = []
  for row in open(fixed_alignment, 'r'):
    if 'GB_' in row or 'GCA_' in row or 'GCF_' in row:
      genomes.append(row.split(' ')[0].replace('>', '').replace('\n', ''))
  
  genomes = set(genomes)
  print(len(genomes))
  #29311, 
  tree = Tree(tree_file, format=1, quoted_node_names=True)
  genome_nodes = []
  for node in tree.traverse("postorder"):
    if node.name in genomes:
      genome_nodes.append(node.name)
  
  tree.prune(genome_nodes)
  tree.write(outfile=tree_file.replace('.tre', '_fixed.tre'), format=1)
```

### Run raxml

```{bash, eval=FALSE}
mkdir archaea_first_raxml
mv archaea_raxml.raxml* archaea_first_raxml/

raxml-ng --evaluate --msa archaea_0.8_raxml-check.raxml.reduced.phy --tree archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre --prefix archaea_0.8_raxml --model GTR+G —threads 24

raxml-ng --evaluate --msa bacteria_0.8_raxml-check.raxml.reduced.phy --tree bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre --prefix bacteria_0.8_raxml --model GTR+G —threads 24
```

### Resave the phylip files and reformat for the hmm files

```{python, eval=FALSE}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment

files = ['bacteria_0.8_raxml-check.raxml.reduced.phy', 'archaea_0.8_raxml-check.raxml.reduced.phy']
new_names = ['bacteria_0.8_raxml-check.raxml.reduced.fna', 'archaea_0.8_raxml-check.raxml.reduced.fna']

for f in range(len(files)):
  sequences = {}
  seq_records = []
  count = 0
  for row in open(files[f], 'r'):
    #if count > 10: break
    if '_' in row:
      name, seq = row.replace('\n', '').split(' ')
      sequences[name] = seq
      seq_records.append(SeqRecord(Seq(seq),id=name, description=''))
    count += 1
  msa = MultipleSeqAlignment(seq_records)
  AlignIO.write(msa, new_names[f], "fasta")
```

Convert the fasta file to DNA and reformat to stockholm:
```{bash, eval=FALSE}
esl-reformat -d -o bacteria_0.8_raxml-check.raxml.reduced_dna.fna afa bacteria_0.8_raxml-check.raxml.reduced.fna

esl-reformat -o bacteria_0.8_raxml-check.raxml.reduced_dna.sto stockholm bacteria_0.8_raxml-check.raxml.reduced_dna.fna

esl-reformat -d -o archaea_0.8_raxml-check.raxml.reduced_dna.fna afa archaea_0.8_raxml-check.raxml.reduced.fna

esl-reformat -o archaea_0.8_raxml-check.raxml.reduced_dna.sto stockholm archaea_0.8_raxml-check.raxml.reduced_dna.fna
```

### Resave the archaea file and reformat for the hmm files

```{python}
# from Bio import SeqIO
# from Bio.SeqRecord import SeqRecord
# from Bio.Seq import Seq
# from Bio.Alphabet import IUPAC
# from Bio import AlignIO
# from Bio.Align import MultipleSeqAlignment
# 
# files = ['bacteria_raxml-check.raxml.reduced.phy']
# new_names = ['bacteria_raxml-check.raxml.reduced.fna']
# 
# for f in range(len(files)):
#   sequences = {}
#   seq_records = []
#   count = 0
#   for row in open(files[f], 'r'):
#     #if count > 10: break
#     if '_' in row:
#       name, seq = row.replace('\n', '').split(' ')
#       sequences[name] = seq
#       seq_records.append(SeqRecord(Seq(seq),id=name, description=''))
#     count += 1
#   msa = MultipleSeqAlignment(seq_records)
#   AlignIO.write(msa, new_names[f], "fasta")
```

### Reformat for the hmm files

```{bash, eval=FALSE}
#esl-reformat -d -o archaea_16S_centroids_ssu_align_best_reduced_dna.fna afa archaea_16S_centroids_ssu_align_best_reduced.fna

#esl-reformat -o archaea_16S_centroids_ssu_align_best_reduced_dna.sto stockholm archaea_16S_centroids_ssu_align_best_reduced_dna.fna
```


### Run the HMMs

```{bash, eval=FALSE}
#hmmbuild --cpu 24 archaea_16S_centroids_ssu_align_best_reduced_dna.hmm archaea_16S_centroids_ssu_align_best_reduced_dna.sto

#esl-reformat -o archaea_16S_centroids_ssu_align_best_reduced_dna_reformat.fna fasta archaea_16S_centroids_ssu_align_best_reduced_dna.sto

hmmbuild --cpu 24 bacteria_0.8_raxml-check.raxml.reduced_dna.hmm bacteria_0.8_raxml-check.raxml.reduced_dna.sto
hmmbuild --cpu 24 archaea_0.8_raxml-check.raxml.reduced_dna.hmm archaea_0.8_raxml-check.raxml.reduced_dna.sto
```

### Copy and rename the resulting files

```{bash, eval=FALSE}
cd ..
mv gtdb_picrust_ref/ gtdb_0.5_picrust_ref
mkdir gtdb_picrust_ref
#mkdir gtdb_picrust_ref/arc_ref
mkdir gtdb_picrust_ref/bac_ref
#cp gtdb_picrust_files/archaea_16S_centroids_ssu_align_best_reduced_dna_reformat.fna gtdb_picrust_ref/arc_ref/arc_ref.fna
#cp gtdb_picrust_files/archaea_16S_centroids_ssu_align_best_reduced_dna.hmm gtdb_picrust_ref/arc_ref/arc_ref.hmm
#cp gtdb_picrust_files/archaea_16S_centroids_ssu_align_best_reduced.tre gtdb_picrust_ref/arc_ref/arc_ref.tre
#cp gtdb_picrust_files/archaea_raxml.raxml.bestModel gtdb_picrust_ref/arc_ref/arc_ref.model
#mv gtdb_picrust_ref/arc_ref gtdb_picrust_ref/arc_ref_initial

cp gtdb_picrust_files/bacteria_0.8_raxml-check.raxml.reduced_dna.fna gtdb_picrust_ref/bac_ref/bac_ref.fna
cp gtdb_picrust_files/bacteria_0.8_raxml-check.raxml.reduced_dna.hmm gtdb_picrust_ref/bac_ref/bac_ref.hmm
cp gtdb_picrust_files/bacteria_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre gtdb_picrust_ref/bac_ref/bac_ref.tre
cp gtdb_picrust_files/bacteria_0.8_raxml.raxml.bestModel gtdb_picrust_ref/bac_ref/bac_ref.model
cp gtdb_picrust_files/bacteria_0.8_raxml.raxml.log gtdb_picrust_ref/bac_ref/bac_ref.raxml_info

mkdir gtdb_picrust_ref/arc_ref
cp gtdb_picrust_files/archaea_0.8_raxml-check.raxml.reduced_dna.fna gtdb_picrust_ref/arc_ref/arc_ref.fna
cp gtdb_picrust_files/archaea_0.8_raxml-check.raxml.reduced_dna.hmm gtdb_picrust_ref/arc_ref/arc_ref.hmm
cp gtdb_picrust_files/archaea_16S_centroids_ssu_align_best_reduced_0.8_fixed.tre gtdb_picrust_ref/arc_ref/arc_ref.tre
cp gtdb_picrust_files/archaea_0.8_raxml.raxml.bestModel gtdb_picrust_ref/arc_ref/arc_ref.model
cp gtdb_picrust_files/archaea_0.8_raxml.raxml.log gtdb_picrust_ref/arc_ref/arc_ref.raxml_info
```